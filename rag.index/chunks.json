["def __init__(\n        self,\n        in_channels,\n        out_channels,\n        dilations,\n        *,\n        norm,\n        activation,\n        pool_kernel_size=None,\n        dropout: float = 0.0,\n        use_depthwise_separable_conv=False,\n    ):\n        \"\"\"\n        Args:\n            in_channels (int): number of input channels for ASPP.\n            out_channels (int): number of output channels.\n            dilations (list): a list of 3 dilations in ASPP.\n            norm (str or callable): normalization for all conv layers.\n                See :func:`layers.get_norm` for supported format. norm is\n                applied to all conv layers except the conv following\n                global average pooling.\n            activation (callable): activation function.\n            pool_kernel_size (tuple, list): the average pooling size (kh, kw)\n                for image pooling layer in ASPP. If set to None, it always\n                performs global average pooling. If not None, it must be\n                divisible by the shape of inputs in forward(). It is recommended\n                to use a fixed input feature size in training, and set this\n                option to match this size, so that it performs global average\n                pooling in training, and the size of the pooling window stays\n                consistent in inference.\n            dropout (float): apply dropout on the output of ASPP. It is used in\n                the official DeepLab implementation with a rate of 0.1:\n                https://github.com/tensorflow/models/blob/21b73d22f3ed05b650e85ac50849408dd36de32e/research/deeplab/model.py#L532  # noqa\n            use_depthwise_separable_conv (bool): use DepthwiseSeparableConv2d\n                for 3x3 convs in ASPP, proposed in :paper:`DeepLabV3+`.\n        \"\"\"\n        super(ASPP, self).__init__()\n        assert len(dilations) == 3, \"ASPP expects 3 dilations, got {}\".format(len(dilations))\n        self.pool_kernel_size = pool_kernel_size\n        self.dropout = dropout\n        use_bias = norm == \"\"\n        self.convs = nn.ModuleList()\n        # conv 1x1\n        self.convs.append(\n            Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size=1,\n                bias=use_bias,\n                norm=get_norm(norm, out_channels),\n                activation=deepcopy(activation),\n            )\n        )\n        weight_init.c2_xavier_fill(self.convs[-1])\n        # atrous convs\n        for dilation in dilations:\n            if use_depthwise_separable_conv:\n                self.convs.append(\n                    DepthwiseSeparableConv2d(\n                        in_channels,\n                        out_channels,\n                        kernel_size=3,\n                        padding=dilation,\n                        dilation=dilation,\n                        norm1=norm,\n                        activation1=deepcopy(activation),\n                        norm2=norm,\n                        activation2=deepcopy(activation),\n                    )\n                )\n            else:\n                self.convs.append(\n                    Conv2d(\n                        in_channels,\n                        out_channels,\n                        kernel_size=3,\n                        padding=dilation,\n                        dilation=dilation,\n                        bias=use_bias,\n                        norm=get_norm(norm, out_channels),\n                        activation=deepcopy(activation),\n                    )\n                )\n                weight_init.c2_xavier_fill(self.convs[-1])\n        # image pooling\n        # We do not add BatchNorm because the spatial resolution is 1x1,\n        # the original TF implementation has BatchNorm.\n        if pool_kernel_size is None:\n            image_pooling = nn.Sequential(\n                nn.AdaptiveAvgPool2d(1),\n                Conv2d(in_channels, out_channels, 1, bias=True, activation=deepcopy(activation)),\n            )\n        else:\n            image_pooling = nn.Sequential(\n                nn.AvgPool2d(kernel_size=pool_kernel_size, stride=1),\n                Conv2d(in_channels, out_channels, 1, bias=True, activation=deepcopy(activation)),\n            )\n        weight_init.c2_xavier_fill(image_pooling[1])\n        self.convs.append(image_pooling)", "class ConvNorm(nn.Module):\n    def __init__(\n            self, in_chs, out_chs, kernel_size=1, stride=1, padding=0, dilation=1, groups=1, bn_weight_init=1):\n        super().__init__()\n        self.linear = nn.Conv2d(in_chs, out_chs, kernel_size, stride, padding, dilation, groups, bias=False)\n        self.bn = nn.BatchNorm2d(out_chs)", "class ModelWithFusable(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.fusable = FusableModule()\n            self.normal = nn.Linear(10, 10)", "self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.norm = norm_layer(attn_dim) if scale_norm else nn.Identity()\n        self.proj = nn.Linear(attn_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)", "class AlexNet(nn.Module):\n    def __init__(self, num_classes: int = 1000, dropout: float = 0.5) -> None:\n        super().__init__()\n        _log_api_usage_once(self)\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=dropout),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=dropout),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes),\n        )", "return nn.Sequential(*[\n        pool,\n        nn.Conv2d(in_channels, out_channels, 1, stride=1, padding=0, bias=False),\n        norm_layer(out_channels)\n    ])", "def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)", "Args:\n            in_channels (int): number of input feature channels. When using multiple\n                input features, they must have the same number of channels.\n            num_anchors (int): number of anchors to predict for *each spatial position*\n                on the feature map. The total number of anchors for each\n                feature map will be `num_anchors * H * W`.\n            box_dim (int): dimension of a box, which is also the number of box regression\n                predictions to make for each anchor. An axis aligned box has\n                box_dim=4, while a rotated box has box_dim=5.\n            conv_dims (list[int]): a list of integers representing the output channels\n                of N conv layers. Set it to -1 to use the same number of output channels\n                as input channels.\n        \"\"\"\n        super().__init__()\n        cur_channels = in_channels\n        # Keeping the old variable names and structure for backwards compatiblity.\n        # Otherwise the old checkpoints will fail to load.\n        if len(conv_dims) == 1:\n            out_channels = cur_channels if conv_dims[0] == -1 else conv_dims[0]\n            # 3x3 conv for the hidden representation\n            self.conv = self._get_rpn_conv(cur_channels, out_channels)\n            cur_channels = out_channels\n        else:\n            self.conv = nn.Sequential()\n            for k, conv_dim in enumerate(conv_dims):\n                out_channels = cur_channels if conv_dim == -1 else conv_dim\n                if out_channels <= 0:\n                    raise ValueError(\n                        f\"Conv output channels should be greater than 0. Got {out_channels}\"\n                    )\n                conv = self._get_rpn_conv(cur_channels, out_channels)\n                self.conv.add_module(f\"conv{k}\", conv)\n                cur_channels = out_channels\n        # 1x1 conv for predicting objectness logits\n        self.objectness_logits = nn.Conv2d(cur_channels, num_anchors, kernel_size=1, stride=1)\n        # 1x1 conv for predicting box2box transform deltas\n        self.anchor_deltas = nn.Conv2d(cur_channels, num_anchors * box_dim, kernel_size=1, stride=1)", "self.fc = nn.Linear(output_channels, num_classes)", "def create_complex_model(self):\n        m = nn.Module()\n        m.block1 = nn.Module()\n        m.block1.layer1 = nn.Linear(2, 3)\n        m.layer2 = nn.Linear(3, 2)\n        m.res = nn.Module()\n        m.res.layer2 = nn.Linear(3, 2)", "self.conv = nn.Conv2d(\n            self.max_in_channels,\n            self.max_out_channels,\n            self.kernel_size,\n            stride=self.stride,\n            bias=False,\n        )", "# mlp to generate continuous relative position bias\n        self.cpb_mlp = nn.Sequential(\n            nn.Linear(2, 512, bias=True),\n            nn.ReLU(inplace=True),\n            nn.Linear(512, num_heads, bias=False)\n        )", "self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))))\n        # mlp to generate continuous relative position bias\n        self.cpb_mlp = nn.Sequential(\n            nn.Linear(2, 512, bias=True), nn.ReLU(inplace=True), nn.Linear(512, num_heads, bias=False)\n        )\n        if qkv_bias:\n            length = self.qkv.bias.numel() // 3\n            self.qkv.bias[length : 2 * length].data.zero_()", "self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)", "class DenseLayer(nn.Module):\n    def __init__(\n            self,\n            num_input_features,\n            growth_rate,\n            bn_size,\n            norm_layer=BatchNormAct2d,\n            drop_rate=0.,\n            grad_checkpointing=False,\n    ):\n        super(DenseLayer, self).__init__()\n        self.add_module('norm1', norm_layer(num_input_features)),\n        self.add_module('conv1', nn.Conv2d(\n            num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)),\n        self.add_module('norm2', norm_layer(bn_size * growth_rate)),\n        self.add_module('conv2', nn.Conv2d(\n            bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)),\n        self.drop_rate = float(drop_rate)\n        self.grad_checkpointing = grad_checkpointing", "self.wq = nn.Linear(dim, dim, bias=qkv_bias)\n        self.wk = nn.Linear(dim, dim, bias=qkv_bias)\n        self.wv = nn.Linear(dim, dim, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)", "self.scale_heads = []\n        for in_feature, stride, channels in zip(\n            self.in_features, feature_strides, feature_channels\n        ):\n            head_ops = []\n            head_length = max(1, int(np.log2(stride) - np.log2(self.common_stride)))\n            for k in range(head_length):\n                norm_module = get_norm(norm, conv_dims)\n                conv = Conv2d(\n                    channels if k == 0 else conv_dims,\n                    conv_dims,\n                    kernel_size=3,\n                    stride=1,\n                    padding=1,\n                    bias=not norm,\n                    norm=norm_module,\n                    activation=F.relu,\n                )\n                weight_init.c2_msra_fill(conv)\n                head_ops.append(conv)\n                if stride != self.common_stride:\n                    head_ops.append(\n                        nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)\n                    )\n            self.scale_heads.append(nn.Sequential(*head_ops))\n            self.add_module(in_feature, self.scale_heads[-1])\n        self.predictor = Conv2d(conv_dims, num_classes, kernel_size=1, stride=1, padding=0)\n        weight_init.c2_msra_fill(self.predictor)", "cur_channels = input_shape.channels\n        for k, conv_dim in enumerate(conv_dims[:-1]):\n            conv = Conv2d(\n                cur_channels,\n                conv_dim,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n                bias=not conv_norm,\n                norm=get_norm(conv_norm, conv_dim),\n                activation=nn.ReLU(),\n            )\n            self.add_module(\"mask_fcn{}\".format(k + 1), conv)\n            self.conv_norm_relus.append(conv)\n            cur_channels = conv_dim", "register_hooks = {\n    nn.Conv1d: count_convNd,\n    nn.Conv2d: count_convNd,\n    nn.Conv3d: count_convNd,\n    MyConv2d: count_convNd,\n    ######################################\n    nn.Linear: count_linear,\n    ######################################\n    nn.Dropout: None,\n    nn.Dropout2d: None,\n    nn.Dropout3d: None,\n    nn.BatchNorm2d: None,\n}", "def __init__(\n        self,\n        input_channels: int,\n        squeeze_channels: int,\n        activation: Callable[..., torch.nn.Module] = torch.nn.ReLU,\n        scale_activation: Callable[..., torch.nn.Module] = torch.nn.Sigmoid,\n    ) -> None:\n        super().__init__()\n        _log_api_usage_once(self)\n        self.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n        self.fc1 = torch.nn.Conv2d(input_channels, squeeze_channels, 1)\n        self.fc2 = torch.nn.Conv2d(squeeze_channels, input_channels, 1)\n        self.activation = activation()\n        self.scale_activation = scale_activation()", "# build layers\n        layers = []\n        for i in range(self.n_layers):\n            layers.append(\n                nn.Sequential(\n                    nn.Linear(\n                        self.arch_encoder.n_dim if i == 0 else self.hidden_size,\n                        self.hidden_size,\n                    ),\n                    nn.ReLU(inplace=True),\n                )\n            )\n        layers.append(nn.Linear(self.hidden_size, 1, bias=False))\n        self.layers = nn.Sequential(*layers)\n        self.base_acc = nn.Parameter(\n            torch.zeros(1, device=self.device), requires_grad=False\n        )", "def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        self.fused_attn = use_fused_attn(experimental=True)\n        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)", "self.conv3 = Conv2d(\n            bottleneck_channels,\n            out_channels,\n            kernel_size=1,\n            bias=False,\n            norm=get_norm(norm, out_channels),\n        )", "def _get_rpn_conv(self, in_channels, out_channels):\n        return Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            activation=nn.ReLU(),\n        )", "self.dim = dim\n        self.qkv = ConvNormAct(\n            in_channels,\n            3 * total_dim,\n            1,\n            bias=use_bias[0],\n            norm_layer=norm_layer[0],\n            act_layer=act_layer[0],\n        )\n        self.aggreg = nn.ModuleList([\n            nn.Sequential(\n                nn.Conv2d(\n                    3 * total_dim,\n                    3 * total_dim,\n                    scale,\n                    padding=get_same_padding(scale),\n                    groups=3 * total_dim,\n                    bias=use_bias[0],\n                ),\n                nn.Conv2d(3 * total_dim, 3 * total_dim, 1, groups=3 * heads, bias=use_bias[0]),\n            )\n            for scale in scales\n        ])\n        self.kernel_func = kernel_func(inplace=False)", "self.projs = nn.ModuleList()\n        for d in range(num_branches):\n            if dim[d] == dim[(d + 1) % num_branches] and False:\n                tmp = [nn.Identity()]\n            else:\n                tmp = [norm_layer(dim[d]), act_layer(), nn.Linear(dim[d], dim[(d + 1) % num_branches])]\n            self.projs.append(nn.Sequential(*tmp))", "self.qkv = nn.Linear(dim, dim_attn * 3, bias=bias)\n        self.rel_pos = rel_pos_cls(num_heads=self.num_heads) if rel_pos_cls else None\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim_attn, dim_out, bias=bias)\n        self.proj_drop = nn.Dropout(proj_drop)", "def _init_weights(self, m: nn.Module) -> None:\n        if isinstance(m, nn.Conv2d):\n            trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)", "self.norm2 = norm_layer(att_dim)\n        mlp_dim_out = dim_out\n        self.shortcut_proj_mlp = nn.Linear(dim, dim_out) if proj_needed and not expand_attn else None\n        self.mlp = Mlp(\n            in_features=att_dim,\n            hidden_features=int(att_dim * mlp_ratio),\n            out_features=mlp_dim_out,\n        )\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()", "# Classifier head\n        self.num_features = self.head_hidden_size = embed_dims[-1]\n        self.norm = norm_layer(embed_dims[-1])\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(embed_dims[-1], num_classes) if num_classes > 0 else nn.Identity()\n        self.dist = distillation\n        if self.dist:\n            self.head_dist = nn.Linear(embed_dims[-1], num_classes) if num_classes > 0 else nn.Identity()\n        else:\n            self.head_dist = None", "def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)", "for n, m in self.named_modules():\n            if 'fc' in n and isinstance(m, nn.Linear):\n                if cfg.zero_init_fc:\n                    nn.init.zeros_(m.weight)\n                else:\n                    nn.init.normal_(m.weight, 0., .01)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='linear')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)", "@META_ARCH_REGISTRY.register()\nclass _SimpleModel(nn.Module):\n    @configurable\n    def __init__(self, sleep_sec=0):\n        super().__init__()\n        self.mod = nn.Linear(10, 20)\n        self.sleep_sec = sleep_sec", "self.cls_logits = nn.Conv2d(in_channels, num_anchors * num_classes, kernel_size=3, stride=1, padding=1)\n        torch.nn.init.normal_(self.cls_logits.weight, std=0.01)\n        torch.nn.init.constant_(self.cls_logits.bias, -math.log((1 - prior_probability) / prior_probability))", "for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)", "self.add_module('conv1', nn.Conv2d(in_chs, out_chs // 2, kernel_size=3, stride=2, padding=1))\n        self.add_module('norm1', norm_layer(out_chs // 2))\n        self.add_module('act1', act_layer())\n        self.add_module('conv2', nn.Conv2d(out_chs // 2, out_chs, kernel_size=3, stride=2, padding=1))\n        self.add_module('norm2', norm_layer(out_chs))\n        self.add_module('act2', act_layer())", "def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n        self.head_dist = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()", "# set output type\n        if use_aux_head:\n            self.aux_head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n        else:\n            self.aux_head = None\n        self.norm = norm_layer(self.num_features)", "if in_channels != out_channels:\n            self.shortcut = Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size=1,\n                stride=stride,\n                bias=False,\n                norm=get_norm(norm, out_channels),\n            )\n        else:\n            self.shortcut = None", "def __init__(self, c1, c2, num_heads, num_layers):\n        \"\"\"Initializes a Transformer block for vision tasks, adapting dimensions if necessary and stacking specified\n        layers.\n        \"\"\"\n        super().__init__()\n        self.conv = None\n        if c1 != c2:\n            self.conv = Conv(c1, c2)\n        self.linear = nn.Linear(c2, c2)  # learnable position embedding\n        self.tr = nn.Sequential(*(TransformerLayer(c2, num_heads) for _ in range(num_layers)))\n        self.c2 = c2", "def __init__(\n        self, c1, c2, k=1, s=1, p=None, g=1, dropout_p=0.0\n    ):  # ch_in, ch_out, kernel, stride, padding, groups, dropout probability\n        \"\"\"Initializes YOLOv5 classification head with convolution, pooling, and dropout layers for input to output\n        channel transformation.\n        \"\"\"\n        super().__init__()\n        c_ = 1280  # efficientnet_b0 size\n        self.conv = Conv(c1, c_, k, s, autopad(k, p), g)\n        self.pool = nn.AdaptiveAvgPool2d(1)  # to x(b,c_,1,1)\n        self.drop = nn.Dropout(p=dropout_p, inplace=True)\n        self.linear = nn.Linear(c_, c2)  # to x(b,c2)", "class FusableModule(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv = nn.Conv2d(3, 3, 1)", "self.conv1 = Conv2d(in_channels, bottleneck_channels, 1, bias=False)\n        self.norm1 = get_norm(norm, bottleneck_channels)\n        self.act1 = act_layer()", "self.conv1 = nn.Conv2d(inplanes, width * scale, kernel_size=1, bias=False)\n        self.bn1 = norm_layer(width * scale)", "convs = []\n        bns = []\n        for i in range(self.num_scales):\n            convs.append(nn.Conv2d(\n                width, width, kernel_size=3, stride=stride, padding=first_dilation,\n                dilation=first_dilation, groups=cardinality, bias=False))\n            bns.append(norm_layer(width))\n        self.convs = nn.ModuleList(convs)\n        self.bns = nn.ModuleList(bns)\n        if self.is_first:\n            # FIXME this should probably have count_include_pad=False, but hurts original weights\n            self.pool = nn.AvgPool2d(kernel_size=3, stride=stride, padding=1)\n        else:\n            self.pool = None", "def _make_layer(self, block_type, inplanes, planes, block_types, stride=1):\n        downsample = None\n        if stride != 1 or inplanes != planes * block_type.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(inplanes, planes * block_type.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block_type.expansion, momentum=_BN_MOMENTUM),\n            )", "self.num_experts = num_experts\n        conv_kwargs = dict(num_experts=self.num_experts)\n        super(CondConvResidual, self).__init__(\n            in_chs,\n            out_chs,\n            dw_kernel_size=dw_kernel_size,\n            stride=stride,\n            dilation=dilation,\n            group_size=group_size,\n            pad_type=pad_type,\n            noskip=noskip,\n            exp_ratio=exp_ratio,\n            exp_kernel_size=exp_kernel_size,\n            pw_kernel_size=pw_kernel_size,\n            act_layer=act_layer,\n            norm_layer=norm_layer,\n            aa_layer=aa_layer,\n            se_layer=se_layer,\n            conv_kwargs=conv_kwargs,\n            drop_path_rate=drop_path_rate,\n        )\n        self.routing_fn = nn.Linear(in_chs, self.num_experts)", "def __init__(self, out_chs, head_dim, norm_layer = nn.BatchNorm2d, act_layer = nn.ReLU):\n        super(ConvAttention, self).__init__()\n        self.group_conv3x3 = nn.Conv2d(\n            out_chs, out_chs,\n            kernel_size=3, stride=1, padding=1, groups=out_chs // head_dim, bias=False\n        )\n        self.norm = norm_layer(out_chs)\n        self.act = act_layer()\n        self.projection = nn.Conv2d(out_chs, out_chs, kernel_size=1, bias=False)", "self.proj = nn.Conv2d(\n            dim_in,\n            dim_out,\n            kernel_size=kernel,\n            stride=stride,\n            padding=padding,\n        )", "def __init__(\n            self,\n            in_features,\n            hidden_features=None,\n            out_features=None,\n            act_layer=nn.GELU,\n            norm_layer=nn.BatchNorm2d,\n            drop=0.\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Conv2d(in_features, hidden_features, 1)\n        self.norm1 = norm_layer(hidden_features) if norm_layer is not None else nn.Identity()\n        self.act = act_layer()\n        self.fc2 = nn.Conv2d(hidden_features, out_features, 1)\n        self.norm2 = norm_layer(out_features) if norm_layer is not None else nn.Identity()\n        self.drop = nn.Dropout(drop)", "for layer in self.modules():\n            if isinstance(layer, nn.Conv2d):\n                torch.nn.init.normal_(layer.weight, std=0.01)  # type: ignore[arg-type]\n                if layer.bias is not None:\n                    torch.nn.init.constant_(layer.bias, 0)  # type: ignore[arg-type]", "if conv_stem_configs is not None:\n            # As per https://arxiv.org/abs/2106.14881\n            seq_proj = nn.Sequential()\n            prev_channels = 3\n            for i, conv_stem_layer_config in enumerate(conv_stem_configs):\n                seq_proj.add_module(\n                    f\"conv_bn_relu_{i}\",\n                    Conv2dNormActivation(\n                        in_channels=prev_channels,\n                        out_channels=conv_stem_layer_config.out_channels,\n                        kernel_size=conv_stem_layer_config.kernel_size,\n                        stride=conv_stem_layer_config.stride,\n                        norm_layer=conv_stem_layer_config.norm_layer,\n                        activation_layer=conv_stem_layer_config.activation_layer,\n                    ),\n                )\n                prev_channels = conv_stem_layer_config.out_channels\n            seq_proj.add_module(\n                \"conv_last\", nn.Conv2d(in_channels=prev_channels, out_channels=hidden_dim, kernel_size=1)\n            )\n            self.conv_proj: nn.Module = seq_proj\n        else:\n            self.conv_proj = nn.Conv2d(\n                in_channels=3, out_channels=hidden_dim, kernel_size=patch_size, stride=patch_size\n            )", "def __init__(self, c1, k=3):  # ch_in, kernel\n        \"\"\"Initializes FReLU activation with channel `c1` and kernel size `k`.\"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(c1, c1, k, 1, 1, groups=c1, bias=False)\n        self.bn = nn.BatchNorm2d(c1)", "@torch.no_grad()\n    def fuse(self):\n        c, bn = self.linear, self.bn\n        w = bn.weight / (bn.running_var + bn.eps) ** 0.5\n        w = c.weight * w[:, None, None, None]\n        b = bn.bias - bn.running_mean * bn.weight / (bn.running_var + bn.eps) ** 0.5\n        m = nn.Conv2d(\n            w.size(1), w.size(0), w.shape[2:], stride=self.linear.stride,\n            padding=self.linear.padding, dilation=self.linear.dilation, groups=self.linear.groups)\n        m.weight.data.copy_(w)\n        m.bias.data.copy_(b)\n        return m", "class Conv2d(torch.nn.Conv2d):\n    \"\"\"\n    A wrapper around :class:`torch.nn.Conv2d` to support empty inputs and more features.\n    \"\"\"", "if in_channels != out_channels:\n            self.shortcut = Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size=1,\n                stride=stride,\n                bias=False,\n                norm=get_norm(norm, out_channels),\n            )\n        else:\n            self.shortcut = None", "Args:\n        feat_shape: Feature shape for embedding.\n        bands: Pre-calculated frequency bands.\n        num_bands: Number of frequency bands (determines output dim).\n        max_res: Maximum resolution for pixel based freq.\n        temperature: Temperature for non-pixel freq.\n        linear_bands: Linear band spacing for pixel based freq.\n        include_grid: Include the spatial grid in output.\n        in_pixels: Output in pixel freq.\n        ref_feat_shape: Reference feature shape for resize / fine-tune.\n        grid_offset: Constant offset to add to grid for non-pixel freq.\n        grid_indexing: Indexing mode for meshgrid ('ij' or 'xy')\n        dtype: Output dtype.\n        device: Output device.", "kernel, bias = self._get_kernel_bias()\n        self.reparam_conv = nn.Conv2d(\n            in_channels=self.conv_kxk.conv.in_channels,\n            out_channels=self.conv_kxk.conv.out_channels,\n            kernel_size=self.conv_kxk.conv.kernel_size,\n            stride=self.conv_kxk.conv.stride,\n            padding=self.conv_kxk.conv.padding,\n            dilation=self.conv_kxk.conv.dilation,\n            groups=self.conv_kxk.conv.groups,\n            bias=True,\n        )\n        self.reparam_conv.weight.data = kernel\n        self.reparam_conv.bias.data = bias", "# building classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=dropout),\n            nn.Linear(self.last_channel, num_classes),\n        )", "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import SelectAdaptivePool2d, Linear, make_divisible\nfrom ._builder import build_model_with_cfg\nfrom ._efficientnet_blocks import SqueezeExcite, ConvBnAct\nfrom ._manipulate import checkpoint_seq\nfrom ._registry import register_model, generate_default_cfgs", "for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                init_range = 1.0 / math.sqrt(m.out_features)\n                nn.init.uniform_(m.weight, -init_range, init_range)\n                nn.init.zeros_(m.bias)", "for m in self.mlp.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.normal_(m.bias, std=1e-6)", "self.q_pool = q_pool\n        self.qkv = nn.Linear(dim, dim_out * 3)\n        self.proj = nn.Linear(dim_out, dim_out)", "self.conv2 = Conv2d(\n            bottleneck_channels,\n            bottleneck_channels,\n            3,\n            padding=1,\n            bias=False,\n        )\n        self.norm2 = get_norm(norm, bottleneck_channels)\n        self.act2 = act_layer()", "if weight_init != 'skip':\n            init_fn = init_weight_jax if weight_init == 'jax' else init_weight_vit\n            init_fn = partial(init_fn, classifier_name='head.fc')\n            named_apply(init_fn, self)\n        if fix_init:\n            self.fix_init_weight()\n        if isinstance(self.head.fc, nn.Linear):\n            self.head.fc.weight.data.mul_(head_init_scale)\n            self.head.fc.bias.data.mul_(head_init_scale)", "self.cheap_operation = nn.Sequential(\n            nn.Conv2d(init_chs, new_chs, dw_size, 1, dw_size//2, groups=init_chs, bias=False),\n            nn.BatchNorm2d(new_chs),\n            act_layer(inplace=True) if use_act else nn.Identity(),\n        )", "def reset(self, num_classes: int, pool_type: Optional[str] = None, reset_other: bool = False):\n        if pool_type is not None:\n            self.pool_type = pool_type\n        if reset_other:\n            self.pre_logits = nn.Identity()\n            self.norm = nn.Identity()\n        self.fc = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()", "def __init__(self, inplanes, planes, groups, reduction, stride=1, downsample=None):\n        super(SEResNetBlock, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, padding=1, stride=stride, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride", "if neck_chans:\n            self.neck = nn.Sequential(\n                nn.Conv2d(\n                    embed_dim,\n                    neck_chans,\n                    kernel_size=1,\n                    bias=False,\n                ),\n                LayerNorm2d(neck_chans),\n                nn.Conv2d(\n                    neck_chans,\n                    neck_chans,\n                    kernel_size=3,\n                    padding=1,\n                    bias=False,\n                ),\n                LayerNorm2d(neck_chans),\n            )\n            self.num_features = neck_chans\n        else:\n            if head_hidden_size:\n                self.neck = nn.Identity()\n            else:\n                # should have a final norm with standard ClassifierHead\n                self.neck = LayerNorm2d(embed_dim)\n            neck_chans = embed_dim", "if proj:\n            self.proj = nn.Conv2d(\n                self.feature_dim,\n                embed_dim,\n                kernel_size=patch_size,\n                stride=patch_size,\n                bias=bias,\n            )\n        else:\n            assert self.feature_dim == embed_dim, \\\n                f'The feature dim ({self.feature_dim} must match embed dim ({embed_dim}) when projection disabled.'\n            self.proj = nn.Identity()", "def __init__(\n        self,\n        kernel_size: Tuple[int, ...] = (7, 7),\n        stride: Tuple[int, ...] = (4, 4),\n        padding: Tuple[int, ...] = (3, 3),\n        in_chans: int = 3,\n        embed_dim: int = 768,\n    ):\n        \"\"\"\n        Args:\n            kernel_size (Tuple): kernel size of the projection layer.\n            stride (Tuple): stride of the projection layer.\n            padding (Tuple): padding size of the projection layer.\n            in_chans (int): Number of input image channels.\n            embed_dim (int):  embed_dim (int): Patch embedding dimension.\n        \"\"\"\n        super().__init__()\n        self.proj = nn.Conv2d(\n            in_chans, embed_dim, kernel_size=kernel_size, stride=stride, padding=padding\n        )", "if qkv_fused:\n            self.qkv = nn.Linear(dim, attn_dim * 3, bias=False)\n            self.q_proj = self.k_proj = self.v_proj = None\n            if qkv_bias:\n                self.q_bias = nn.Parameter(torch.zeros(attn_dim))\n                self.register_buffer('k_bias', torch.zeros(attn_dim), persistent=False)\n                self.v_bias = nn.Parameter(torch.zeros(attn_dim))\n            else:\n                self.q_bias = self.k_bias = self.v_bias = None\n        else:\n            self.q_proj = nn.Linear(dim, attn_dim, bias=qkv_bias)\n            self.k_proj = nn.Linear(dim, attn_dim, bias=False)\n            self.v_proj = nn.Linear(dim, attn_dim, bias=qkv_bias)\n            self.qkv = None\n            self.q_bias = self.k_bias = self.v_bias = None\n        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.norm = norm_layer(attn_dim) if scale_norm else nn.Identity()\n        self.proj = nn.Linear(attn_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)", "self.q = nn.Linear(dim, dim, bias=qkv_bias)\n        self.k = nn.Linear(dim, dim, bias=qkv_bias)\n        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)", "self.conv = nn.Conv2d(\n            self.in_channels,\n            self.out_channels,\n            max(self.kernel_size_list),\n            self.stride,\n            groups=min(self.groups_list),\n            bias=False,\n        )", "def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):\n        self.num_classes = num_classes\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n        self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()", "def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size=3,\n        padding=1,\n        dilation=1,\n        *,\n        norm1=None,\n        activation1=None,\n        norm2=None,\n        activation2=None,\n    ):\n        \"\"\"\n        Args:\n            norm1, norm2 (str or callable): normalization for the two conv layers.\n            activation1, activation2 (callable(Tensor) -> Tensor): activation\n                function for the two conv layers.\n        \"\"\"\n        super().__init__()\n        self.depthwise = Conv2d(\n            in_channels,\n            in_channels,\n            kernel_size=kernel_size,\n            padding=padding,\n            dilation=dilation,\n            groups=in_channels,\n            bias=not norm1,\n            norm=get_norm(norm1, in_channels),\n            activation=activation1,\n        )\n        self.pointwise = Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=1,\n            bias=not norm2,\n            norm=get_norm(norm2, out_channels),\n            activation=activation2,\n        )", "self.merge = nn.Linear(self.head_dim * self.n_heads, feat_dim)\n        self.relative_position_bias_table = nn.parameter.Parameter(\n            torch.empty(((2 * self.size - 1) * (2 * self.size - 1), self.n_heads), dtype=torch.float32),\n        )", "def _init_nest_weights(module: nn.Module, name: str = '', head_bias: float = 0.):\n    \"\"\" NesT weight initialization\n    Can replicate Jax implementation. Otherwise follows vision_transformer.py\n    \"\"\"\n    if isinstance(module, nn.Linear):\n        if name.startswith('head'):\n            trunc_normal_(module.weight, std=.02, a=-2, b=2)\n            nn.init.constant_(module.bias, head_bias)\n        else:\n            trunc_normal_(module.weight, std=.02, a=-2, b=2)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Conv2d):\n        trunc_normal_(module.weight, std=.02, a=-2, b=2)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)", "for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            elif isinstance(m, (nn.BatchNorm2d, nn.InstanceNorm2d)):\n                if m.weight is not None:\n                    nn.init.constant_(m.weight, 1)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)", "if in_channels != out_channels:\n            self.shortcut = Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size=1,\n                stride=stride,\n                bias=False,\n                norm=get_norm(norm, out_channels),\n            )\n        else:\n            self.shortcut = None", "for i in range(self.n_stacked_convs):\n            norm_module = nn.GroupNorm(32, hidden_dim) if norm == \"GN\" else None\n            layer = Conv2d(\n                n_channels,\n                hidden_dim,\n                kernel_size,\n                stride=1,\n                padding=pad_size,\n                bias=not norm,\n                norm=norm_module,\n            )\n            weight_init.c2_msra_fill(layer)\n            n_channels = hidden_dim\n            layer_name = self._get_layer_name(i)\n            self.add_module(layer_name, layer)\n        self.n_out_channels = hidden_dim\n        # initialize_module_params(self)", "# Linear layer\n        self.classifier = nn.Linear(num_features, num_classes)", "self.mlp_drop = nn.Dropout(proj_drop)\n        self.mlp_act = act_layer()\n        self.mlp_out_proj = nn.Linear(mlp_hidden_dim, dim, bias=proj_bias)", "def _init_weights(module, name=None, head_init_scale=1.0):\n    if isinstance(module, nn.Conv2d):\n        nn.init.kaiming_normal_(module.weight)\n    elif isinstance(module, nn.BatchNorm2d):\n        nn.init.constant_(module.weight, 1)\n        nn.init.constant_(module.bias, 0)\n    elif isinstance(module, nn.Linear):\n        nn.init.constant_(module.bias, 0)\n        if name and 'head.' in name:\n            module.weight.data.mul_(head_init_scale)\n            module.bias.data.mul_(head_init_scale)", "See  as described in https://arxiv.org/abs/2010.11929.\n        \"\"\"\n        super().__init__()\n        self.q = nn.Linear(c, c, bias=False)\n        self.k = nn.Linear(c, c, bias=False)\n        self.v = nn.Linear(c, c, bias=False)\n        self.ma = nn.MultiheadAttention(embed_dim=c, num_heads=num_heads)\n        self.fc1 = nn.Linear(c, c, bias=False)\n        self.fc2 = nn.Linear(c, c, bias=False)", "# Classifier head\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n        self.head_dist = None\n        if distilled:\n            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n        self.distilled_training = False  # must set this True to train w/ distillation token", "class ConvNormAct(nn.Module):\n    def __init__(\n            self,\n            in_chs,\n            out_chs,\n            kernel_size=3,\n            stride=1,\n            groups=1,\n            norm_layer=nn.BatchNorm2d,\n            act_layer=nn.ReLU,\n    ):\n        super(ConvNormAct, self).__init__()\n        self.conv = nn.Conv2d(\n            in_chs, out_chs, kernel_size=kernel_size, stride=stride,\n            padding=1, groups=groups, bias=False)\n        self.norm = norm_layer(out_chs)\n        self.act = act_layer()", "from detectron2.config import configurable\nfrom detectron2.layers import Conv2d, ShapeSpec, cat, interpolate\nfrom detectron2.modeling import ROI_MASK_HEAD_REGISTRY\nfrom detectron2.modeling.roi_heads.mask_head import mask_rcnn_inference, mask_rcnn_loss\nfrom detectron2.structures import Boxes", "for m in self.modules():\n            if isinstance(m, (nn.Conv2d, nn.Linear)):\n                nn.init.trunc_normal_(m.weight, std=0.02)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)", "def __init__(self, *, input_size, hidden_size, kernel_size, padding):\n        super().__init__()\n        self.convz = nn.Conv2d(hidden_size + input_size, hidden_size, kernel_size=kernel_size, padding=padding)\n        self.convr = nn.Conv2d(hidden_size + input_size, hidden_size, kernel_size=kernel_size, padding=padding)\n        self.convq = nn.Conv2d(hidden_size + input_size, hidden_size, kernel_size=kernel_size, padding=padding)", "if isinstance(self.conv_proj, nn.Conv2d):\n            # Init the patchify stem\n            fan_in = self.conv_proj.in_channels * self.conv_proj.kernel_size[0] * self.conv_proj.kernel_size[1]\n            nn.init.trunc_normal_(self.conv_proj.weight, std=math.sqrt(1 / fan_in))\n            if self.conv_proj.bias is not None:\n                nn.init.zeros_(self.conv_proj.bias)\n        elif self.conv_proj.conv_last is not None and isinstance(self.conv_proj.conv_last, nn.Conv2d):\n            # Init the last 1x1 conv of the conv stem\n            nn.init.normal_(\n                self.conv_proj.conv_last.weight, mean=0.0, std=math.sqrt(2.0 / self.conv_proj.conv_last.out_channels)\n            )\n            if self.conv_proj.conv_last.bias is not None:\n                nn.init.zeros_(self.conv_proj.conv_last.bias)", "class Downsample2d(nn.Module):\n    def __init__(self, input_dim, output_dim, patch_size):\n        super().__init__()\n        self.down = nn.Conv2d(input_dim, output_dim, kernel_size=patch_size, stride=patch_size)", "self.conv2d = nn.Conv2d(128, 320, kernel_size=1, stride=1)\n        self.act = nn.ReLU()", "# Official init from torch repo.\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.constant_(m.bias, 0)", "def _xavier_init(conv: nn.Module):\n    for layer in conv.modules():\n        if isinstance(layer, nn.Conv2d):\n            torch.nn.init.xavier_uniform_(layer.weight)\n            if layer.bias is not None:\n                torch.nn.init.constant_(layer.bias, 0.0)", "self.qkv = nn.Linear(embed_dim, 3 * output_dim)\n        layers: list[nn.Module] = [nn.Linear(output_dim, output_dim)]\n        if dropout > 0.0:\n            layers.append(nn.Dropout(dropout, inplace=True))\n        self.project = nn.Sequential(*layers)", "self.conv2 = nn.Conv2d(\n            first_planes, outplanes, kernel_size=3, padding=dilation, dilation=dilation, bias=False)\n        self.bn2 = norm_layer(outplanes)", "class Conv2dSameExport(nn.Conv2d):\n    \"\"\" ONNX export friendly Tensorflow like 'SAME' convolution wrapper for 2D convolutions", "self.conv3 = nn.Conv2d(mid_planes * scale, outplanes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(outplanes)\n        self.relu = nn.ReLU(inplace=True)", "def set_input_size(\n            self,\n            img_size: Optional[Union[int, Tuple[int, int]]] = None,\n            patch_size: Optional[Union[int, Tuple[int, int]]] = None,\n    ):\n        new_patch_size = None\n        if patch_size is not None:\n            new_patch_size = to_2tuple(patch_size)\n        if new_patch_size is not None and new_patch_size != self.patch_size:\n            with torch.no_grad():\n                new_proj = nn.Conv2d(\n                    self.proj.in_channels,\n                    self.proj.out_channels,\n                    kernel_size=new_patch_size,\n                    stride=new_patch_size,\n                    bias=self.proj.bias is not None,\n                )\n                new_proj.weight.copy_(resample_patch_embed(self.proj.weight, new_patch_size, verbose=True))\n                if self.proj.bias is not None:\n                    new_proj.bias.copy_(self.proj.bias)\n                self.proj = new_proj\n            self.patch_size = new_patch_size\n        img_size = img_size or self.img_size\n        if img_size != self.img_size or new_patch_size is not None:\n            self.img_size, self.grid_size, self.num_patches = self._init_img_size(img_size)", "# Stem\n        deep_stem = 'deep' in stem_type  # 3x3 deep stem\n        num_init_features = growth_rate * 2\n        if aa_layer is None:\n            stem_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        else:\n            stem_pool = nn.Sequential(*[\n                nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n                aa_layer(channels=num_init_features, stride=2)])\n        if deep_stem:\n            stem_chs_1 = stem_chs_2 = growth_rate\n            if 'tiered' in stem_type:\n                stem_chs_1 = 3 * (growth_rate // 4)\n                stem_chs_2 = num_init_features if 'narrow' in stem_type else 6 * (growth_rate // 4)\n            self.features = nn.Sequential(OrderedDict([\n                ('conv0', nn.Conv2d(in_chans, stem_chs_1, 3, stride=2, padding=1, bias=False)),\n                ('norm0', norm_layer(stem_chs_1)),\n                ('conv1', nn.Conv2d(stem_chs_1, stem_chs_2, 3, stride=1, padding=1, bias=False)),\n                ('norm1', norm_layer(stem_chs_2)),\n                ('conv2', nn.Conv2d(stem_chs_2, num_init_features, 3, stride=1, padding=1, bias=False)),\n                ('norm2', norm_layer(num_init_features)),\n                ('pool0', stem_pool),\n            ]))\n        else:\n            self.features = nn.Sequential(OrderedDict([\n                ('conv0', nn.Conv2d(in_chans, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n                ('norm0', norm_layer(num_init_features)),\n                ('pool0', stem_pool),\n            ]))\n        self.feature_info = [\n            dict(num_chs=num_init_features, reduction=2, module=f'features.norm{2 if deep_stem else 0}')]\n        current_stride = 4", "class DeepLabHead(nn.Sequential):\n    def __init__(self, in_channels: int, num_classes: int, atrous_rates: Sequence[int] = (12, 24, 36)) -> None:\n        super().__init__(\n            ASPP(in_channels, atrous_rates),\n            nn.Conv2d(256, 256, 3, padding=1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.Conv2d(256, num_classes, 1),\n        )", "See https://tehnokv.com/posts/fusing-batchnorm-and-conv/.\n    \"\"\"\n    fusedconv = (\n        nn.Conv2d(\n            conv.in_channels,\n            conv.out_channels,\n            kernel_size=conv.kernel_size,\n            stride=conv.stride,\n            padding=conv.padding,\n            dilation=conv.dilation,\n            groups=conv.groups,\n            bias=True,\n        )\n        .requires_grad_(False)\n        .to(conv.weight.device)\n    )", "# mask subnet\n        if self.mask_on:\n            mask_subnet = []\n            cur_channels = in_channels\n            for _ in range(num_convs):\n                mask_subnet.append(\n                    nn.Conv2d(cur_channels, mask_channels, kernel_size=3, stride=1, padding=1)\n                )\n                cur_channels = mask_channels\n                mask_subnet.append(nn.ReLU())", "self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim)", "self.linear = nn.Linear(self.max_in_features, self.max_out_features, self.bias)", "def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\n        \"\"\"Initializes a standard convolution layer with optional batch normalization and activation.\"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)\n        self.bn = nn.BatchNorm2d(c2)\n        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()", "class MlpWithDepthwiseConv(nn.Module):\n    def __init__(\n            self,\n            in_features,\n            hidden_features=None,\n            out_features=None,\n            act_layer=nn.GELU,\n            drop=0.,\n            extra_relu=False,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.relu = nn.ReLU() if extra_relu else nn.Identity()\n        self.dwconv = nn.Conv2d(hidden_features, hidden_features, 3, 1, 1, bias=True, groups=hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)", "in_channels_list = [backbone[stage_indices[i]].out_channels for i in returned_layers]\n        return BackboneWithFPN(\n            backbone, return_layers, in_channels_list, out_channels, extra_blocks=extra_blocks, norm_layer=norm_layer\n        )\n    else:\n        m = nn.Sequential(\n            backbone,\n            # depthwise linear combination of channels to reduce their size\n            nn.Conv2d(backbone[-1].out_channels, out_channels, 1),\n        )\n        m.out_channels = out_channels  # type: ignore[assignment]\n        return m", "# shortcut\n        if in_chs == out_chs and self.stride == 1:\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(\n                    in_chs, in_chs, dw_kernel_size, stride=stride,\n                    padding=(dw_kernel_size-1)//2, groups=in_chs, bias=False),\n                nn.BatchNorm2d(in_chs),\n                nn.Conv2d(in_chs, out_chs, 1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_chs),\n            )", "blocks = []\n        previous_channels = in_channels\n        for current_channels in conv_layers:\n            blocks.append(misc_nn_ops.Conv2dNormActivation(previous_channels, current_channels, norm_layer=norm_layer))\n            previous_channels = current_channels\n        blocks.append(nn.Flatten())\n        previous_channels = previous_channels * in_height * in_width\n        for current_channels in fc_layers:\n            blocks.append(nn.Linear(previous_channels, current_channels))\n            blocks.append(nn.ReLU(inplace=True))\n            previous_channels = current_channels", "class LRASPPHead(nn.Module):\n    def __init__(self, low_channels: int, high_channels: int, num_classes: int, inter_channels: int) -> None:\n        super().__init__()\n        self.cbr = nn.Sequential(\n            nn.Conv2d(high_channels, inter_channels, 1, bias=False),\n            nn.BatchNorm2d(inter_channels),\n            nn.ReLU(inplace=True),\n        )\n        self.scale = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(high_channels, inter_channels, 1, bias=False),\n            nn.Sigmoid(),\n        )\n        self.low_classifier = nn.Conv2d(low_channels, num_classes, 1)\n        self.high_classifier = nn.Conv2d(inter_channels, num_classes, 1)", "self.base_layer = nn.Sequential(\n            nn.Conv2d(in_chans, channels[0], kernel_size=7, stride=1, padding=3, bias=False),\n            nn.BatchNorm2d(channels[0]),\n            nn.ReLU(inplace=True),\n        )\n        self.level0 = self._make_conv_level(channels[0], channels[0], levels[0])\n        self.level1 = self._make_conv_level(channels[0], channels[1], levels[1], stride=2)\n        cargs = dict(cardinality=cardinality, base_width=base_width, root_shortcut=shortcut_root)\n        self.level2 = DlaTree(levels[2], block, channels[1], channels[2], 2, level_root=False, **cargs)\n        self.level3 = DlaTree(levels[3], block, channels[2], channels[3], 2, level_root=True, **cargs)\n        self.level4 = DlaTree(levels[4], block, channels[3], channels[4], 2, level_root=True, **cargs)\n        self.level5 = DlaTree(levels[5], block, channels[4], channels[5], 2, level_root=True, **cargs)\n        self.feature_info = [\n            dict(num_chs=channels[0], reduction=1, module='level0'),  # rare to have a meaningful stride 1 level\n            dict(num_chs=channels[1], reduction=2, module='level1'),\n            dict(num_chs=channels[2], reduction=4, module='level2'),\n            dict(num_chs=channels[3], reduction=8, module='level3'),\n            dict(num_chs=channels[4], reduction=16, module='level4'),\n            dict(num_chs=channels[5], reduction=32, module='level5'),\n        ]", "class ASPP(nn.Module):\n    def __init__(self, in_channels, atrous_rates, out_channels):\n        super(ASPP, self).__init__()\n        modules = []\n        modules.append(\n            nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, 1, bias=False),\n                nn.GroupNorm(32, out_channels),\n                nn.ReLU(),\n            )\n        )", "self.qkv = nn.Linear(dim, dim * 3, bias=True)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.ws = ws", "# box subnet\n        bbox_subnet = []\n        cur_channels = in_channels\n        for _ in range(num_convs):\n            bbox_subnet.append(\n                nn.Conv2d(cur_channels, bbox_channels, kernel_size=3, stride=1, padding=1)\n            )\n            cur_channels = bbox_channels\n            bbox_subnet.append(nn.ReLU())", "pad = get_same_padding(self.kernel_size)\n        groups = (\n            feature_dim\n            if self.groups is None\n            else min_divisible_value(feature_dim, self.groups)\n        )\n        depth_conv_modules = [\n            (\n                \"conv\",\n                nn.Conv2d(\n                    feature_dim,\n                    feature_dim,\n                    kernel_size,\n                    stride,\n                    pad,\n                    groups=groups,\n                    bias=False,\n                ),\n            ),\n            (\"bn\", nn.BatchNorm2d(feature_dim)),\n            (\"act\", build_activation(self.act_func, inplace=True)),\n        ]\n        if self.use_se:\n            depth_conv_modules.append((\"se\", SEModule(feature_dim)))\n        self.depth_conv = nn.Sequential(OrderedDict(depth_conv_modules))", "self.project = nn.Sequential(\n            nn.Conv2d(len(self.convs) * out_channels, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n        )", "Handles layers in EfficientNet, EfficientNet-CondConv, MixNet, MnasNet, MobileNetV3, etc:\n    * https://github.com/tensorflow/tpu/blob/master/models/official/mnasnet/mnasnet_model.py\n    * https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/efficientnet_model.py\n    \"\"\"\n    if isinstance(m, CondConv2d):\n        fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        if fix_group_fanout:\n            fan_out //= m.groups\n        init_weight_fn = get_condconv_initializer(\n            lambda w: nn.init.normal_(w, 0, math.sqrt(2.0 / fan_out)), m.num_experts, m.weight_shape)\n        init_weight_fn(m.weight)\n        if m.bias is not None:\n            nn.init.zeros_(m.bias)\n    elif isinstance(m, nn.Conv2d):\n        fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        if fix_group_fanout:\n            fan_out //= m.groups\n        nn.init.normal_(m.weight, 0, math.sqrt(2.0 / fan_out))\n        if m.bias is not None:\n            nn.init.zeros_(m.bias)\n    elif isinstance(m, nn.BatchNorm2d):\n        nn.init.ones_(m.weight)\n        nn.init.zeros_(m.bias)\n    elif isinstance(m, nn.Linear):\n        fan_out = m.weight.size(0)  # fan-out\n        fan_in = 0\n        if 'routing_fn' in n:\n            fan_in = m.weight.size(1)\n        init_range = 1.0 / math.sqrt(fan_in + fan_out)\n        nn.init.uniform_(m.weight, -init_range, init_range)\n        nn.init.zeros_(m.bias)", "def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.trunc_normal_(m.weight, std=0.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)", "lateral_conv = Conv2d(\n                in_channels, out_channels, kernel_size=1, bias=use_bias, norm=lateral_norm\n            )\n            output_conv = Conv2d(\n                out_channels,\n                out_channels,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n                bias=use_bias,\n                norm=output_norm,\n            )\n            weight_init.c2_xavier_fill(lateral_conv)\n            weight_init.c2_xavier_fill(output_conv)\n            stage = int(math.log2(strides[idx]))\n            self.add_module(\"fpn_lateral{}\".format(stage), lateral_conv)\n            self.add_module(\"fpn_output{}\".format(stage), output_conv)", "class LocalGlobalQuery(torch.nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.pool = nn.AvgPool2d(1, 2, 0)\n        self.local = nn.Conv2d(in_dim, in_dim, kernel_size=3, stride=2, padding=1, groups=in_dim)\n        self.proj = ConvNorm(in_dim, out_dim, 1)", "self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim)", "self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.pos_proj = nn.Linear(3, num_heads)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.gating_param = nn.Parameter(torch.ones(self.num_heads))\n        self.rel_indices: torch.Tensor = torch.zeros(1, 1, 1, 3)  # silly torchscript hack, won't work with None", "if qkv_fused:\n            self.qkv = nn.Linear(dim, attn_dim * 3, bias=qkv_bias)\n            self.q_proj = self.k_proj = self.v_proj = None\n        else:\n            self.qkv = None\n            self.q_proj = nn.Linear(dim, attn_dim, bias=qkv_bias)\n            self.k_proj = nn.Linear(dim, attn_dim, bias=qkv_bias)\n            self.v_proj = nn.Linear(dim, attn_dim, bias=qkv_bias)", "self.conv3 = nn.Conv2d(width * scale, outplanes, kernel_size=1, bias=False)\n        self.bn3 = norm_layer(outplanes)\n        self.se = attn_layer(outplanes) if attn_layer is not None else None", "self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(all_head_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)", "def __init__(\n            self,\n            in_chs=96,\n            out_chs=198,\n            norm_layer=LayerNorm,\n    ):\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_chs,\n            out_chs,\n            kernel_size=3,\n            stride=2,\n            padding=1\n        )\n        self.norm = norm_layer(out_chs)", "self.conv3 = Conv2d(\n            bottleneck_channels,\n            out_channels,\n            kernel_size=1,\n            bias=False,\n            norm=get_norm(norm, out_channels),\n        )", "def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            assert global_pool in ('', 'token', 'avg')\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()", "# initialize parameters now to avoid modifying the initialization of top_blocks\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_uniform_(m.weight, a=1)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)", "if inference_mode:\n            self.reparam_conv = nn.Conv2d(\n                in_channels=in_chs,\n                out_channels=out_chs,\n                kernel_size=kernel_size,\n                stride=stride,\n                dilation=dilation,\n                groups=groups,\n                bias=True)\n        else:\n            self.reparam_conv = None", "self.qkv = nn.Linear(dim, self.attention_dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(self.attention_dim, dim, bias=proj_bias)\n        self.proj_drop = nn.Dropout(proj_drop)", "assert pool_type, 'Cannot disable pooling'\n        self.in_conv = ConvNormAct(in_channels, widths[0], 1, norm_layer=norm_layer, act_layer=act_layer)\n        self.global_pool = SelectAdaptivePool2d(pool_type=pool_type, flatten=True)\n        self.classifier = nn.Sequential(\n            nn.Linear(widths[0], widths[1], bias=False),\n            nn.LayerNorm(widths[1], eps=norm_eps),\n            act_layer(inplace=True) if act_layer is not None else nn.Identity(),\n            nn.Dropout(dropout, inplace=False),\n            nn.Linear(widths[1], num_classes, bias=True) if num_classes > 0 else nn.Identity(),\n        )", "self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n        if norm_layer is not None:\n            self.norm = norm_layer(embed_dim)\n        else:\n            self.norm = None", "self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)", "# Implementation of Feedforward model\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)", "class _InvertedResidual(nn.Module):\n    def __init__(\n        self, in_ch: int, out_ch: int, kernel_size: int, stride: int, expansion_factor: int, bn_momentum: float = 0.1\n    ) -> None:\n        super().__init__()\n        if stride not in [1, 2]:\n            raise ValueError(f\"stride should be 1 or 2 instead of {stride}\")\n        if kernel_size not in [3, 5]:\n            raise ValueError(f\"kernel_size should be 3 or 5 instead of {kernel_size}\")\n        mid_ch = in_ch * expansion_factor\n        self.apply_residual = in_ch == out_ch and stride == 1\n        self.layers = nn.Sequential(\n            # Pointwise\n            nn.Conv2d(in_ch, mid_ch, 1, bias=False),\n            nn.BatchNorm2d(mid_ch, momentum=bn_momentum),\n            nn.ReLU(inplace=True),\n            # Depthwise\n            nn.Conv2d(mid_ch, mid_ch, kernel_size, padding=kernel_size // 2, stride=stride, groups=mid_ch, bias=False),\n            nn.BatchNorm2d(mid_ch, momentum=bn_momentum),\n            nn.ReLU(inplace=True),\n            # Linear pointwise. Note that there's no activation.\n            nn.Conv2d(mid_ch, out_ch, 1, bias=False),\n            nn.BatchNorm2d(out_ch, momentum=bn_momentum),\n        )", "def test_param_groups_layer_decay_with_matcher():\n    class ModelWithMatcher(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.layer1 = torch.nn.Linear(10, 5)\n            self.layer2 = torch.nn.Linear(5, 2)", "def _init_weights(module, name=None, head_init_scale=1.0):\n    if isinstance(module, nn.Conv2d):\n        trunc_normal_tf_(module.weight, std=.02)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Linear):\n        trunc_normal_tf_(module.weight, std=.02)\n        nn.init.zeros_(module.bias)\n        if name and 'head.' in name:\n            module.weight.data.mul_(head_init_scale)\n            module.bias.data.mul_(head_init_scale)", "def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n        if self.aux_head is not None:\n            self.aux_head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()", "# classification head\n        self.num_features = self.head_hidden_size = embed_dims[-1]\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(embed_dims[-1], num_classes) if num_classes > 0 else nn.Identity()", "if qkv_separate:\n            self.q = nn.Linear(in_features, embed_dim, bias=qkv_bias)\n            self.k = nn.Linear(in_features, embed_dim, bias=qkv_bias)\n            self.v = nn.Linear(in_features, embed_dim, bias=qkv_bias)\n            self.qkv = None\n        else:\n            self.qkv = nn.Linear(in_features, embed_dim * 3, bias=qkv_bias)\n        self.drop = nn.Dropout(drop_rate)\n        self.proj = nn.Linear(embed_dim, self.out_features)\n        self.pos_embed = RotaryEmbedding(self.head_dim, in_pixels=False, ref_feat_shape=ref_feat_size)", "for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)", "self.conv_list = nn.ModuleList()\n        self.head_splits = []\n        for cur_window, cur_head_split in window.items():\n            dilation = 1\n            # Determine padding size.\n            # Ref: https://discuss.pytorch.org/t/how-to-keep-the-shape-of-input-and-output-same-when-dilation-conv/14338\n            padding_size = (cur_window + (cur_window - 1) * (dilation - 1)) // 2\n            cur_conv = nn.Conv2d(\n                cur_head_split * head_chs,\n                cur_head_split * head_chs,\n                kernel_size=(cur_window, cur_window), \n                padding=(padding_size, padding_size),\n                dilation=(dilation, dilation),                          \n                groups=cur_head_split * head_chs,\n            )\n            self.conv_list.append(cur_conv)\n            self.head_splits.append(cur_head_split)\n        self.channel_splits = [x * head_chs for x in self.head_splits]", "# multi-head attention\n        self.query_proj = nn.Linear(dim_model, dim_model, bias=False)\n        self.key_proj = nn.Linear(dim_model, dim_model, bias=False)\n        self.value_proj = nn.Linear(dim_model, dim_model, bias=False)\n        self.merge = nn.Linear(dim_model, dim_model, bias=False)", "# #------- init weights --------\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()", "def _initialize_weights(self):\n        for n, m in self.named_modules():\n            if isinstance(m, nn.Linear):\n                trunc_normal_(m.weight, std=.02)\n                if hasattr(m, 'bias') and m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Conv2d):\n                trunc_normal_(m.weight, std=.02)\n                if hasattr(m, 'bias') and m.bias is not None:\n                    nn.init.constant_(m.bias, 0)", "self.conv = nn.Conv2d(\n            self.max_in_channels,\n            self.max_in_channels,\n            max(self.kernel_size_list),\n            self.stride,\n            groups=self.max_in_channels,\n            bias=False,\n        )", "def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            assert global_pool in ('', 'token', 'avg')\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()", "class ConvNormActivation(torch.nn.Sequential):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: Union[int, tuple[int, ...]] = 3,\n        stride: Union[int, tuple[int, ...]] = 1,\n        padding: Optional[Union[int, tuple[int, ...], str]] = None,\n        groups: int = 1,\n        norm_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.BatchNorm2d,\n        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n        dilation: Union[int, tuple[int, ...]] = 1,\n        inplace: Optional[bool] = True,\n        bias: Optional[bool] = None,\n        conv_layer: Callable[..., torch.nn.Module] = torch.nn.Conv2d,\n    ) -> None:", "self.kv = nn.Linear(dim, self.head_dim * self.num_heads * 2, bias=qkv_bias)\n        self.q = nn.Linear(dim, self.head_dim * self.num_heads, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(self.head_dim * self.num_heads, dim)\n        self.proj_drop = nn.Dropout(proj_drop)", "# Keeping the order of weights initialization same for backwards compatiblility.\n        for layer in self.modules():\n            if isinstance(layer, nn.Conv2d):\n                nn.init.normal_(layer.weight, std=0.01)\n                nn.init.constant_(layer.bias, 0)", "self.branch2 = nn.Sequential(\n            nn.Conv2d(\n                inp if (self.stride > 1) else branch_features,\n                branch_features,\n                kernel_size=1,\n                stride=1,\n                padding=0,\n                bias=False,\n            ),\n            nn.BatchNorm2d(branch_features),\n            nn.ReLU(inplace=True),\n            self.depthwise_conv(branch_features, branch_features, kernel_size=3, stride=self.stride, padding=1),\n            nn.BatchNorm2d(branch_features),\n            nn.Conv2d(branch_features, branch_features, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(branch_features),\n            nn.ReLU(inplace=True),\n        )", "if dim != dim_out:\n            self.proj = nn.Linear(dim, dim_out)\n        else:\n            self.proj = nn.Identity()\n        self.pool = None\n        if self.q_stride:\n            # note make a different instance for this Module so that it's not shared with attn module\n            self.pool = nn.MaxPool2d(\n                kernel_size=q_stride,\n                stride=q_stride,\n                ceil_mode=False,\n            )", "self.mask_subnet = nn.Sequential(*mask_subnet)\n            modules_list.append(self.mask_subnet)\n            for mask_size in self.mask_sizes:\n                cur_mask_module = \"mask_pred_%02d\" % mask_size\n                self.add_module(\n                    cur_mask_module,\n                    nn.Conv2d(\n                        cur_channels, mask_size * mask_size, kernel_size=1, stride=1, padding=0\n                    ),\n                )\n                modules_list.append(getattr(self, cur_mask_module))\n            if self.align_on:\n                if self.bipyramid_on:\n                    for lvl in range(num_levels):\n                        cur_mask_module = \"align2nat_%02d\" % lvl\n                        lambda_val = 2**lvl\n                        setattr(self, cur_mask_module, SwapAlign2Nat(lambda_val))\n                    # Also the fusing layer, stay at the same channel size\n                    mask_fuse = [\n                        nn.Conv2d(cur_channels, cur_channels, kernel_size=3, stride=1, padding=1),\n                        nn.ReLU(),\n                    ]\n                    self.mask_fuse = nn.Sequential(*mask_fuse)\n                    modules_list.append(self.mask_fuse)\n                else:\n                    self.align2nat = SwapAlign2Nat(1)", "for layer in self.conv.modules():\n            if isinstance(layer, nn.Conv2d):\n                torch.nn.init.normal_(layer.weight, std=0.01)\n                if layer.bias is not None:\n                    torch.nn.init.zeros_(layer.bias)", "new_module = deepcopy(parent_module)\n    for n, m in parent_module.named_modules():\n        old_module = extract_layer(parent_module, n)\n        if isinstance(old_module, nn.Conv2d) or isinstance(old_module, Conv2dSame):\n            if isinstance(old_module, Conv2dSame):\n                conv = Conv2dSame\n            else:\n                conv = nn.Conv2d\n            s = state_dict[n + '.weight']\n            in_channels = s[1]\n            out_channels = s[0]\n            g = 1\n            if old_module.groups > 1:\n                in_channels = out_channels\n                g = in_channels\n            new_conv = conv(\n                in_channels=in_channels, out_channels=out_channels, kernel_size=old_module.kernel_size,\n                bias=old_module.bias is not None, padding=old_module.padding, dilation=old_module.dilation,\n                groups=g, stride=old_module.stride)\n            set_layer(new_module, n, new_conv)\n        elif isinstance(old_module, BatchNormAct2d):\n            new_bn = BatchNormAct2d(\n                state_dict[n + '.weight'][0], eps=old_module.eps, momentum=old_module.momentum,\n                affine=old_module.affine, track_running_stats=True)\n            new_bn.drop = old_module.drop\n            new_bn.act = old_module.act\n            set_layer(new_module, n, new_bn)\n        elif isinstance(old_module, nn.BatchNorm2d):\n            new_bn = nn.BatchNorm2d(\n                num_features=state_dict[n + '.weight'][0], eps=old_module.eps, momentum=old_module.momentum,\n                affine=old_module.affine, track_running_stats=True)\n            set_layer(new_module, n, new_bn)\n        elif isinstance(old_module, nn.Linear):\n            # FIXME extra checks to ensure this is actually the FC classifier layer and not a diff Linear layer?\n            num_features = state_dict[n + '.weight'][1]\n            new_fc = Linear(\n                in_features=num_features, out_features=old_module.out_features, bias=old_module.bias is not None)\n            set_layer(new_module, n, new_fc)\n            if hasattr(new_module, 'num_features'):\n                if getattr(new_module, 'head_hidden_size', 0) == new_module.num_features:\n                    new_module.head_hidden_size = num_features\n                new_module.num_features = num_features", "def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            # NOTE: cannot meaningfully change pooling of efficient head after creation\n            self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n            self.flatten = nn.Flatten(1) if global_pool else nn.Identity()  # don't flatten if pooling disabled\n        self.classifier = Linear(self.head_hidden_size, num_classes) if num_classes > 0 else nn.Identity()", "self.norm1 = norm_layer(dim) if norm_layer is not None else nn.Identity()\n        self.conv_block = MbConvBlock(dim, act_layer=act_layer)\n        assert reduction in ('conv', 'max', 'avg')\n        if reduction == 'conv':\n            self.reduction = nn.Conv2d(dim, dim_out, 3, 2, 1, bias=False)\n        elif reduction == 'max':\n            assert dim == dim_out\n            self.reduction = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        else:\n            assert dim == dim_out\n            self.reduction = nn.AvgPool2d(kernel_size=2)\n        self.norm2 = norm_layer(dim_out) if norm_layer is not None else nn.Identity()", "# Skip pooling with kernel and stride size of (1, 1, 1).\n        if prod(kernel_q) == 1 and prod(stride_q) == 1:\n            kernel_q = None\n        if prod(kernel_kv) == 1 and prod(stride_kv) == 1:\n            kernel_kv = None\n        self.mode = mode\n        self.unshared = mode == 'conv_unshared'\n        self.norm_q, self.norm_k, self.norm_v = None, None, None\n        self.pool_q, self.pool_k, self.pool_v = None, None, None\n        if mode in (\"avg\", \"max\"):\n            pool_op = nn.MaxPool2d if mode == \"max\" else nn.AvgPool2d\n            if kernel_q:\n                self.pool_q = pool_op(kernel_q, stride_q, padding_q)\n            if kernel_kv:\n                self.pool_k = pool_op(kernel_kv, stride_kv, padding_kv)\n                self.pool_v = pool_op(kernel_kv, stride_kv, padding_kv)\n        elif mode == \"conv\" or mode == \"conv_unshared\":\n            dim_conv = dim_out // num_heads if mode == \"conv\" else dim_out\n            if kernel_q:\n                self.pool_q = nn.Conv2d(\n                    dim_conv,\n                    dim_conv,\n                    kernel_q,\n                    stride=stride_q,\n                    padding=padding_q,\n                    groups=dim_conv,\n                    bias=False,\n                )\n                self.norm_q = norm_layer(dim_conv)\n            if kernel_kv:\n                self.pool_k = nn.Conv2d(\n                    dim_conv,\n                    dim_conv,\n                    kernel_kv,\n                    stride=stride_kv,\n                    padding=padding_kv,\n                    groups=dim_conv,\n                    bias=False,\n                )\n                self.norm_k = norm_layer(dim_conv)\n                self.pool_v = nn.Conv2d(\n                    dim_conv,\n                    dim_conv,\n                    kernel_kv,\n                    stride=stride_kv,\n                    padding=padding_kv,\n                    groups=dim_conv,\n                    bias=False,\n                )\n                self.norm_v = norm_layer(dim_conv)\n        else:\n            raise NotImplementedError(f\"Unsupported model {mode}\")", "return nn.Sequential(*[\n        nn.Conv2d(\n            in_channels, out_channels, kernel_size, stride=stride, padding=p, dilation=first_dilation, bias=False),\n        norm_layer(out_channels)\n    ])", "def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            assert global_pool in ('', 'avg')\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()", "# downsampling modules\n        downsamp_modules = []\n        for i in range(len(pre_stage_channels) - 1):\n            in_channels = self.head_channels[i] * head_block_type.expansion\n            out_channels = self.head_channels[i + 1] * head_block_type.expansion\n            downsamp_module = nn.Sequential(\n                nn.Conv2d(\n                    in_channels=in_channels, out_channels=out_channels,\n                    kernel_size=3, stride=2, padding=1, bias=conv_bias),\n                nn.BatchNorm2d(out_channels, momentum=_BN_MOMENTUM),\n                nn.ReLU(inplace=True)\n            )\n            downsamp_modules.append(downsamp_module)\n        downsamp_modules = nn.ModuleList(downsamp_modules)", "self.conv1 = Conv2d(\n            in_channels,\n            bottleneck_channels,\n            kernel_size=1,\n            stride=stride_1x1,\n            bias=False,\n            norm=get_norm(norm, bottleneck_channels),\n        )", "from detectron2.config import configurable\nfrom detectron2.layers import Conv2d, ShapeSpec, get_norm\nfrom detectron2.utils.registry import Registry", "# NOTE timm.models.layers is DEPRECATED, please use timm.layers, this is here to reduce breakages in transition\nfrom timm.layers.activations import *\nfrom timm.layers.adaptive_avgmax_pool import \\\n    adaptive_avgmax_pool2d, select_adaptive_pool2d, AdaptiveAvgMaxPool2d, SelectAdaptivePool2d\nfrom timm.layers.attention_pool2d import AttentionPool2d, RotAttentionPool2d, RotaryEmbedding\nfrom timm.layers.blur_pool import BlurPool2d\nfrom timm.layers.classifier import ClassifierHead, create_classifier\nfrom timm.layers.cond_conv2d import CondConv2d, get_condconv_initializer\nfrom timm.layers.config import is_exportable, is_scriptable, is_no_jit, set_exportable, set_scriptable, set_no_jit,\\\n    set_layer_config\nfrom timm.layers.conv2d_same import Conv2dSame, conv2d_same\nfrom timm.layers.conv_bn_act import ConvNormAct, ConvNormActAa, ConvBnAct\nfrom timm.layers.create_act import create_act_layer, get_act_layer, get_act_fn\nfrom timm.layers.create_attn import get_attn, create_attn\nfrom timm.layers.create_conv2d import create_conv2d\nfrom timm.layers.create_norm import get_norm_layer, create_norm_layer\nfrom timm.layers.create_norm_act import get_norm_act_layer, create_norm_act_layer, get_norm_act_layer\nfrom timm.layers.drop import DropBlock2d, DropPath, drop_block_2d, drop_path\nfrom timm.layers.eca import EcaModule, CecaModule, EfficientChannelAttn, CircularEfficientChannelAttn\nfrom timm.layers.evo_norm import EvoNorm2dB0, EvoNorm2dB1, EvoNorm2dB2,\\\n    EvoNorm2dS0, EvoNorm2dS0a, EvoNorm2dS1, EvoNorm2dS1a, EvoNorm2dS2, EvoNorm2dS2a\nfrom timm.layers.fast_norm import is_fast_norm, set_fast_norm, fast_group_norm, fast_layer_norm\nfrom timm.layers.filter_response_norm import FilterResponseNormTlu2d, FilterResponseNormAct2d\nfrom timm.layers.gather_excite import GatherExcite\nfrom timm.layers.global_context import GlobalContext\nfrom timm.layers.helpers import to_ntuple, to_2tuple, to_3tuple, to_4tuple, make_divisible, extend_tuple\nfrom timm.layers.inplace_abn import InplaceAbn\nfrom timm.layers.linear import Linear\nfrom timm.layers.mixed_conv2d import MixedConv2d\nfrom timm.layers.mlp import Mlp, GluMlp, GatedMlp, ConvMlp\nfrom timm.layers.non_local_attn import NonLocalAttn, BatNonLocalAttn\nfrom timm.layers.norm import GroupNorm, GroupNorm1, LayerNorm, LayerNorm2d\nfrom timm.layers.norm_act import BatchNormAct2d, GroupNormAct, convert_sync_batchnorm\nfrom timm.layers.padding import get_padding, get_same_padding, pad_same\nfrom timm.layers.patch_embed import PatchEmbed\nfrom timm.layers.pool2d_same import AvgPool2dSame, create_pool2d\nfrom timm.layers.squeeze_excite import SEModule, SqueezeExcite, EffectiveSEModule, EffectiveSqueezeExcite\nfrom timm.layers.selective_kernel import SelectiveKernel\nfrom timm.layers.separable_conv import SeparableConv2d, SeparableConvNormAct\nfrom timm.layers.split_attn import SplitAttn\nfrom timm.layers.split_batchnorm import SplitBatchNorm2d, convert_splitbn_model\nfrom timm.layers.std_conv import StdConv2d, StdConv2dSame, ScaledStdConv2d, ScaledStdConv2dSame\nfrom timm.layers.test_time_pool import TestTimePoolHead, apply_test_time_pool\nfrom timm.layers.trace_utils import _assert, _float_to_int\nfrom timm.layers.weight_init import trunc_normal_, trunc_normal_tf_, variance_scaling_, lecun_normal_", "if global_pool == 'map':\n            attn_pool_num_heads = attn_pool_num_heads or num_heads\n            attn_pool_mlp_ratio = attn_pool_mlp_ratio or mlp_ratio\n            self.attn_pool = AttentionPoolLatent(\n                self.embed_dim,\n                num_heads=attn_pool_num_heads,\n                mlp_ratio=attn_pool_mlp_ratio,\n                norm_layer=norm_layer,\n                act_layer=nn.GELU,\n            )\n        else:\n            self.attn_pool = None\n        self.fc_norm = norm_layer(embed_dim) if activate_fc_norm else nn.Identity()\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()", "class NormLinear(nn.Sequential):\n    def __init__(self, in_dim, out_dim, bias=True, std=0.02):\n        super().__init__()\n        self.add_module('bn', nn.BatchNorm1d(in_dim))\n        self.add_module('l', nn.Linear(in_dim, out_dim, bias=bias))\n        trunc_normal_(self.l.weight, std=std)\n        if bias:\n            nn.init.constant_(self.l.bias, 0)", "def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):\n        \"\"\"Initializes CSP bottleneck with optional shortcuts; args: ch_in, ch_out, number of repeats, shortcut bool,\n        groups, expansion.\n        \"\"\"\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = nn.Conv2d(c1, c_, 1, 1, bias=False)\n        self.cv3 = nn.Conv2d(c_, c_, 1, 1, bias=False)\n        self.cv4 = Conv(2 * c_, c2, 1, 1)\n        self.bn = nn.BatchNorm2d(2 * c_)  # applied to cat(cv2, cv3)\n        self.act = nn.SiLU()\n        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))", "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import SelectAdaptivePool2d, Linear, make_divisible\nfrom ._builder import build_model_with_cfg\nfrom ._efficientnet_blocks import SqueezeExcite, ConvBnAct\nfrom ._features import feature_take_indices\nfrom ._manipulate import checkpoint_seq\nfrom ._registry import register_model, generate_default_cfgs", "for layer in self.conv.children():\n            if isinstance(layer, nn.Conv2d):\n                torch.nn.init.normal_(layer.weight, std=0.01)\n                torch.nn.init.zeros_(layer.bias)", "pad = get_same_padding(self.kernel_size)\n        self.conv2 = nn.Sequential(\n            OrderedDict(\n                [\n                    (\n                        \"conv\",\n                        nn.Conv2d(\n                            feature_dim,\n                            feature_dim,\n                            kernel_size,\n                            stride,\n                            pad,\n                            groups=groups,\n                            bias=False,\n                        ),\n                    ),\n                    (\"bn\", nn.BatchNorm2d(feature_dim)),\n                    (\"act\", build_activation(self.act_func, inplace=True)),\n                ]\n            )\n        )", "def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.Conv2d):\n            if self.conv_init:\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            else:\n                trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0.)", "from .batch_norm import FrozenBatchNorm2d, get_norm\nfrom .wrappers import Conv2d", "def __init__(\n            self,\n            in_chs=3,\n            out_chs=96,\n            stride=4,\n            norm_layer=LayerNorm2d,\n    ):\n        super().__init__()\n        stride = to_2tuple(stride)\n        self.stride = stride\n        self.in_chs = in_chs\n        self.out_chs = out_chs\n        assert stride[0] == 4  # only setup for stride==4\n        self.conv = nn.Conv2d(\n            in_chs,\n            out_chs,\n            kernel_size=7,\n            stride=stride,\n            padding=3,\n        )\n        self.norm = norm_layer(out_chs)", "class GhostModuleV2(nn.Module):\n    def __init__(\n            self,\n            in_chs,\n            out_chs,\n            kernel_size=1,\n            ratio=2,\n            dw_size=3,\n            stride=1,\n            use_act=True,\n            act_layer=nn.ReLU,\n    ):\n        super().__init__()\n        self.gate_fn = nn.Sigmoid()\n        self.out_chs = out_chs\n        init_chs = math.ceil(out_chs / ratio)\n        new_chs = init_chs * (ratio - 1)\n        self.primary_conv = nn.Sequential(\n            nn.Conv2d(in_chs, init_chs, kernel_size, stride, kernel_size // 2, bias=False),\n            nn.BatchNorm2d(init_chs),\n            act_layer(inplace=True) if use_act else nn.Identity(),\n        )\n        self.cheap_operation = nn.Sequential(\n            nn.Conv2d(init_chs, new_chs, dw_size, 1, dw_size // 2, groups=init_chs, bias=False),\n            nn.BatchNorm2d(new_chs),\n            act_layer(inplace=True) if use_act else nn.Identity(),\n        )\n        self.short_conv = nn.Sequential(\n            nn.Conv2d(in_chs, out_chs, kernel_size, stride, kernel_size // 2, bias=False),\n            nn.BatchNorm2d(out_chs),\n            nn.Conv2d(out_chs, out_chs, kernel_size=(1, 5), stride=1, padding=(0, 2), groups=out_chs, bias=False),\n            nn.BatchNorm2d(out_chs),\n            nn.Conv2d(out_chs, out_chs, kernel_size=(5, 1), stride=1, padding=(2, 0), groups=out_chs, bias=False),\n            nn.BatchNorm2d(out_chs),\n        )", "self.aux_logits = aux_logits\n        self.transform_input = transform_input\n        self.Conv2d_1a_3x3 = conv_block(3, 32, kernel_size=3, stride=2)\n        self.Conv2d_2a_3x3 = conv_block(32, 32, kernel_size=3)\n        self.Conv2d_2b_3x3 = conv_block(32, 64, kernel_size=3, padding=1)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n        self.Conv2d_3b_1x1 = conv_block(64, 80, kernel_size=1)\n        self.Conv2d_4a_3x3 = conv_block(80, 192, kernel_size=3)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n        self.Mixed_5b = inception_a(192, pool_features=32)\n        self.Mixed_5c = inception_a(256, pool_features=64)\n        self.Mixed_5d = inception_a(288, pool_features=64)\n        self.Mixed_6a = inception_b(288)\n        self.Mixed_6b = inception_c(768, channels_7x7=128)\n        self.Mixed_6c = inception_c(768, channels_7x7=160)\n        self.Mixed_6d = inception_c(768, channels_7x7=160)\n        self.Mixed_6e = inception_c(768, channels_7x7=192)\n        self.AuxLogits: Optional[nn.Module] = None\n        if aux_logits:\n            self.AuxLogits = inception_aux(768, num_classes)\n        self.Mixed_7a = inception_d(768)\n        self.Mixed_7b = inception_e(1280)\n        self.Mixed_7c = inception_e(2048)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.dropout = nn.Dropout(p=dropout)\n        self.fc = nn.Linear(2048, num_classes)\n        if init_weights:\n            for m in self.modules():\n                if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n                    stddev = float(m.stddev) if hasattr(m, \"stddev\") else 0.1  # type: ignore\n                    torch.nn.init.trunc_normal_(m.weight, mean=0.0, std=stddev, a=-2, b=2)\n                elif isinstance(m, nn.BatchNorm2d):\n                    nn.init.constant_(m.weight, 1)\n                    nn.init.constant_(m.bias, 0)", "self.num_features = embed_dim * 2 ** (len(depths) - 1)\n        self.norm = norm_layer(self.num_features)\n        self.avgpool = nn.AdaptiveAvgPool3d(1)\n        self.head = nn.Linear(self.num_features, num_classes)", "class RepVggDw(nn.Module):\n    def __init__(self, ed, kernel_size, legacy=False):\n        super().__init__()\n        self.conv = ConvNorm(ed, ed, kernel_size, 1, (kernel_size - 1) // 2, groups=ed)\n        if legacy:\n            self.conv1 = ConvNorm(ed, ed, 1, 1, 0, groups=ed)\n            # Make torchscript happy.\n            self.bn = nn.Identity()\n        else:\n            self.conv1 = nn.Conv2d(ed, ed, 1, 1, 0, groups=ed)\n            self.bn = nn.BatchNorm2d(ed)\n        self.dim = ed\n        self.legacy = legacy", "self.conv1 = nn.Conv2d(\n            in_channels, in_channels, kernel_size, stride, padding, dilation, groups=in_channels, bias=False)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, 1, 0, 1, 1, bias=False)", "def _init_weights(module, name=None, head_init_scale=1.0):\n    if isinstance(module, nn.Conv2d):\n        trunc_normal_(module.weight, std=.02)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Linear):\n        trunc_normal_(module.weight, std=.02)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n        if name and 'head.fc' in name:\n            module.weight.data.mul_(head_init_scale)\n            module.bias.data.mul_(head_init_scale)", "def __init__(\n            self,\n            in_chs: int,\n            rd_ratio: float = 0.25,\n            rd_channels: Optional[int] = None,\n            act_layer: LayerType = nn.ReLU,\n            gate_layer: LayerType = nn.Sigmoid,\n            force_act_layer: Optional[LayerType] = None,\n            rd_round_fn: Optional[Callable] = None,\n    ):\n        super(SqueezeExcite, self).__init__()\n        if rd_channels is None:\n            rd_round_fn = rd_round_fn or round\n            rd_channels = rd_round_fn(in_chs * rd_ratio)\n        act_layer = force_act_layer or act_layer\n        self.conv_reduce = nn.Conv2d(in_chs, rd_channels, 1, bias=True)\n        self.act1 = create_act_layer(act_layer, inplace=True)\n        self.conv_expand = nn.Conv2d(rd_channels, in_chs, 1, bias=True)\n        self.gate = create_act_layer(gate_layer)", "def test_param_groups_weight_decay():\n    model = torch.nn.Sequential(\n        torch.nn.Linear(10, 5),\n        torch.nn.ReLU(),\n        torch.nn.Linear(5, 2)\n    )\n    weight_decay = 0.01\n    no_weight_decay_list = ['1.weight']", "num_branches = self.num_branches\n        num_inchannels = self.num_inchannels\n        fuse_layers = []\n        for i in range(num_branches if self.multi_scale_output else 1):\n            fuse_layer = []\n            for j in range(num_branches):\n                if j > i:\n                    fuse_layer.append(\n                        nn.Sequential(\n                            nn.Conv2d(num_inchannels[j], num_inchannels[i], 1, 1, 0, bias=False),\n                            nn.BatchNorm2d(num_inchannels[i]),\n                            nn.Upsample(scale_factor=2 ** (j - i), mode=\"nearest\"),\n                        )\n                    )\n                elif j == i:\n                    fuse_layer.append(None)\n                else:\n                    conv3x3s = []\n                    for k in range(i - j):\n                        if k == i - j - 1:\n                            num_outchannels_conv3x3 = num_inchannels[i]\n                            conv3x3s.append(\n                                nn.Sequential(\n                                    nn.Conv2d(\n                                        num_inchannels[j],\n                                        num_outchannels_conv3x3,\n                                        3,\n                                        2,\n                                        1,\n                                        bias=False,\n                                    ),\n                                    nn.BatchNorm2d(num_outchannels_conv3x3),\n                                )\n                            )\n                        else:\n                            num_outchannels_conv3x3 = num_inchannels[j]\n                            conv3x3s.append(\n                                nn.Sequential(\n                                    nn.Conv2d(\n                                        num_inchannels[j],\n                                        num_outchannels_conv3x3,\n                                        3,\n                                        2,\n                                        1,\n                                        bias=False,\n                                    ),\n                                    nn.BatchNorm2d(num_outchannels_conv3x3),\n                                    nn.ReLU(True),\n                                )\n                            )\n                    fuse_layer.append(nn.Sequential(*conv3x3s))\n            fuse_layers.append(nn.ModuleList(fuse_layer))", "from detectron2.layers import Conv2d, ShapeSpec, get_norm", "self.conv3 = nn.Conv2d(width, outplanes, kernel_size=1, bias=False)\n        self.bn3 = norm_layer(outplanes)", "def _init_weights(self, m):\n        if isinstance(m, (nn.Conv2d, nn.Linear)):\n            trunc_normal_(m.weight, std=.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)", "def __init__(self, in_embed_dim, out_embed_dim, patch_size=2):\n        super().__init__()\n        self.proj = nn.Conv2d(in_embed_dim, out_embed_dim, kernel_size=patch_size, stride=patch_size)", "self.conv1 = nn.Conv2d(inplanes, first_planes, kernel_size=1, bias=False)\n        self.bn1 = norm_layer(first_planes)\n        self.act1 = act_layer(inplace=True)", "self.bbox_subnet = nn.Sequential(*bbox_subnet)\n        self.bbox_pred = nn.Conv2d(\n            cur_channels, num_anchors * 4, kernel_size=3, stride=1, padding=1\n        )\n        modules_list.extend([self.bbox_subnet, self.bbox_pred])", "# weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)", "self.revert_projs = nn.ModuleList()\n        for d in range(num_branches):\n            if dim[(d + 1) % num_branches] == dim[d] and False:\n                tmp = [nn.Identity()]\n            else:\n                tmp = [norm_layer(dim[(d + 1) % num_branches]), act_layer(),\n                       nn.Linear(dim[(d + 1) % num_branches], dim[d])]\n            self.revert_projs.append(nn.Sequential(*tmp))", "class MLP(nn.Module):\n    def __init__(self, act_layer=\"relu\", inplace=True):\n        super(MLP, self).__init__()\n        self.fc1 = nn.Linear(1000, 100)\n        self.act = create_act_layer(act_layer, inplace=inplace)\n        self.fc2 = nn.Linear(100, 10)", "# Classifier Head\n        self.fc_norm = norm_layer(embed_dim) if fc_norm else nn.Identity()\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()", "self.stem = PatchEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n            norm_layer=norm_layer if stem_norm else None,\n        )\n        reduction = self.stem.feat_ratio() if hasattr(self.stem, 'feat_ratio') else patch_size\n        # FIXME drop_path (stochastic depth scaling rule or all the same?)\n        self.blocks = nn.Sequential(*[\n            block_layer(\n                embed_dim,\n                self.stem.num_patches,\n                mlp_ratio,\n                mlp_layer=mlp_layer,\n                norm_layer=norm_layer,\n                act_layer=act_layer,\n                drop=proj_drop_rate,\n                drop_path=drop_path_rate,\n            )\n            for _ in range(num_blocks)])\n        self.feature_info = [\n            dict(module=f'blocks.{i}', num_chs=embed_dim, reduction=reduction) for i in range(num_blocks)]\n        self.norm = norm_layer(embed_dim)\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()", "class GELU(nn.Module):\n    \"\"\"Applies the Gaussian Error Linear Units function (w/ dummy inplace arg)\n    \"\"\"\n    def __init__(self, inplace: bool = False):\n        super(GELU, self).__init__()", "def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()", "@torch.no_grad()\n    def fuse(self):\n        c, bn = self.conv, self.bn\n        w = bn.weight / (bn.running_var + bn.eps)**0.5\n        w = c.weight * w[:, None, None, None]\n        b = bn.bias - bn.running_mean * bn.weight / \\\n            (bn.running_var + bn.eps)**0.5\n        m = torch.nn.Conv2d(\n            w.size(1) * self.conv.groups, w.size(0), w.shape[2:],\n            stride=self.conv.stride, padding=self.conv.padding, dilation=self.conv.dilation, groups=self.conv.groups)\n        m.weight.data.copy_(w)\n        m.bias.data.copy_(b)\n        return m", "self.primary_conv = nn.Sequential(\n            nn.Conv2d(in_chs, init_chs, kernel_size, stride, kernel_size // 2, bias=False),\n            nn.BatchNorm2d(init_chs),\n            act_layer(inplace=True) if use_act else nn.Identity(),\n        )", "@staticmethod\n    def forward(x):\n        \"\"\"\n        Applies the Sigmoid-weighted Linear Unit (SiLU) activation function.", "if isinstance(self.heads.head, nn.Linear):\n            nn.init.zeros_(self.heads.head.weight)\n            nn.init.zeros_(self.heads.head.bias)", "@torch.no_grad()\n    def fuse(self):\n        bn, l = self.bn, self.linear\n        w = bn.weight / (bn.running_var + bn.eps) ** 0.5\n        b = bn.bias - self.bn.running_mean * self.bn.weight / (bn.running_var + bn.eps) ** 0.5\n        w = l.weight * w[None, :]\n        if l.bias is None:\n            b = b @ self.linear.weight.T\n        else:\n            b = (l.weight @ b[:, None]).view(-1) + self.linear.bias\n        m = nn.Linear(w.size(1), w.size(0))\n        m.weight.data.copy_(w)\n        m.bias.data.copy_(b)\n        return m", "def _make_one_branch(self, branch_index, block, num_blocks, num_channels, stride=1):\n        downsample = None\n        if (\n            stride != 1\n            or self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion\n        ):\n            downsample = nn.Sequential(\n                nn.Conv2d(\n                    self.num_inchannels[branch_index],\n                    num_channels[branch_index] * block.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                    bias=False,\n                ),\n                nn.BatchNorm2d(num_channels[branch_index] * block.expansion, momentum=BN_MOMENTUM),\n            )", "fc = nn.Sequential(\n            nn.MaxPool2d(kernel_size=3, stride=1, padding=1, ceil_mode=False),  # add modified maxpool5\n            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=6, dilation=6),  # FC6 with atrous\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=1),  # FC7\n            nn.ReLU(inplace=True),\n        )\n        _xavier_init(fc)\n        extra.insert(\n            0,\n            nn.Sequential(\n                *backbone[maxpool4_pos:-1],  # until conv5_3, skip maxpool5\n                fc,\n            ),\n        )\n        self.extra = extra", "class Block(nn.Module):\n    def __init__(self, in_chs, inter_chs, out_chs, norm_layer, act_layer):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Conv2d(in_chs, in_chs, groups=in_chs, kernel_size=7, stride=1, padding=3),\n            norm_layer(in_chs),\n            nn.Conv2d(in_chs, inter_chs, kernel_size=1, stride=1, padding=0),\n            act_layer(),\n            nn.Conv2d(inter_chs, out_chs, kernel_size=1, stride=1, padding=0),\n        )", "def initialize_weights(model):\n    \"\"\"Initializes weights of Conv2d, BatchNorm2d, and activations (Hardswish, LeakyReLU, ReLU, ReLU6, SiLU) in the\n    model.\n    \"\"\"\n    for m in model.modules():\n        t = type(m)\n        if t is nn.Conv2d:\n            pass  # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n        elif t is nn.BatchNorm2d:\n            m.eps = 1e-3\n            m.momentum = 0.03\n        elif t in [nn.Hardswish, nn.LeakyReLU, nn.ReLU, nn.ReLU6, nn.SiLU]:\n            m.inplace = True", "def switch_to_deploy(self):\n        if len(self.fusion_conv) == 0 and len(self.fusion_bn) == 0:\n            return\n        kernel, bias = self.get_equivalent_kernel_bias()\n        self.cheap_operation = nn.Conv2d(\n            in_channels=self.cheap_operation[0].in_channels,\n            out_channels=self.cheap_operation[0].out_channels,\n            kernel_size=self.cheap_operation[0].kernel_size,\n            padding=self.cheap_operation[0].padding,\n            dilation=self.cheap_operation[0].dilation,\n            groups=self.cheap_operation[0].groups,\n            bias=True)\n        self.cheap_operation.weight.data = kernel\n        self.cheap_operation.bias.data = bias\n        self.__delattr__('fusion_conv')\n        self.__delattr__('fusion_bn')\n        self.fusion_conv = []\n        self.fusion_bn = []", "for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                if m is final_conv:\n                    init.normal_(m.weight, mean=0.0, std=0.01)\n                else:\n                    init.kaiming_uniform_(m.weight)\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)", "weight_dict = OrderedDict(\n            {\n                \"conv\": nn.Conv2d(\n                    self.in_channels,\n                    self.out_channels,\n                    kernel_size=self.kernel_size,\n                    stride=self.stride,\n                    padding=padding,\n                    dilation=self.dilation,\n                    groups=min_divisible_value(self.in_channels, self.groups),\n                    bias=self.bias,\n                )\n            }\n        )\n        if self.has_shuffle and self.groups > 1:\n            weight_dict[\"shuffle\"] = ShuffleLayer(self.groups)", "def _init_weights(module: nn.Module, name: str = '', zero_init_last=True):\n    if isinstance(module, nn.Linear) or ('head.fc' in name and isinstance(module, nn.Conv2d)):\n        nn.init.normal_(module.weight, mean=0.0, std=0.01)\n        nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Conv2d):\n        nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, (nn.BatchNorm2d, nn.LayerNorm, nn.GroupNorm)):\n        nn.init.ones_(module.weight)\n        nn.init.zeros_(module.bias)\n    elif zero_init_last and hasattr(module, 'zero_init_last'):\n        module.zero_init_last()", "def remove_reshape_for_fc(predict_net, params):\n    \"\"\"\n    In PyTorch nn.Linear has to take 2D tensor, this often leads to reshape\n        a 4D tensor to 2D by calling .view(). However this (dynamic) reshaping\n        doesn't work well with ONNX and Int8 tools, and cause using extra\n        ops (eg. ExpandDims) that might not be available on mobile.\n    Luckily Caffe2 supports 4D tensor for FC, so we can remove those reshape\n        after exporting ONNX model.\n    \"\"\"\n    from caffe2.python import core", "def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1)\n        self.sigmoid = nn.Sigmoid()", "def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            assert global_pool in ('', 'avg', 'token')\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()", "def addmm_flop(inputs: list[Any], outputs: list[Any]) -> Number:\n    \"\"\"\n    Count flops for fully connected layers.\n    \"\"\"\n    # Count flop for nn.Linear\n    # inputs is a list of length 3.\n    input_shapes = [get_shape(v) for v in inputs[1:3]]\n    # input_shapes[0]: [batch size, input feature dimension]\n    # input_shapes[1]: [batch size, output feature dimension]\n    assert len(input_shapes[0]) == 2, input_shapes[0]\n    assert len(input_shapes[1]) == 2, input_shapes[1]\n    batch_size, input_dim = input_shapes[0]\n    output_dim = input_shapes[1][1]\n    flops = batch_size * input_dim * output_dim\n    return flops", "n = max(round(n * gd), 1) if n > 1 else n  # depth gain\n        if m in [\n            nn.Conv2d,\n            Conv,\n            DWConv,\n            DWConvTranspose2d,\n            Bottleneck,\n            SPP,\n            SPPF,\n            MixConv2d,\n            Focus,\n            CrossConv,\n            BottleneckCSP,\n            C3,\n            C3x,\n        ]:\n            c1, c2 = ch[f], args[0]\n            c2 = make_divisible(c2 * gw, ch_mul) if c2 != no else c2", "def _init_weights(module: nn.Module, name: str, head_bias: float = 0., flax=False):\n    \"\"\" Mixer weight initialization (trying to match Flax defaults)\n    \"\"\"\n    if isinstance(module, nn.Linear):\n        if name.startswith('head'):\n            nn.init.zeros_(module.weight)\n            nn.init.constant_(module.bias, head_bias)\n        else:\n            if flax:\n                # Flax defaults\n                lecun_normal_(module.weight)\n                if module.bias is not None:\n                    nn.init.zeros_(module.bias)\n            else:\n                # like MLP init in vit (my original init)\n                nn.init.xavier_uniform_(module.weight)\n                if module.bias is not None:\n                    if 'mlp' in name:\n                        nn.init.normal_(module.bias, std=1e-6)\n                    else:\n                        nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Conv2d):\n        lecun_normal_(module.weight)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, (nn.LayerNorm, nn.BatchNorm2d, nn.GroupNorm)):\n        nn.init.ones_(module.weight)\n        nn.init.zeros_(module.bias)\n    elif hasattr(module, 'init_weights'):\n        # NOTE if a parent module contains init_weights method, it can override the init of the\n        # child modules as this will be called in depth-first order.\n        module.init_weights()", "@torch.no_grad()\n    def fuse(self):\n        c, bn = self.conv, self.bn\n        w = bn.weight / (bn.running_var + bn.eps) ** 0.5\n        w = c.weight * w[:, None, None, None]\n        b = bn.bias - bn.running_mean * bn.weight / \\\n            (bn.running_var + bn.eps) ** 0.5\n        m = torch.nn.Conv2d(\n            w.size(1) * self.conv.groups, w.size(0), w.shape[2:],\n            stride=self.conv.stride, padding=self.conv.padding, dilation=self.conv.dilation, groups=self.conv.groups)\n        m.weight.data.copy_(w)\n        m.bias.data.copy_(b)\n        return m", "def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            assert global_pool in ('', 'token', 'avg')\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()", "from detectron2.config import configurable\nfrom detectron2.layers import Conv2d, ConvTranspose2d, ShapeSpec, cat, get_norm\nfrom detectron2.layers.wrappers import move_device_like\nfrom detectron2.structures import Instances\nfrom detectron2.utils.events import get_event_storage\nfrom detectron2.utils.registry import Registry", "# lf = lambda x: ((1 + math.cos(x * math.pi / epochs)) / 2) * (1 - lrf) + lrf  # cosine\n    def lf(x):\n        \"\"\"Linear learning rate scheduler function, scaling learning rate from initial value to `lrf` over `epochs`.\"\"\"\n        return (1 - x / epochs) * (1 - lrf) + lrf  # linear", "def _init_transformer(module, name, scheme=''):\n    if isinstance(module, (nn.Conv2d, nn.Linear)):\n        if scheme == 'normal':\n            nn.init.normal_(module.weight, std=.02)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        elif scheme == 'trunc_normal':\n            trunc_normal_tf_(module.weight, std=.02)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        elif scheme == 'xavier_normal':\n            nn.init.xavier_normal_(module.weight)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        else:\n            # vit like\n            nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                if 'mlp' in name:\n                    nn.init.normal_(module.bias, std=1e-6)\n                else:\n                    nn.init.zeros_(module.bias)", ">>> # Specifying leaf modules and leaf functions\n        >>> def leaf_function(x):\n        >>>     # This would raise a TypeError if traced through\n        >>>     return int(x)\n        >>>\n        >>> class LeafModule(torch.nn.Module):\n        >>>     def forward(self, x):\n        >>>         # This would raise a TypeError if traced through\n        >>>         int(x.shape[0])\n        >>>         return torch.nn.functional.relu(x + 4)\n        >>>\n        >>> class MyModule(torch.nn.Module):\n        >>>     def __init__(self):\n        >>>         super().__init__()\n        >>>         self.conv = torch.nn.Conv2d(3, 1, 3)\n        >>>         self.leaf_module = LeafModule()\n        >>>\n        >>>     def forward(self, x):\n        >>>         leaf_function(x.shape[0])\n        >>>         x = self.conv(x)\n        >>>         return self.leaf_module(x)\n        >>>\n        >>> model = create_feature_extractor(\n        >>>     MyModule(), return_nodes=['leaf_module'],\n        >>>     tracer_kwargs={'leaf_modules': [LeafModule],\n        >>>                    'autowrap_functions': [leaf_function]})", "# Stem\n        deep_stem = 'deep' in stem_type\n        inplanes = stem_width * 2 if deep_stem else 64\n        if deep_stem:\n            stem_chs = (stem_width, stem_width)\n            if 'tiered' in stem_type:\n                stem_chs = (3 * (stem_width // 4), stem_width)\n            self.conv1 = nn.Sequential(*[\n                nn.Conv2d(in_chans, stem_chs[0], 3, stride=2, padding=1, bias=False),\n                norm_layer(stem_chs[0]),\n                act_layer(inplace=True),\n                nn.Conv2d(stem_chs[0], stem_chs[1], 3, stride=1, padding=1, bias=False),\n                norm_layer(stem_chs[1]),\n                act_layer(inplace=True),\n                nn.Conv2d(stem_chs[1], inplanes, 3, stride=1, padding=1, bias=False)])\n        else:\n            self.conv1 = nn.Conv2d(in_chans, inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = norm_layer(inplanes)\n        self.act1 = act_layer(inplace=True)\n        self.feature_info = [dict(num_chs=inplanes, reduction=2, module='act1')]", "def create_conv2d(in_channels, out_channels, kernel_size, **kwargs):\n    \"\"\" Select a 2d convolution implementation based on arguments\n    Creates and returns one of torch.nn.Conv2d, Conv2dSame, MixedConv2d, or CondConv2d.", "@register_model()\n@handle_legacy_interface(weights=(\"pretrained\", MobileNet_V2_Weights.IMAGENET1K_V1))\ndef mobilenet_v2(\n    *, weights: Optional[MobileNet_V2_Weights] = None, progress: bool = True, **kwargs: Any\n) -> MobileNetV2:\n    \"\"\"MobileNetV2 architecture from the `MobileNetV2: Inverted Residuals and Linear\n    Bottlenecks <https://arxiv.org/abs/1801.04381>`_ paper.", "def __init__(\n            self,\n            dim,\n            expansion_ratio=2,\n            act1_layer=StarReLU,\n            act2_layer=nn.Identity,\n            bias=False,\n            kernel_size=7,\n            padding=3,\n            **kwargs\n    ):\n        super().__init__()\n        mid_channels = int(expansion_ratio * dim)\n        self.pwconv1 = nn.Conv2d(dim, mid_channels, kernel_size=1, bias=bias)\n        self.act1 = act1_layer()\n        self.dwconv = nn.Conv2d(\n            mid_channels, mid_channels, kernel_size=kernel_size,\n            padding=padding, groups=mid_channels, bias=bias)  # depthwise conv\n        self.act2 = act2_layer()\n        self.pwconv2 = nn.Conv2d(mid_channels, dim, kernel_size=1, bias=bias)", "def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)", "# init for classification\n    def init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)", "def __init__(self, *args, **kwargs):\n        \"\"\"\n        Extra keyword arguments supported in addition to those in `torch.nn.Conv2d`:", "self.conv2_offset = Conv2d(\n            bottleneck_channels,\n            offset_channels * deform_num_groups,\n            kernel_size=3,\n            stride=stride_3x3,\n            padding=1 * dilation,\n            dilation=dilation,\n        )\n        self.conv2 = deform_conv_op(\n            bottleneck_channels,\n            bottleneck_channels,\n            kernel_size=3,\n            stride=stride_3x3,\n            padding=1 * dilation,\n            bias=False,\n            groups=num_groups,\n            dilation=dilation,\n            deformable_groups=deform_num_groups,\n            norm=get_norm(norm, bottleneck_channels),\n        )", "class EffectiveSEModule(nn.Module):\n    \"\"\" 'Effective Squeeze-Excitation\n    From `CenterMask : Real-Time Anchor-Free Instance Segmentation` - https://arxiv.org/abs/1911.06667\n    \"\"\"\n    def __init__(self, channels, add_maxpool=False, gate_layer='hard_sigmoid', **_):\n        super(EffectiveSEModule, self).__init__()\n        self.add_maxpool = add_maxpool\n        self.fc = nn.Conv2d(channels, channels, kernel_size=1, padding=0)\n        self.gate = create_act_layer(gate_layer)", "@staticmethod\n    def _fuse_bn_tensor(conv, bn, in_channels=None, device=None):\n        in_channels = in_channels if in_channels else bn.running_mean.shape[0]\n        device = device if device else bn.weight.device\n        if isinstance(conv, nn.Conv2d):\n            kernel = conv.weight\n            assert conv.bias is None\n        else:\n            assert isinstance(conv, nn.Identity)\n            kernel = torch.ones(in_channels, 1, 1, 1, device=device)", "class ConvNorm(nn.Sequential):\n    def __init__(self, in_dim, out_dim, ks=1, stride=1, pad=0, dilation=1, groups=1, bn_weight_init=1):\n        super().__init__()\n        self.add_module('c', nn.Conv2d(in_dim, out_dim, ks, stride, pad, dilation, groups, bias=False))\n        self.add_module('bn', nn.BatchNorm2d(out_dim))\n        nn.init.constant_(self.bn.weight, bn_weight_init)\n        nn.init.constant_(self.bn.bias, 0)", "self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(in_features=current_width, out_features=num_classes)", "def __init__(self, in_channels, num_classes):\n        super().__init__()\n        self.cls_score = nn.Linear(in_channels, num_classes)\n        self.bbox_pred = nn.Linear(in_channels, num_classes * 4)", "\"\"\" Linear layer (alternate definition)\n\"\"\"\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn as nn", "model = torch.nn.Sequential(torch.nn.Linear(2, 3),\n                                torch.nn.Sigmoid(),\n                                torch.nn.Linear(3, 1),\n                                torch.nn.Sigmoid())\n    model.to(device)", "def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)", "class Conv2dSame(nn.Conv2d):\n    \"\"\" Tensorflow like 'SAME' convolution wrapper for 2D convolutions\n    \"\"\"", "self.q = nn.Linear(dim, dim, bias=True)\n        self.kv = nn.Linear(dim, dim * 2, bias=True)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)", "self.path_2 = nn.Sequential()\n        self.path_2.add_module('pad', nn.ZeroPad2d((-1, 1, -1, 1)))\n        self.path_2.add_module('avgpool', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_2.add_module('conv', nn.Conv2d(in_chs_left, out_chs_left, 1, stride=1, bias=False))", "def _make_one_branch(self, branch_index, block_type, num_blocks, num_channels, stride=1):\n        downsample = None\n        if stride != 1 or self.num_in_chs[branch_index] != num_channels[branch_index] * block_type.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(\n                    self.num_in_chs[branch_index], num_channels[branch_index] * block_type.expansion,\n                    kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(num_channels[branch_index] * block_type.expansion, momentum=_BN_MOMENTUM),\n            )", "from timm.data import IMAGENET_DEFAULT_STD, IMAGENET_DEFAULT_MEAN, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\nfrom timm.layers import trunc_normal_, create_classifier, Linear, ConvNormAct\nfrom ._builder import build_model_with_cfg\nfrom ._builder import resolve_pretrained_cfg\nfrom ._manipulate import flatten_modules\nfrom ._registry import register_model, generate_default_cfgs, register_model_deprecations", "\"\"\" Split Attention Conv2d (for ResNeSt Models)", "for layer in self.conv.modules():\n            if isinstance(layer, nn.Conv2d):\n                torch.nn.init.normal_(layer.weight, std=0.01)\n                if layer.bias is not None:\n                    torch.nn.init.constant_(layer.bias, 0)", "from timm.layers import Conv2dSame, BatchNormAct2d, Linear", "conv = []\n        for _ in range(num_convs):\n            conv.append(nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1))\n            conv.append(norm_layer(in_channels))\n            conv.append(nn.ReLU())\n        self.conv = nn.Sequential(*conv)", "class SiLU(nn.Module):\n    \"\"\"Applies the Sigmoid-weighted Linear Unit (SiLU) activation function, also known as Swish.\"\"\"", "self.conv = nn.Conv2d(\n            in_feature,\n            out_feature,\n            kernel_size=stride + 1,\n            padding=stride // 2,\n            stride=stride,\n            padding_mode=padding_mode,\n            groups=in_feature,\n        )\n        self.fc = nn.Linear(in_feature, out_feature)", "for n, m in self.named_modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.Linear):\n                nn.init.zeros_(m.bias)", "self.f = nn.Conv2d(dim, 2 * dim + (self.focal_level + 1), kernel_size=1, bias=bias)\n        self.h = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)", "# Note: the expected losses are slightly different even if\n        # the boxes are essentially the same as in the FastRCNNOutput test, because\n        # bbox_pred in FastRCNNOutputLayers have different Linear layers/initialization\n        # between the two cases.\n        expected_losses = {\n            \"loss_cls\": torch.tensor(1.7920907736),\n            \"loss_box_reg\": torch.tensor(4.0410838127),\n        }\n        for name in expected_losses.keys():\n            assert torch.allclose(losses[name], expected_losses[name])", "ConvTranspose2d = torch.nn.ConvTranspose2d\nBatchNorm2d = torch.nn.BatchNorm2d\ninterpolate = F.interpolate\nLinear = torch.nn.Linear", "from detectron2.config import CfgNode\nfrom detectron2.layers import Conv2d", "NOTE: When use_conv=True, expects 2D NCHW tensors, otherwise N*C expected.\n    \"\"\"\n    def __init__(\n            self,\n            in_features,\n            hidden_features=None,\n            out_features=None,\n            act_layer=nn.Sigmoid,\n            norm_layer=None,\n            bias=True,\n            drop=0.,\n            use_conv=False,\n            gate_last=True,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        assert hidden_features % 2 == 0\n        bias = to_2tuple(bias)\n        drop_probs = to_2tuple(drop)\n        linear_layer = partial(nn.Conv2d, kernel_size=1) if use_conv else nn.Linear\n        self.chunk_dim = 1 if use_conv else -1\n        self.gate_last = gate_last  # use second half of width for gate", "layers = []\n        in_dim = in_channels\n        for hidden_dim in hidden_channels[:-1]:\n            layers.append(torch.nn.Linear(in_dim, hidden_dim, bias=bias))\n            if norm_layer is not None:\n                layers.append(norm_layer(hidden_dim))\n            layers.append(activation_layer(**params))\n            layers.append(torch.nn.Dropout(dropout, **params))\n            in_dim = hidden_dim", "self.sr_ratio = sr_ratio\n        if sr_ratio > 1:\n            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n            self.norm = nn.LayerNorm(dim)\n        else:\n            self.sr = None\n            self.norm = None", "self.proj = nn.Conv2d(\n            hidden_dim, embed_dim, kernel_size=patch_size // stem_stride, stride=patch_size // stem_stride)\n        self.num_patches = (img_size // patch_size) * (img_size // patch_size)", "def __init__(\n            self,\n            in_features=512,\n            out_features=4096,\n            kernel_size=7,\n            mlp_ratio=1.0,\n            drop_rate: float = 0.2,\n            act_layer: Type[nn.Module] = nn.ReLU,\n            conv_layer: Type[nn.Module] = nn.Conv2d,\n    ):\n        super(ConvMlp, self).__init__()\n        self.input_kernel_size = kernel_size\n        mid_features = int(out_features * mlp_ratio)\n        self.fc1 = conv_layer(in_features, mid_features, kernel_size, bias=True)\n        self.act1 = act_layer(True)\n        self.drop = nn.Dropout(drop_rate)\n        self.fc2 = conv_layer(mid_features, out_features, 1, bias=True)\n        self.act2 = act_layer(True)", "def _make_conv_level(self, inplanes, planes, convs, stride=1, dilation=1):\n        modules = []\n        for i in range(convs):\n            modules.extend([\n                nn.Conv2d(\n                    inplanes, planes, kernel_size=3,\n                    stride=stride if i == 0 else 1,\n                    padding=dilation, bias=False, dilation=dilation),\n                nn.BatchNorm2d(planes),\n                nn.ReLU(inplace=True)])\n            inplanes = planes\n        return nn.Sequential(*modules)", "def lf(x):\n            \"\"\"Linear learning rate scheduler decreasing from 1 to hyp['lrf'] over 'epochs'.\"\"\"\n            return (1 - x / epochs) * (1.0 - hyp[\"lrf\"]) + hyp[\"lrf\"]  # linear", "self.q = nn.Linear(dim, self.dim, bias=qkv_bias)\n        self.k = nn.Linear(dim, self.dim, bias=qkv_bias)\n        self.v = nn.Linear(dim, self.dim, bias=qkv_bias)\n        self.proj = nn.Linear(self.dim, self.out_dim)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj_drop = nn.Dropout(proj_drop)", "from detectron2.layers import CNNBlockBase, Conv2d, get_norm\nfrom detectron2.modeling import BACKBONE_REGISTRY\nfrom detectron2.modeling.backbone.resnet import (\n    BasicStem,\n    BottleneckBlock,\n    DeformBottleneckBlock,\n    ResNet,\n)", "\"\"\"\n        super(SelectiveKernelAttn, self).__init__()\n        self.num_paths = num_paths\n        self.fc_reduce = nn.Conv2d(channels, attn_channels, kernel_size=1, bias=False)\n        self.bn = norm_layer(attn_channels)\n        self.act = act_layer(inplace=True)\n        self.fc_select = nn.Conv2d(attn_channels, channels * num_paths, kernel_size=1, bias=False)", "self.project = nn.Sequential(\n            nn.Conv2d(5 * out_channels, out_channels, 1, bias=False),\n            # nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            # nn.Dropout(0.5)\n        )", "Wraps torch.nn.Linear to support AMP + torchscript usage by manually casting\n    weight & bias to input.dtype to work around an issue w/ torch.addmm in this use case.\n    \"\"\"\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        if torch.jit.is_scripting():\n            bias = self.bias.to(dtype=input.dtype) if self.bias is not None else None\n            return F.linear(input, self.weight.to(dtype=input.dtype), bias=bias)\n        else:\n            return F.linear(input, self.weight, self.bias)", "for n, m in self.named_modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.zeros_(m.bias)", "def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            assert global_pool in ('token', 'avg')\n            self.global_pool = global_pool\n        self.head = nn.ModuleList([\n            nn.Linear(self.embed_dim[i], num_classes) if num_classes > 0 else nn.Identity()\n            for i in range(self.num_branches)\n        ])", "self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)", "def __init__(self, nc=80, anchors=(), nm=32, npr=256, ch=(), inplace=True):\n        \"\"\"Initializes YOLOv5 Segment head with options for mask count, protos, and channel adjustments.\"\"\"\n        super().__init__(nc, anchors, ch, inplace)\n        self.nm = nm  # number of masks\n        self.npr = npr  # number of protos\n        self.no = 5 + nc + self.nm  # number of outputs per anchor\n        self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, 1) for x in ch)  # output conv\n        self.proto = Proto(ch[0], self.npr, self.nm)  # protos\n        self.detect = Detect.forward", "hidden_dims = self.update_block.hidden_dims\n        # Follow the original implementation to do pre convolution on the context\n        # See: https://github.com/princeton-vl/RAFT-Stereo/blob/main/core/raft_stereo.py#L32\n        self.context_convs = nn.ModuleList(\n            [nn.Conv2d(hidden_dims[i], hidden_dims[i] * 3, kernel_size=3, padding=1) for i in range(self.num_level)]\n        )\n        self.slow_fast = slow_fast", "self.q = ConvNorm(dim, kh)\n        self.k = ConvNorm(dim, kh)\n        self.v = ConvNorm(dim, self.dh)\n        self.v_local = ConvNorm(self.dh, self.dh, kernel_size=3, groups=self.dh)\n        self.talking_head1 = nn.Conv2d(self.num_heads, self.num_heads, kernel_size=1)\n        self.talking_head2 = nn.Conv2d(self.num_heads, self.num_heads, kernel_size=1)", "def __init__(self, inplanes, planes, stride=1, dilation=1, **_):\n        super(DlaBasic, self).__init__()\n        self.conv1 = nn.Conv2d(\n            inplanes, planes, kernel_size=3,\n            stride=stride, padding=dilation, bias=False, dilation=dilation)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(\n            planes, planes, kernel_size=3,\n            stride=1, padding=dilation, bias=False, dilation=dilation)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.stride = stride", "if hasattr(self.heads, \"pre_logits\") and isinstance(self.heads.pre_logits, nn.Linear):\n            fan_in = self.heads.pre_logits.in_features\n            nn.init.trunc_normal_(self.heads.pre_logits.weight, std=math.sqrt(1 / fan_in))\n            nn.init.zeros_(self.heads.pre_logits.bias)", "class BlockESE(nn.Module):\n    def __init__(self, in_chs, inter_chs, out_chs, norm_layer, act_layer):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Conv2d(in_chs, in_chs, groups=in_chs, kernel_size=7, stride=1, padding=3),\n            norm_layer(in_chs),\n            nn.Conv2d(in_chs, inter_chs, kernel_size=1, stride=1, padding=0),\n            act_layer(),\n            nn.Conv2d(inter_chs, out_chs, kernel_size=1, stride=1, padding=0),\n            EffectiveSEModule(out_chs),\n        )", "class VGG(nn.Module):\n    def __init__(\n        self, features: nn.Module, num_classes: int = 1000, init_weights: bool = True, dropout: float = 0.5\n    ) -> None:\n        super().__init__()\n        _log_api_usage_once(self)\n        self.features = features\n        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(p=dropout),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(p=dropout),\n            nn.Linear(4096, num_classes),\n        )\n        if init_weights:\n            for m in self.modules():\n                if isinstance(m, nn.Conv2d):\n                    nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, nn.BatchNorm2d):\n                    nn.init.constant_(m.weight, 1)\n                    nn.init.constant_(m.bias, 0)\n                elif isinstance(m, nn.Linear):\n                    nn.init.normal_(m.weight, 0, 0.01)\n                    nn.init.constant_(m.bias, 0)", "def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)", "self.fc1 = nn.Conv2d(in_features, hidden_features, kernel_size=1, bias=bias[0])\n        self.norm = norm_layer(hidden_features) if norm_layer else nn.Identity()\n        self.act = act_layer()\n        self.drop = nn.Dropout(drop)\n        self.fc2 = nn.Conv2d(hidden_features, out_features, kernel_size=1, bias=bias[1])", "def __init__(self, *, input_shape: List[ShapeSpec], conv_dims: List[int], **kwargs):\n        super().__init__(input_shape=input_shape, conv_dims=conv_dims, num_anchors=1, **kwargs)\n        # Unlike original FCOS, we do not add an additional learnable scale layer\n        # because it's found to have no benefits after normalizing regression targets by stride.\n        self._num_features = len(input_shape)\n        self.ctrness = nn.Conv2d(conv_dims[-1], 1, kernel_size=3, stride=1, padding=1)\n        torch.nn.init.normal_(self.ctrness.weight, std=0.01)\n        torch.nn.init.constant_(self.ctrness.bias, 0)", "class NormMlp(nn.Module):\n    def __init__(\n            self,\n            in_features,\n            hidden_features=None,\n            out_features=None,\n            norm_layer=nn.LayerNorm,\n            act_layer=nn.GELU,\n            drop=0.,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.norm = norm_layer(in_features)\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.drop1 = nn.Dropout(drop)\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop2 = nn.Dropout(drop)", "self.conv_kxk = layers.conv_norm_act(\n            in_chs, in_chs, kernel_size=kernel_size,\n            stride=1, groups=groups, dilation=dilation[0])\n        self.conv_1x1 = nn.Conv2d(in_chs, transformer_dim, kernel_size=1, bias=False)", "self.qk = nn.Linear(dim, hidden_dim * 2, bias=qkv_bias)\n        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop, inplace=True)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop, inplace=True)", "_layers = OrderedDict()\n        _layers[\"pre_norm\"] = norm_layer(in_channels)\n        _layers[\"conv_a\"] = Conv2dNormActivation(\n            in_channels,\n            mid_channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            activation_layer=activation_layer,\n            norm_layer=norm_layer,\n            inplace=None,\n        )\n        _layers[\"conv_b\"] = Conv2dNormActivation(\n            mid_channels,\n            mid_channels,\n            kernel_size=3,\n            stride=stride,\n            padding=1,\n            activation_layer=activation_layer,\n            norm_layer=norm_layer,\n            groups=mid_channels,\n            inplace=None,\n        )\n        _layers[\"squeeze_excitation\"] = SqueezeExcitation(mid_channels, sqz_channels, activation=nn.SiLU)\n        _layers[\"conv_c\"] = nn.Conv2d(in_channels=mid_channels, out_channels=out_channels, kernel_size=1, bias=True)", "# Linear layer\n        global_pool, classifier = create_classifier(\n            self.num_features,\n            self.num_classes,\n            pool_type=global_pool,\n        )\n        self.global_pool = global_pool\n        self.head_drop = nn.Dropout(drop_rate)\n        self.classifier = classifier", "def unitwise_norm(x, norm_type=2.0):\n    if x.ndim <= 1:\n        return x.norm(norm_type)\n    else:\n        # works for nn.ConvNd and nn,Linear where output dim is first in the kernel/weight tensor\n        # might need special cases for other weights (possibly MHA) where this may not be true\n        return x.norm(norm_type, dim=tuple(range(1, x.ndim)), keepdim=True)", "self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim, bias=proj_bias)\n        self.proj_drop = nn.Dropout(proj_drop)", "def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.normal_(m.weight, std=0.02)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, std=0.02)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)", "self.block = nn.Sequential(\n            nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim, bias=True),\n            Permute([0, 2, 3, 1]),\n            norm_layer(dim),\n            nn.Linear(in_features=dim, out_features=4 * dim, bias=True),\n            nn.GELU(),\n            nn.Linear(in_features=4 * dim, out_features=dim, bias=True),\n            Permute([0, 3, 1, 2]),\n        )\n        self.layer_scale = nn.Parameter(torch.ones(dim, 1, 1) * layer_scale)\n        self.stochastic_depth = StochasticDepth(stochastic_depth_prob, \"row\")", "total_stage_blocks = sum(cnf.num_layers for cnf in block_setting)\n        stage_block_id = 0\n        for cnf in block_setting:\n            # Bottlenecks\n            stage: list[nn.Module] = []\n            for _ in range(cnf.num_layers):\n                # adjust stochastic depth probability based on the depth of the stage block\n                sd_prob = stochastic_depth_prob * stage_block_id / (total_stage_blocks - 1.0)\n                stage.append(block(cnf.input_channels, layer_scale, sd_prob))\n                stage_block_id += 1\n            layers.append(nn.Sequential(*stage))\n            if cnf.out_channels is not None:\n                # Downsampling\n                layers.append(\n                    nn.Sequential(\n                        norm_layer(cnf.input_channels),\n                        nn.Conv2d(cnf.input_channels, cnf.out_channels, kernel_size=2, stride=2),\n                    )\n                )", "if qkv_separate:\n            self.q = nn.Linear(in_features, embed_dim, bias=qkv_bias)\n            self.k = nn.Linear(in_features, embed_dim, bias=qkv_bias)\n            self.v = nn.Linear(in_features, embed_dim, bias=qkv_bias)\n            self.qkv = None\n        else:\n            self.q = self.k = self.v = None\n            self.qkv = nn.Linear(in_features, embed_dim * 3, bias=qkv_bias)\n        self.drop = nn.Dropout(drop_rate)\n        self.proj = nn.Linear(embed_dim, self.out_features)\n        self.pos_embed = nn.Parameter(torch.zeros(self.seq_len + 1, in_features))", "def __init__(self, *, in_channels, hidden_size):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, hidden_size, 3, padding=1)\n        self.conv2 = nn.Conv2d(hidden_size, 2, 3, padding=1)\n        self.relu = nn.ReLU(inplace=True)", "self.path_2 = nn.Sequential()\n        self.path_2.add_module('pad', nn.ZeroPad2d((-1, 1, -1, 1)))\n        self.path_2.add_module('avgpool', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_2.add_module('conv', nn.Conv2d(self.stem_size, self.num_channels // 2, 1, stride=1, bias=False))", "self.norm = norm_layer(in_features)\n        if hidden_size:\n            self.pre_logits = nn.Sequential(OrderedDict([\n                ('fc', nn.Linear(in_features, hidden_size)),\n                ('act', act_layer()),\n            ]))\n            self.num_features = hidden_size\n        else:\n            self.pre_logits = nn.Identity()\n        self.drop = nn.Dropout(drop_rate)\n        self.fc = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()", "def __init__(self, *, in_channels, hidden_size, multiplier=0.25):\n        super().__init__()\n        self.convrelu = Conv2dNormActivation(in_channels, hidden_size, norm_layer=None, kernel_size=3)\n        # 8 * 8 * 9 because the predicted flow is downsampled by 8, from the downsampling of the initial FeatureEncoder,\n        # and we interpolate with all 9 surrounding neighbors. See paper and appendix B.\n        self.conv = nn.Conv2d(hidden_size, 8 * 8 * 9, 1, padding=0)", "self.bbox_reg = nn.Conv2d(in_channels, num_anchors * 4, kernel_size=3, stride=1, padding=1)\n        torch.nn.init.normal_(self.bbox_reg.weight, std=0.01)\n        torch.nn.init.zeros_(self.bbox_reg.bias)", "class TestTimePoolHead(nn.Module):\n    def __init__(self, base, original_pool=7):\n        super(TestTimePoolHead, self).__init__()\n        self.base = base\n        self.original_pool = original_pool\n        base_fc = self.base.get_classifier()\n        if isinstance(base_fc, nn.Conv2d):\n            self.fc = base_fc\n        else:\n            self.fc = nn.Conv2d(\n                self.base.num_features, self.base.num_classes, kernel_size=1, bias=True)\n            self.fc.weight.data.copy_(base_fc.weight.data.view(self.fc.weight.size()))\n            self.fc.bias.data.copy_(base_fc.bias.data.view(self.fc.bias.size()))\n        self.base.reset_classifier(0)  # delete original fc layer", "# class subnet\n        cls_subnet = []\n        cur_channels = in_channels\n        for _ in range(num_convs):\n            cls_subnet.append(\n                nn.Conv2d(cur_channels, cls_channels, kernel_size=3, stride=1, padding=1)\n            )\n            cur_channels = cls_channels\n            cls_subnet.append(nn.ReLU())", "self.conv3 = nn.Sequential(\n            OrderedDict(\n                [\n                    (\n                        \"conv\",\n                        nn.Conv2d(feature_dim, self.out_channels, 1, 1, 0, bias=False),\n                    ),\n                    (\"bn\", nn.BatchNorm2d(self.out_channels)),\n                ]\n            )\n        )", "# Initialization\n        for modules in [self.cls_subnet, self.bbox_subnet, self.cls_score, self.bbox_pred]:\n            for layer in modules.modules():\n                if isinstance(layer, nn.Conv2d):\n                    torch.nn.init.normal_(layer.weight, mean=0, std=0.01)\n                    torch.nn.init.constant_(layer.bias, 0)", "if stride == 2: \n            self.shortcut = Downsample2d(in_chs, out_chs, pool_type='avg', bias=True)\n        elif in_chs != out_chs:\n            self.shortcut = nn.Conv2d(in_chs, out_chs, 1, bias=True)\n        else:\n            self.shortcut = nn.Identity()", "def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            assert global_pool in ('', 'avg')\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()", "def forward(self, x):\n        # torchscript does not support SyncBatchNorm yet\n        # https://github.com/pytorch/pytorch/issues/40507\n        # and we skip these codes in torchscript since:\n        # 1. currently we only support torchscript in evaluation mode\n        # 2. features needed by exporting module to torchscript are added in PyTorch 1.6 or\n        # later version, `Conv2d` in these PyTorch versions has already supported empty inputs.\n        if not torch.jit.is_scripting():\n            # Dynamo doesn't support context managers yet\n            is_dynamo_compiling = check_if_dynamo_compiling()\n            if not is_dynamo_compiling:\n                with warnings.catch_warnings(record=True):\n                    if x.numel() == 0 and self.training:\n                        # https://github.com/pytorch/pytorch/issues/12013\n                        assert not isinstance(\n                            self.norm, torch.nn.SyncBatchNorm\n                        ), \"SyncBatchNorm does not support empty inputs!\"", "# init weights\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm3d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)", "def init_weights(module: nn.Module, name: str = ''):\n    # FIXME WIP determining if there's a better weight init\n    if isinstance(module, nn.Linear):\n        if 'qkv' in name:\n            # treat the weights of Q, K, V separately\n            val = math.sqrt(6. / float(module.weight.shape[0] // 3 + module.weight.shape[1]))\n            nn.init.uniform_(module.weight, -val, val)\n        elif 'head' in name:\n            nn.init.zeros_(module.weight)\n        else:\n            nn.init.xavier_uniform_(module.weight)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif hasattr(module, 'init_weights'):\n        module.init_weights()", "layer_cfg = LazyCall(nn.Conv2d)(in_channels=32, out_channels=32)\n        layer_cfg.out_channels = 64   # can edit it afterwards\n        layer = instantiate(layer_cfg)\n    \"\"\"", "\"\"\" modules \"\"\"\n        modules = {}\n        # batch norm\n        if self.use_bn:\n            if self.bn_before_weight:\n                modules[\"bn\"] = nn.BatchNorm1d(in_features)\n            else:\n                modules[\"bn\"] = nn.BatchNorm1d(out_features)\n        else:\n            modules[\"bn\"] = None\n        # activation\n        modules[\"act\"] = build_activation(self.act_func, self.ops_list[0] != \"act\")\n        # dropout\n        if self.dropout_rate > 0:\n            modules[\"dropout\"] = nn.Dropout(self.dropout_rate, inplace=True)\n        else:\n            modules[\"dropout\"] = None\n        # linear\n        modules[\"weight\"] = {\n            \"linear\": nn.Linear(self.in_features, self.out_features, self.bias)\n        }", "NOTE: When use_conv=True, expects 2D NCHW tensors, otherwise N*C expected.\n    \"\"\"\n    def __init__(\n            self,\n            in_features,\n            hidden_features=None,\n            out_features=None,\n            act_layer=nn.GELU,\n            norm_layer=None,\n            bias=True,\n            drop=0.,\n            use_conv=False,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        bias = to_2tuple(bias)\n        drop_probs = to_2tuple(drop)\n        linear_layer = partial(nn.Conv2d, kernel_size=1) if use_conv else nn.Linear", "def fuse_conv_and_bn(conv, bn):\n    \"\"\"\n    Fuses Conv2d and BatchNorm2d layers into a single Conv2d layer.", "def reset(self, num_classes: int, pool_type: Optional[str] = None, reset_other: bool = False):\n        if pool_type is not None:\n            self.pool_type = pool_type\n        if reset_other:\n            self.norm = nn.Identity()\n            self.pre_logits = nn.Identity()\n            self.num_features = self.in_features\n        self.fc = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()", "self.conv1 = Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=stride,\n            padding=1,\n            bias=False,\n            norm=get_norm(norm, out_channels),\n        )", "def __init__(\n            self,\n            dim,\n            num_classes=1000,\n            mlp_ratio=4,\n            act_layer=SquaredReLU,\n            norm_layer=LayerNorm,\n            drop_rate=0.,\n            bias=True\n    ):\n        super().__init__()\n        hidden_features = int(mlp_ratio * dim)\n        self.fc1 = nn.Linear(dim, hidden_features, bias=bias)\n        self.act = act_layer()\n        self.norm = norm_layer(hidden_features)\n        self.fc2 = nn.Linear(hidden_features, num_classes, bias=bias)\n        self.head_drop = nn.Dropout(drop_rate)", "if self.stride > 1:\n            self.branch1 = nn.Sequential(\n                self.depthwise_conv(inp, inp, kernel_size=3, stride=self.stride, padding=1),\n                nn.BatchNorm2d(inp),\n                nn.Conv2d(inp, branch_features, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(branch_features),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            self.branch1 = nn.Sequential()", "self.num_features = self.head_hidden_size = embed_dim\n        self.norm = norm_layer(embed_dim)\n        self.head = nn.Sequential(OrderedDict([\n            ('drop', nn.Dropout(self.drop_rate)),\n            ('fc', nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity())\n        ]))", "super().__init__(*blocks)\n        for layer in self.modules():\n            if isinstance(layer, nn.Conv2d):\n                nn.init.kaiming_normal_(layer.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n                if layer.bias is not None:\n                    nn.init.zeros_(layer.bias)", "from detectron2.layers import CNNBlockBase, Conv2d, get_norm\nfrom detectron2.modeling.backbone.fpn import _assert_strides_are_log2_contiguous", "input_channels = 3\n        output_channels = self._stage_out_channels[0]\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(input_channels, output_channels, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(output_channels),\n            nn.ReLU(inplace=True),\n        )\n        input_channels = output_channels", "if self.parallel_depth > 0:\n                # CoaT series: Aggregate features of last three scales for classification.\n                assert embed_dims[1] == embed_dims[2] == embed_dims[3]\n                self.aggregate = torch.nn.Conv1d(in_channels=3, out_channels=1, kernel_size=1)\n                self.head_drop = nn.Dropout(drop_rate)\n                self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n            else:\n                # CoaT-Lite series: Use feature of last scale for classification.\n                self.aggregate = None\n                self.head_drop = nn.Dropout(drop_rate)\n                self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()", "self.qkv = nn.Linear(dim, dim_out * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim_out, dim_out)", "self.conv3 = nn.Conv2d(group_width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = norm_layer(planes*4)\n        self.act3 = act_layer(inplace=True)\n        self.downsample = downsample", "lastblock = block_setting[-1]\n        lastconv_output_channels = (\n            lastblock.out_channels if lastblock.out_channels is not None else lastblock.input_channels\n        )\n        self.classifier = nn.Sequential(\n            norm_layer(lastconv_output_channels), nn.Flatten(1), nn.Linear(lastconv_output_channels, num_classes)\n        )", "class ConvPosEnc(nn.Module):\n    \"\"\" Convolutional Position Encoding. \n        Note: This module is similar to the conditional position encoding in CPVT.\n    \"\"\"\n    def __init__(self, dim, k=3):\n        super(ConvPosEnc, self).__init__()\n        self.proj = nn.Conv2d(dim, dim, k, 1, k//2, groups=dim)", "self.qkv = nn.Linear(dim, dim_out * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim_out, dim_out)", "Based on: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404\n    \"\"\"\n    def __init__(\n            self,\n            dim,\n            seq_len,\n            mlp_ratio=4,\n            mlp_layer=Mlp,\n            norm_layer=Affine,\n            act_layer=nn.GELU,\n            init_values=1e-4,\n            drop=0.,\n            drop_path=0.,\n    ):\n        super().__init__()\n        channel_dim = int(dim * mlp_ratio)\n        self.norm1 = norm_layer(dim)\n        self.linear_tokens = nn.Linear(seq_len, seq_len)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        self.mlp_channels = mlp_layer(dim, channel_dim, act_layer=act_layer, drop=drop)\n        self.ls1 = nn.Parameter(init_values * torch.ones(dim))\n        self.ls2 = nn.Parameter(init_values * torch.ones(dim))", "if not linear_attn:\n            self.pool = None\n            if sr_ratio > 1:\n                self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n                self.norm = nn.LayerNorm(dim)\n            else:\n                self.sr = None\n                self.norm = None\n            self.act = None\n        else:\n            self.pool = nn.AdaptiveAvgPool2d(7)\n            self.sr = nn.Conv2d(dim, dim, kernel_size=1, stride=1)\n            self.norm = nn.LayerNorm(dim)\n            self.act = nn.GELU()", "self.fc = nn.Sequential(\n            OrderedDict(\n                [\n                    (\"reduce\", nn.Conv2d(self.channel, num_mid, 1, 1, 0, bias=True)),\n                    (\"relu\", nn.ReLU(inplace=True)),\n                    (\"expand\", nn.Conv2d(num_mid, self.channel, 1, 1, 0, bias=True)),\n                    (\"h_sigmoid\", Hsigmoid(inplace=True)),\n                ]\n            )\n        )", "def _weight_init(m):\n    if isinstance(m, nn.Conv2d):\n        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n    elif isinstance(m, nn.BatchNorm2d):\n        nn.init.constant_(m.weight, 1.)\n        nn.init.constant_(m.bias, 0.)", "if stride == 2:\n            self.shortcut = Downsample2d(in_chs, out_chs)\n        elif in_chs != out_chs:\n            self.shortcut = nn.Conv2d(in_chs, out_chs, kernel_size=1, bias=cfg.output_bias)\n        else:\n            self.shortcut = nn.Identity()", "kernel, bias = self._get_kernel_bias()\n        self.reparam_conv = nn.Conv2d(\n            in_channels=self.conv_kxk[0].conv.in_channels,\n            out_channels=self.conv_kxk[0].conv.out_channels,\n            kernel_size=self.conv_kxk[0].conv.kernel_size,\n            stride=self.conv_kxk[0].conv.stride,\n            padding=self.conv_kxk[0].conv.padding,\n            dilation=self.conv_kxk[0].conv.dilation,\n            groups=self.conv_kxk[0].conv.groups,\n            bias=True)\n        self.reparam_conv.weight.data = kernel\n        self.reparam_conv.bias.data = bias", "if dimension == 3:\n            conv_nd = nn.Conv3d\n            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))\n            bn = nn.GroupNorm  # (32, hidden_dim) #nn.BatchNorm3d\n        elif dimension == 2:\n            conv_nd = nn.Conv2d\n            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n            bn = nn.GroupNorm  # (32, hidden_dim)nn.BatchNorm2d\n        else:\n            conv_nd = nn.Conv1d\n            max_pool_layer = nn.MaxPool1d(kernel_size=2)\n            bn = nn.GroupNorm  # (32, hidden_dim)nn.BatchNorm1d", "@register_notrace_module  # reason: FX can't symbolically trace torch.arange in forward method\nclass PositionalEncodingFourier(nn.Module):\n    def __init__(self, hidden_dim=32, dim=768, temperature=10000):\n        super().__init__()\n        self.token_projection = nn.Conv2d(hidden_dim * 2, dim, kernel_size=1)\n        self.scale = 2 * math.pi\n        self.temperature = temperature\n        self.hidden_dim = hidden_dim\n        self.dim = dim", "self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim, bias=proj_bias)", "self.fc = nn.Linear(self.num_features, num_classes, bias=bias) if num_classes > 0 else nn.Identity()\n        self.head_dropout = nn.Dropout(drop_rate)", "class MetaNeXtStage(nn.Module):\n    def __init__(\n            self,\n            in_chs,\n            out_chs,\n            stride=2,\n            depth=2,\n            dilation=(1, 1),\n            drop_path_rates=None,\n            ls_init_value=1.0,\n            token_mixer=InceptionDWConv2d,\n            act_layer=nn.GELU,\n            norm_layer=None,\n            mlp_ratio=4,\n    ):\n        super().__init__()\n        self.grad_checkpointing = False\n        if stride > 1 or dilation[0] != dilation[1]:\n            self.downsample = nn.Sequential(\n                norm_layer(in_chs),\n                nn.Conv2d(\n                    in_chs,\n                    out_chs,\n                    kernel_size=2,\n                    stride=stride,\n                    dilation=dilation[0],\n                ),\n            )\n        else:\n            self.downsample = nn.Identity()", "def __init__(\n            self,\n            in_chs=3,\n            out_chs=96,\n            mid_norm: bool = True,\n            act_layer=nn.GELU,\n            norm_layer=LayerNorm,\n    ):\n        super().__init__()\n        self.conv1 = nn.Conv2d(\n            in_chs,\n            out_chs // 2,\n            kernel_size=3,\n            stride=2,\n            padding=1\n        )\n        self.norm1 = norm_layer(out_chs // 2) if mid_norm else None\n        self.act = act_layer()\n        self.conv2 = nn.Conv2d(\n            out_chs // 2,\n            out_chs,\n            kernel_size=3,\n            stride=2,\n            padding=1\n        )\n        self.norm2 = norm_layer(out_chs)", "self.project = Conv2d(\n            5 * out_channels,\n            out_channels,\n            kernel_size=1,\n            bias=use_bias,\n            norm=get_norm(norm, out_channels),\n            activation=deepcopy(activation),\n        )\n        weight_init.c2_xavier_fill(self.project)", "self.norm_mlp_in = norm_layer(dim)\n        self.mlp_in = Mlp(\n            in_features=dim,\n            hidden_features=int(dim * 4),\n            out_features=dim,\n            act_layer=act_layer,\n            drop=proj_drop,\n        )\n        self.legacy = legacy\n        if self.legacy:\n            self.norm1_proj = norm_layer(dim)\n            self.proj = nn.Linear(dim * num_pixel, dim_out, bias=True)\n            self.norm2_proj = None\n        else:\n            self.norm1_proj = norm_layer(dim * num_pixel)\n            self.proj = nn.Linear(dim * num_pixel, dim_out, bias=False)\n            self.norm2_proj = norm_layer(dim_out)", "dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n        # stage 1\n        if self.vit_stem:\n            self.stem = None\n            self.patch_embed1 = PatchEmbed(\n                img_size=img_size,\n                patch_size=patch_size,\n                in_chans=in_chans,\n                embed_dim=embed_dim,\n                norm_layer=embed_norm,\n                flatten=False,\n            )\n            img_size = [x // patch_size for x in img_size]\n        else:\n            if self.init_channels is None:\n                self.stem = None\n                self.patch_embed1 = PatchEmbed(\n                    img_size=img_size,\n                    patch_size=patch_size // 2,\n                    in_chans=in_chans,\n                    embed_dim=embed_dim // 2,\n                    norm_layer=embed_norm,\n                    flatten=False,\n                )\n                img_size = [x // (patch_size // 2) for x in img_size]\n            else:\n                self.stem = nn.Sequential(\n                    nn.Conv2d(in_chans, self.init_channels, 7, stride=2, padding=3, bias=False),\n                    nn.BatchNorm2d(self.init_channels),\n                    nn.ReLU(inplace=True)\n                )\n                img_size = [x // 2 for x in img_size]\n                self.patch_embed1 = PatchEmbed(\n                    img_size=img_size,\n                    patch_size=patch_size // 4,\n                    in_chans=self.init_channels,\n                    embed_dim=embed_dim // 2,\n                    norm_layer=embed_norm,\n                    flatten=False,\n                )\n                img_size = [x // (patch_size // 4) for x in img_size]", "NOTE: Intended for '2D' NCHW (use_conv=True) or NHWC (use_conv=False, channels-last) tensor layouts\n    \"\"\"\n    def __init__(\n            self,\n            in_features,\n            hidden_features=None,\n            out_features=None,\n            act_layer=nn.GELU,\n            bias=True,\n            drop=0.,\n            use_conv=False,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        bias = to_2tuple(bias)\n        drop_probs = to_2tuple(drop)\n        linear_layer = partial(nn.Conv2d, kernel_size=1) if use_conv else nn.Linear", "# Copied from\n# https://github.com/pytorch/vision/blob/master/torchvision/models/segmentation/deeplabv3.py\n# See https://arxiv.org/pdf/1706.05587.pdf for details\nclass ASPPConv(nn.Sequential):\n    def __init__(self, in_channels, out_channels, dilation):\n        modules = [\n            nn.Conv2d(\n                in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False\n            ),\n            nn.GroupNorm(32, out_channels),\n            nn.ReLU(),\n        ]\n        super(ASPPConv, self).__init__(*modules)", "for m in net.modules():\n        to_update_dict = {}\n        for name, sub_module in m.named_children():\n            if isinstance(sub_module, nn.Conv2d) and not sub_module.bias:\n                # only replace conv2d layers that are followed by normalization layers (i.e., no bias)\n                to_update_dict[name] = sub_module\n        for name, sub_module in to_update_dict.items():\n            m._modules[name] = MyConv2d(\n                sub_module.in_channels,\n                sub_module.out_channels,\n                sub_module.kernel_size,\n                sub_module.stride,\n                sub_module.padding,\n                sub_module.dilation,\n                sub_module.groups,\n                sub_module.bias,\n            )\n            # load weight\n            m._modules[name].load_state_dict(sub_module.state_dict())\n            # load requires_grad\n            m._modules[name].weight.requires_grad = sub_module.weight.requires_grad\n            if sub_module.bias is not None:\n                m._modules[name].bias.requires_grad = sub_module.bias.requires_grad\n    # set ws_eps\n    for m in net.modules():\n        if isinstance(m, MyConv2d):\n            m.WS_EPS = ws_eps", "self.img_size = img_size\n        self.patch_size = patch_size\n        assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \\\n            f\"img_size {img_size} should be divided by patch_size {patch_size}.\"\n        self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n        self.num_patches = self.H * self.W\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n        self.norm = nn.LayerNorm(embed_dim)", "def __init__(self, dim: int, norm_layer: Type[nn.Module] = nn.LayerNorm) -> None:\n        super(PatchMerging, self).__init__()\n        self.norm = norm_layer(4 * dim)\n        self.reduction = nn.Linear(in_features=4 * dim, out_features=2 * dim, bias=False)", "self.fc1 = nn.Linear(2048, 1024)\n        self.fc2 = nn.Linear(1024, num_classes)\n        self.dropout = nn.Dropout(p=dropout)", "if self.radix >= 1:\n            self.conv2 = SplitAttn(\n                group_width, group_width, kernel_size=3, stride=stride, padding=first_dilation,\n                dilation=first_dilation, groups=cardinality, radix=radix, norm_layer=norm_layer, drop_layer=drop_block)\n            self.bn2 = nn.Identity()\n            self.drop_block = nn.Identity()\n            self.act2 = nn.Identity()\n        else:\n            self.conv2 = nn.Conv2d(\n                group_width, group_width, kernel_size=3, stride=stride, padding=first_dilation,\n                dilation=first_dilation, groups=cardinality, bias=False)\n            self.bn2 = norm_layer(group_width)\n            self.drop_block = drop_block() if drop_block is not None else nn.Identity()\n            self.act2 = act_layer(inplace=True)\n        self.avd_last = nn.AvgPool2d(3, avd_stride, padding=1) if avd_stride > 0 and not avd_first else None", "class NormLinear(nn.Module):\n    def __init__(self, in_features, out_features, bias=True, std=0.02, drop=0.):\n        super().__init__()\n        self.bn = nn.BatchNorm1d(in_features)\n        self.drop = nn.Dropout(drop)\n        self.linear = nn.Linear(in_features, out_features, bias=bias)", "self.proj = nn.Conv2d(in_chans, self.in_dim, kernel_size=7, padding=3, stride=stride)\n        if self.legacy:\n            self.unfold = nn.Unfold(kernel_size=new_patch_size, stride=new_patch_size)\n        else:\n            self.unfold = nn.Unfold(kernel_size=patch_size, stride=patch_size)", "# remap renamed layers\n        k = k.replace('patch_embed', 'stem')\n        k = k.replace('rbr_conv', 'conv_kxk')\n        k = k.replace('rbr_scale', 'conv_scale')\n        k = k.replace('rbr_skip', 'identity')\n        k = k.replace('conv_exp', 'final_conv')  # to match byobnet, regnet, nfnet\n        k = k.replace('lkb_origin', 'large_conv')\n        k = k.replace('convffn', 'mlp')\n        k = k.replace('se.reduce', 'se.fc1')\n        k = k.replace('se.expand', 'se.fc2')\n        k = re.sub(r'layer_scale_([0-9])', r'layer_scale_\\1.gamma', k)\n        if k.endswith('layer_scale'):\n            k = k.replace('layer_scale', 'layer_scale.gamma')\n        k = k.replace('dist_head', 'head_dist')\n        if k.startswith('head.'):\n            if k == 'head.proj' and hasattr(model.head, 'fc') and isinstance(model.head.fc, nn.Linear):\n                # if CLIP projection, map to head.fc w/ bias = zeros\n                k = k.replace('head.proj', 'head.fc.weight')\n                v = v.T\n                out_dict['head.fc.bias'] = torch.zeros(v.shape[0])\n            else:\n                k = k.replace('head.', 'head.fc.')", "self.qkv = nn.Linear(in_features=dim, out_features=dim * 3, bias=True)\n        self.attn_drop = nn.Dropout(drop_attn)\n        self.proj = nn.Linear(in_features=dim, out_features=dim, bias=True)\n        self.proj_drop = nn.Dropout(drop_proj)\n        # meta network for positional encodings\n        self.meta_mlp = Mlp(\n            2,  # x, y\n            hidden_features=meta_hidden_dim,\n            out_features=num_heads,\n            act_layer=nn.ReLU,\n            drop=(0.125, 0.)  # FIXME should there be stochasticity, appears to 'overfit' without?\n        )\n        # NOTE old checkpoints used inverse of logit_scale ('tau') following the paper, see conversion fn\n        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones(num_heads)))\n        self._make_pair_wise_relative_positions()", "self.in_norm = norm_layer(dim)\n        self.in_proj = nn.Linear(dim, in_proj_out_dim, bias=qkv_bias)\n        self.in_split = [mlp_hidden_dim] + [dim] * 3\n        if qkv_bias:\n            self.register_buffer('qkv_bias', None)\n            self.register_parameter('mlp_bias', None)\n        else:\n            self.register_buffer('qkv_bias', torch.zeros(3 * dim), persistent=False)\n            self.mlp_bias = nn.Parameter(torch.zeros(mlp_hidden_dim))", "# Classifier head\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()", "def init_weight_vit(\n        module: nn.Module,\n        name: str,\n        init_bias: float = 0.02,\n        head_bias: float = 0.,\n        classifier_name: str = 'head'\n):\n    if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d)):\n        if name.startswith(classifier_name):\n            nn.init.zeros_(module.weight)\n            nn.init.constant_(module.bias, head_bias)\n        else:\n            nn.init.trunc_normal_(module.weight, std=0.02)\n            if isinstance(module, nn.Linear) and module.bias is not None:\n                nn.init.constant_(module.bias, init_bias)\n    elif hasattr(module, 'init_weights'):\n        module.init_weights()", "model.patch_embed.proj.weight.copy_(embed_conv_w)\n    model.patch_embed.proj.bias.copy_(_n2p(w[f'{prefix}embedding/bias']))\n    if model.cls_token is not None:\n        model.cls_token.copy_(_n2p(w[f'{prefix}cls'], t=False))\n    if big_vision:\n        pos_embed_w = _n2p(w[f'{prefix}pos_embedding'], t=False)\n    else:\n        pos_embed_w = _n2p(w[f'{prefix}Transformer/posembed_input/pos_embedding'], t=False)\n    if pos_embed_w.shape != model.pos_embed.shape:\n        num_prefix_tokens = 0 if getattr(model, 'no_embed_class', False) else getattr(model, 'num_prefix_tokens', 1)\n        pos_embed_w = resample_abs_pos_embed(  # resize pos embedding when different size from pretrained weights\n            pos_embed_w,\n            new_size=model.patch_embed.grid_size,\n            num_prefix_tokens=num_prefix_tokens,\n            interpolation=interpolation,\n            antialias=antialias,\n            verbose=True,\n        )\n    model.pos_embed.copy_(pos_embed_w)\n    model.norm.weight.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/scale']))\n    model.norm.bias.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/bias']))\n    if (isinstance(model.head, nn.Linear) and\n            f'{prefix}head/bias' in w and\n            model.head.bias.shape[0] == w[f'{prefix}head/bias'].shape[-1]):\n        model.head.weight.copy_(_n2p(w[f'{prefix}head/kernel']))\n        model.head.bias.copy_(_n2p(w[f'{prefix}head/bias']))\n    # NOTE representation layer has been removed, not used in latest 21k/1k pretrained weights\n    # if isinstance(getattr(model.pre_logits, 'fc', None), nn.Linear) and f'{prefix}pre_logits/bias' in w:\n    #     model.pre_logits.fc.weight.copy_(_n2p(w[f'{prefix}pre_logits/kernel']))\n    #     model.pre_logits.fc.bias.copy_(_n2p(w[f'{prefix}pre_logits/bias']))\n    if model.attn_pool is not None:\n        block_prefix = f'{prefix}MAPHead_0/'\n        mha_prefix = block_prefix + f'MultiHeadDotProductAttention_0/'\n        model.attn_pool.latent.copy_(_n2p(w[f'{block_prefix}probe'], t=False))\n        model.attn_pool.kv.weight.copy_(torch.cat([\n            _n2p(w[f'{mha_prefix}{n}/kernel'], t=False).flatten(1).T for n in ('key', 'value')]))\n        model.attn_pool.kv.bias.copy_(torch.cat([\n            _n2p(w[f'{mha_prefix}{n}/bias'], t=False).reshape(-1) for n in ('key', 'value')]))\n        model.attn_pool.q.weight.copy_(_n2p(w[f'{mha_prefix}query/kernel'], t=False).flatten(1).T)\n        model.attn_pool.q.bias.copy_(_n2p(w[f'{mha_prefix}query/bias'], t=False).reshape(-1))\n        model.attn_pool.proj.weight.copy_(_n2p(w[f'{mha_prefix}out/kernel']).flatten(1))\n        model.attn_pool.proj.bias.copy_(_n2p(w[f'{mha_prefix}out/bias']))\n        model.attn_pool.norm.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/scale']))\n        model.attn_pool.norm.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/bias']))\n        for r in range(2):\n            getattr(model.attn_pool.mlp, f'fc{r + 1}').weight.copy_(_n2p(w[f'{block_prefix}MlpBlock_0/Dense_{r}/kernel']))\n            getattr(model.attn_pool.mlp, f'fc{r + 1}').bias.copy_(_n2p(w[f'{block_prefix}MlpBlock_0/Dense_{r}/bias']))", "self.fix_init_weight()\n        if isinstance(self.head, nn.Linear):\n            trunc_normal_(self.head.weight, std=.02)\n            self.head.weight.data.mul_(head_init_scale)\n            self.head.bias.data.mul_(head_init_scale)", "self.conv2 = Conv2d(\n            bottleneck_channels,\n            bottleneck_channels,\n            kernel_size=3,\n            stride=stride_3x3,\n            padding=1 * dilation,\n            bias=False,\n            groups=num_groups,\n            dilation=dilation,\n            norm=get_norm(norm, bottleneck_channels),\n        )", "self.qk = nn.Linear(dim, dim * 2, bias=qkv_bias)\n        self.v = nn.Linear(dim, dim, bias=qkv_bias)", "from .activations import *\nfrom .adaptive_avgmax_pool import \\\n    adaptive_avgmax_pool2d, select_adaptive_pool2d, AdaptiveAvgMaxPool2d, SelectAdaptivePool2d\nfrom .attention import Attention, AttentionRope\nfrom .attention2d import MultiQueryAttention2d, Attention2d, MultiQueryAttentionV2\nfrom .attention_pool import AttentionPoolLatent\nfrom .attention_pool2d import AttentionPool2d, RotAttentionPool2d, RotaryEmbedding\nfrom .blur_pool import BlurPool2d, create_aa\nfrom .classifier import create_classifier, ClassifierHead, NormMlpClassifierHead, ClNormMlpClassifierHead\nfrom .cond_conv2d import CondConv2d, get_condconv_initializer\nfrom .config import is_exportable, is_scriptable, is_no_jit, use_fused_attn, \\\n    set_exportable, set_scriptable, set_no_jit, set_layer_config, set_fused_attn, \\\n    set_reentrant_ckpt, use_reentrant_ckpt\nfrom .conv2d_same import Conv2dSame, conv2d_same\nfrom .conv_bn_act import ConvNormAct, ConvNormActAa, ConvBnAct\nfrom .create_act import create_act_layer, get_act_layer, get_act_fn\nfrom .create_attn import get_attn, create_attn\nfrom .create_conv2d import create_conv2d\nfrom .create_norm import get_norm_layer, create_norm_layer\nfrom .create_norm_act import get_norm_act_layer, create_norm_act_layer, get_norm_act_layer\nfrom .drop import DropBlock2d, DropPath, drop_block_2d, drop_path\nfrom .eca import EcaModule, CecaModule, EfficientChannelAttn, CircularEfficientChannelAttn\nfrom .evo_norm import EvoNorm2dB0, EvoNorm2dB1, EvoNorm2dB2,\\\n    EvoNorm2dS0, EvoNorm2dS0a, EvoNorm2dS1, EvoNorm2dS1a, EvoNorm2dS2, EvoNorm2dS2a\nfrom .fast_norm import is_fast_norm, set_fast_norm, fast_group_norm, fast_layer_norm\nfrom .filter_response_norm import FilterResponseNormTlu2d, FilterResponseNormAct2d\nfrom .format import Format, get_channel_dim, get_spatial_dim, nchw_to, nhwc_to\nfrom .gather_excite import GatherExcite\nfrom .global_context import GlobalContext\nfrom .grid import ndgrid, meshgrid\nfrom .helpers import to_ntuple, to_2tuple, to_3tuple, to_4tuple, make_divisible, extend_tuple\nfrom .hybrid_embed import HybridEmbed, HybridEmbedWithSize\nfrom .inplace_abn import InplaceAbn\nfrom .layer_scale import LayerScale, LayerScale2d\nfrom .linear import Linear\nfrom .mixed_conv2d import MixedConv2d\nfrom .mlp import Mlp, GluMlp, GatedMlp, SwiGLU, SwiGLUPacked, ConvMlp, GlobalResponseNormMlp\nfrom .non_local_attn import NonLocalAttn, BatNonLocalAttn\nfrom .norm import GroupNorm, GroupNorm1, LayerNorm, LayerNorm2d, RmsNorm, RmsNorm2d, SimpleNorm, SimpleNorm2d\nfrom .norm_act import BatchNormAct2d, GroupNormAct, GroupNorm1Act, LayerNormAct, LayerNormAct2d,\\\n    SyncBatchNormAct, convert_sync_batchnorm, FrozenBatchNormAct2d, freeze_batch_norm_2d, unfreeze_batch_norm_2d\nfrom .padding import get_padding, get_same_padding, pad_same\nfrom .patch_dropout import PatchDropout\nfrom .patch_embed import PatchEmbed, PatchEmbedWithSize, resample_patch_embed\nfrom .pool1d import global_pool_nlc\nfrom .pool2d_same import AvgPool2dSame, create_pool2d\nfrom .pos_embed import resample_abs_pos_embed, resample_abs_pos_embed_nhwc\nfrom .pos_embed_rel import RelPosMlp, RelPosBias, RelPosBiasTf, gen_relative_position_index, gen_relative_log_coords, \\\n    resize_rel_pos_bias_table, resize_rel_pos_bias_table_simple, resize_rel_pos_bias_table_levit\nfrom .pos_embed_sincos import pixel_freq_bands, freq_bands, build_sincos2d_pos_embed, build_fourier_pos_embed, \\\n    build_rotary_pos_embed, apply_rot_embed, apply_rot_embed_cat, apply_rot_embed_list, apply_keep_indices_nlc, \\\n    FourierEmbed, RotaryEmbedding, RotaryEmbeddingCat\nfrom .squeeze_excite import SEModule, SqueezeExcite, EffectiveSEModule, EffectiveSqueezeExcite\nfrom .selective_kernel import SelectiveKernel\nfrom .separable_conv import SeparableConv2d, SeparableConvNormAct\nfrom .space_to_depth import SpaceToDepth, DepthToSpace\nfrom .split_attn import SplitAttn\nfrom .split_batchnorm import SplitBatchNorm2d, convert_splitbn_model\nfrom .std_conv import StdConv2d, StdConv2dSame, ScaledStdConv2d, ScaledStdConv2dSame\nfrom .test_time_pool import TestTimePoolHead, apply_test_time_pool\nfrom .trace_utils import _assert, _float_to_int\nfrom .typing import LayerType, PadType\nfrom .weight_init import trunc_normal_, trunc_normal_tf_, variance_scaling_, lecun_normal_, \\\n    init_weight_jax, init_weight_vit", "self.conv1 = nn.Conv2d(inplanes, group_width, kernel_size=1, bias=False)\n        self.bn1 = norm_layer(group_width)\n        self.act1 = act_layer(inplace=True)\n        self.avd_first = nn.AvgPool2d(3, avd_stride, padding=1) if avd_stride > 0 and avd_first else None", "def init_weights_vit_moco(module: nn.Module, name: str = '') -> None:\n    \"\"\" ViT weight initialization, matching moco-v3 impl minus fixed PatchEmbed \"\"\"\n    if isinstance(module, nn.Linear):\n        if 'qkv' in name:\n            # treat the weights of Q, K, V separately\n            val = math.sqrt(6. / float(module.weight.shape[0] // 3 + module.weight.shape[1]))\n            nn.init.uniform_(module.weight, -val, val)\n        else:\n            nn.init.xavier_uniform_(module.weight)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif hasattr(module, 'init_weights'):\n        module.init_weights()", "self.cls_logits = nn.Conv2d(in_channels, num_anchors * num_classes, kernel_size=3, stride=1, padding=1)\n        torch.nn.init.normal_(self.cls_logits.weight, std=0.01)\n        torch.nn.init.constant_(self.cls_logits.bias, -math.log((1 - prior_probability) / prior_probability))", "self.conv1 = Conv2d(\n            in_channels,\n            bottleneck_channels,\n            kernel_size=1,\n            stride=stride_1x1,\n            bias=False,\n            norm=get_norm(norm, bottleneck_channels),\n        )", "from detectron2.layers import Conv2d, ShapeSpec, get_norm\nfrom detectron2.modeling import ROI_HEADS_REGISTRY, StandardROIHeads\nfrom detectron2.modeling.poolers import ROIPooler\nfrom detectron2.modeling.roi_heads import select_foreground_proposals\nfrom detectron2.structures import ImageList, Instances", "class QuickGELU(nn.Module):\n    \"\"\"Applies the Gaussian Error Linear Units function (w/ dummy inplace arg)\n    \"\"\"\n    def __init__(self, inplace: bool = False):\n        super(QuickGELU, self).__init__()", "# Classifier head\n        self.norm = norm_layer(embed_dim)\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()", "self.conv2d = nn.Conv2d(448, 2080, kernel_size=1, stride=1)\n        self.relu = None if no_relu else nn.ReLU()", "def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n        if self.head_dist is not None:\n            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()", "def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()", "assert stem_type in ('patch', 'overlap', 'overlap_tiered', 'overlap_act')\n        if stem_type == 'patch':\n            # NOTE: this stem is a minimal form of ViT PatchEmbed, as used in SwinTransformer w/ patch_size = 4\n            self.stem = nn.Sequential(\n                nn.Conv2d(in_chans, dims[0], kernel_size=patch_size, stride=patch_size, bias=conv_bias),\n                norm_layer(dims[0]),\n            )\n            stem_stride = patch_size\n        else:\n            mid_chs = make_divisible(dims[0] // 2) if 'tiered' in stem_type else dims[0]\n            self.stem = nn.Sequential(*filter(None, [\n                nn.Conv2d(in_chans, mid_chs, kernel_size=3, stride=2, padding=1, bias=conv_bias),\n                act_layer() if 'act' in stem_type else None,\n                nn.Conv2d(mid_chs, dims[0], kernel_size=3, stride=2, padding=1, bias=conv_bias),\n                norm_layer(dims[0]),\n            ]))\n            stem_stride = 4", "for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.kaiming_uniform_(m.weight, mode=\"fan_out\", nonlinearity=\"sigmoid\")\n                nn.init.zeros_(m.bias)", "self.predictor = Conv2d(conv_dims, num_classes, kernel_size=1, stride=1, padding=0)\n        nn.init.normal_(self.predictor.weight, 0, 0.001)\n        nn.init.constant_(self.predictor.bias, 0)", "self.conv_layers = []\n        if input_channels > conv_dim:\n            self.reduce_channel_dim_conv = Conv2d(\n                input_channels,\n                conv_dim,\n                kernel_size=1,\n                stride=1,\n                padding=0,\n                bias=True,\n                activation=F.relu,\n            )\n            self.conv_layers.append(self.reduce_channel_dim_conv)", "def reshape_classifier_output(model, n=1000):\n    \"\"\"Reshapes last layer of model to match class count 'n', supporting Classify, Linear, Sequential types.\"\"\"\n    from models.common import Classify", "self.qkv = nn.Conv2d(\n            dim,\n            num_heads * self.dim_qk + self.dim_qk + self.dim_v,\n            kernel_size=1, bias=qkv_bias)\n        self.norm_q = nn.BatchNorm2d(num_heads * self.dim_qk)\n        self.norm_v = nn.BatchNorm2d(self.dim_v)", "def __init__(self, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n        self.norm = norm_layer(4 * dim)", "self.fcs = []\n        for k, fc_dim in enumerate(fc_dims):\n            fc = nn.Linear(input_dim, fc_dim)\n            self.add_module(\"fc{}\".format(k + 1), fc)\n            self.fcs.append(fc)\n            input_dim = fc_dim", "self.norm = nn.ModuleList([norm_layer(embed_dim[i]) for i in range(self.num_branches)])\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.ModuleList([\n            nn.Linear(embed_dim[i], num_classes) if num_classes > 0 else nn.Identity()\n            for i in range(self.num_branches)])", "# weights in nn.Conv2d mode\n    'levit_conv_128s.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        pool_size=(4, 4),\n    ),\n    'levit_conv_128.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        pool_size=(4, 4),\n    ),\n    'levit_conv_192.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        pool_size=(4, 4),\n    ),\n    'levit_conv_256.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        pool_size=(4, 4),\n    ),\n    'levit_conv_384.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        pool_size=(4, 4),\n    ),", "self.conv_attn = nn.Conv2d(channels, 1, kernel_size=1, bias=True) if use_attn else None", "class Stem(nn.Module):\n    def __init__(\n            self,\n            in_chs: int = 3,\n            out_chs: int = 96,\n            act_layer: Callable = nn.GELU,\n            norm_layer: Callable = LayerNorm2d,  # NOTE stem in NCHW\n    ):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_chs, out_chs, kernel_size=3, stride=2, padding=1)\n        self.down = Downsample2d(out_chs, act_layer=act_layer, norm_layer=norm_layer)", "# Stem pooling. The name 'maxpool' remains for weight compatibility.\n        if replace_stem_pool:\n            self.maxpool = nn.Sequential(*filter(None, [\n                nn.Conv2d(inplanes, inplanes, 3, stride=1 if aa_layer else 2, padding=1, bias=False),\n                create_aa(aa_layer, channels=inplanes, stride=2) if aa_layer is not None else None,\n                norm_layer(inplanes),\n                act_layer(inplace=True),\n            ]))\n        else:\n            if aa_layer is not None:\n                if issubclass(aa_layer, nn.AvgPool2d):\n                    self.maxpool = aa_layer(2)\n                else:\n                    self.maxpool = nn.Sequential(*[\n                        nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n                        aa_layer(channels=inplanes, stride=2)])\n            else:\n                self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)", "def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)", "transition_layers = []\n        for i in range(num_branches_cur):\n            if i < num_branches_pre:\n                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n                    transition_layers.append(nn.Sequential(\n                        nn.Conv2d(num_channels_pre_layer[i], num_channels_cur_layer[i], 3, 1, 1, bias=False),\n                        nn.BatchNorm2d(num_channels_cur_layer[i], momentum=_BN_MOMENTUM),\n                        nn.ReLU(inplace=True)))\n                else:\n                    transition_layers.append(nn.Identity())\n            else:\n                conv3x3s = []\n                for j in range(i + 1 - num_branches_pre):\n                    _in_chs = num_channels_pre_layer[-1]\n                    _out_chs = num_channels_cur_layer[i] if j == i - num_branches_pre else _in_chs\n                    conv3x3s.append(nn.Sequential(\n                        nn.Conv2d(_in_chs, _out_chs, 3, 2, 1, bias=False),\n                        nn.BatchNorm2d(_out_chs, momentum=_BN_MOMENTUM),\n                        nn.ReLU(inplace=True)))\n                transition_layers.append(nn.Sequential(*conv3x3s))", "self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)", "class LinearAttention(nn.Module):\n    \"\"\"\n    Linear attention operation from: https://arxiv.org/pdf/2006.16236.pdf\n    Canonical implementation reference: https://github.com/idiap/fast-transformers/blob/master/fast_transformers/attention/linear_attention.py\n    LoFTR implementation reference: https://github.com/zju3dv/LoFTR/blob/2122156015b61fbb650e28b58a958e4d632b1058/src/loftr/loftr_module/linear_attention.py\n    \"\"\"", "Args:\n            input_shape (ShapeSpec): shape of the input feature to this module\n            box2box_transform (Box2BoxTransform or Box2BoxTransformRotated):\n            num_classes (int): number of foreground classes\n            test_score_thresh (float): threshold to filter predictions results.\n            test_nms_thresh (float): NMS threshold for prediction results.\n            test_topk_per_image (int): number of top predictions to produce per image.\n            cls_agnostic_bbox_reg (bool): whether to use class agnostic for bbox regression\n            smooth_l1_beta (float): transition point from L1 to L2 loss. Only used if\n                `box_reg_loss_type` is \"smooth_l1\"\n            box_reg_loss_type (str): Box regression loss type. One of: \"smooth_l1\", \"giou\",\n                \"diou\", \"ciou\"\n            loss_weight (float|dict): weights to use for losses. Can be single float for weighting\n                all losses, or a dict of individual weightings. Valid dict keys are:\n                    * \"loss_cls\": applied to classification loss\n                    * \"loss_box_reg\": applied to box regression loss\n            use_fed_loss (bool): whether to use federated loss which samples additional negative\n                classes to calculate the loss\n            use_sigmoid_ce (bool): whether to calculate the loss using weighted average of binary\n                cross entropy with logits. This could be used together with federated loss\n            get_fed_loss_cls_weights (Callable): a callable which takes dataset name and frequency\n                weight power, and returns the probabilities to sample negative classes for\n                federated loss. The implementation can be found in\n                detectron2/data/detection_utils.py\n            fed_loss_num_classes (int): number of federated classes to keep in total\n        \"\"\"\n        super().__init__()\n        if isinstance(input_shape, int):  # some backward compatibility\n            input_shape = ShapeSpec(channels=input_shape)\n        self.num_classes = num_classes\n        input_size = input_shape.channels * (input_shape.width or 1) * (input_shape.height or 1)\n        # prediction layer for num_classes foreground classes and one background class (hence + 1)\n        self.cls_score = nn.Linear(input_size, num_classes + 1)\n        num_bbox_reg_classes = 1 if cls_agnostic_bbox_reg else num_classes\n        box_dim = len(box2box_transform.weights)\n        self.bbox_pred = nn.Linear(input_size, num_bbox_reg_classes * box_dim)", "if stride == 1 and in_channels == out_channels:\n            self.downsample = IdentityLayer(in_channels, out_channels)\n        elif self.downsample_mode == \"conv\":\n            self.downsample = nn.Sequential(\n                OrderedDict(\n                    [\n                        (\n                            \"conv\",\n                            nn.Conv2d(\n                                in_channels, out_channels, 1, stride, 0, bias=False\n                            ),\n                        ),\n                        (\"bn\", nn.BatchNorm2d(out_channels)),\n                    ]\n                )\n            )\n        elif self.downsample_mode == \"avgpool_conv\":\n            self.downsample = nn.Sequential(\n                OrderedDict(\n                    [\n                        (\n                            \"avg_pool\",\n                            nn.AvgPool2d(\n                                kernel_size=stride,\n                                stride=stride,\n                                padding=0,\n                                ceil_mode=True,\n                            ),\n                        ),\n                        (\n                            \"conv\",\n                            nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False),\n                        ),\n                        (\"bn\", nn.BatchNorm2d(out_channels)),\n                    ]\n                )\n            )\n        else:\n            raise NotImplementedError", "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)", "@torch.jit.ignore\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(\n                    m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)", "@torch.no_grad()\n    def fuse(self):\n        c, bn = self._modules.values()\n        w = bn.weight / (bn.running_var + bn.eps) ** 0.5\n        w = c.weight * w[:, None, None, None]\n        b = bn.bias - bn.running_mean * bn.weight / (bn.running_var + bn.eps) ** 0.5\n        m = nn.Conv2d(\n            w.size(1) * self.c.groups,\n            w.size(0),\n            w.shape[2:],\n            stride=self.c.stride,\n            padding=self.c.padding,\n            dilation=self.c.dilation,\n            groups=self.c.groups,\n            device=c.weight.device,\n        )\n        m.weight.data.copy_(w)\n        m.bias.data.copy_(b)\n        return m", "Args:\n        in_chans (int): Number of input image channels. Default: 3.\n        num_classes (int): Number of classes for classification head. Default: 1000.\n        depths (list or tuple): Number of blocks at each stage. Default: [3, 3, 9, 3].\n        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 576].\n        downsample_layers: (list or tuple): Downsampling layers before each stage.\n        drop_path_rate (float): Stochastic depth rate. Default: 0.\n        output_norm: norm before classifier head. Default: partial(nn.LayerNorm, eps=1e-6).\n        head_fn: classification head. Default: nn.Linear.\n        head_dropout (float): dropout for MLP classifier. Default: 0.\n    \"\"\"", "class InceptionAux(nn.Module):\n    def __init__(\n        self, in_channels: int, num_classes: int, conv_block: Optional[Callable[..., nn.Module]] = None\n    ) -> None:\n        super().__init__()\n        if conv_block is None:\n            conv_block = BasicConv2d\n        self.conv0 = conv_block(in_channels, 128, kernel_size=1)\n        self.conv1 = conv_block(128, 768, kernel_size=5)\n        self.conv1.stddev = 0.01  # type: ignore[assignment]\n        self.fc = nn.Linear(768, num_classes)\n        self.fc.stddev = 0.001  # type: ignore[assignment]", "class PosConv(nn.Module):\n    # PEG  from https://arxiv.org/abs/2102.10882\n    def __init__(self, in_chans, embed_dim=768, stride=1):\n        super(PosConv, self).__init__()\n        self.proj = nn.Sequential(\n            nn.Conv2d(in_chans, embed_dim, 3, stride, 1, bias=True, groups=embed_dim),\n        )\n        self.stride = stride", "self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim, bias=proj_bias)", "num_scale_convs = max(1, scale - 1)\n        convs = []\n        bns = []\n        for _ in range(num_scale_convs):\n            convs.append(nn.Conv2d(\n                mid_planes, mid_planes, kernel_size=3,\n                stride=stride, padding=dilation, dilation=dilation, groups=cardinality, bias=False))\n            bns.append(nn.BatchNorm2d(mid_planes))\n        self.convs = nn.ModuleList(convs)\n        self.bns = nn.ModuleList(bns)\n        self.pool = nn.AvgPool2d(kernel_size=3, stride=stride, padding=1) if self.is_first else None", "class SEModule(nn.Module):\n    \"\"\" SE Module as defined in original SE-Nets with a few additions\n    Additions include:\n        * divisor can be specified to keep channels % div == 0 (default: 8)\n        * reduction channels can be specified directly by arg (if rd_channels is set)\n        * reduction channels can be specified by float rd_ratio (default: 1/16)\n        * global max pooling can be added to the squeeze aggregation\n        * customizable activation, normalization, and gate layer\n    \"\"\"\n    def __init__(\n            self, channels, rd_ratio=1. / 16, rd_channels=None, rd_divisor=8, add_maxpool=False,\n            bias=True, act_layer=nn.ReLU, norm_layer=None, gate_layer='sigmoid'):\n        super(SEModule, self).__init__()\n        self.add_maxpool = add_maxpool\n        if not rd_channels:\n            rd_channels = make_divisible(channels * rd_ratio, rd_divisor, round_limit=0.)\n        self.fc1 = nn.Conv2d(channels, rd_channels, kernel_size=1, bias=bias)\n        self.bn = norm_layer(rd_channels) if norm_layer else nn.Identity()\n        self.act = create_act_layer(act_layer, inplace=True)\n        self.fc2 = nn.Conv2d(rd_channels, channels, kernel_size=1, bias=bias)\n        self.gate = create_act_layer(gate_layer)", "def _init_weights(module, name, zero_init_last=False):\n    if isinstance(module, nn.Conv2d):\n        nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Linear):\n        nn.init.normal_(module.weight, mean=0.0, std=0.01)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif zero_init_last and hasattr(module, 'zero_init_last'):\n        module.zero_init_last()", "def _make_out_layer(self, in_channels, out_channels, with_block=True, block=ResidualBlock):\n        layers = []\n        if with_block:\n            layers.append(block(in_channels, in_channels, norm_layer=nn.BatchNorm2d, stride=1))\n        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n        return nn.Sequential(*layers)", "self.qkv = nn.Conv2d(dim, self.dim_out_qk * 2 + self.dim_out_v, 1, bias=qkv_bias)", "self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)  # Note: attn_drop is actually not used.\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)", "self.fc2 = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()", "def conv2d(w_in, w_out, k, *, stride=1, groups=1, bias=False):\n    \"\"\"Helper for building a conv2d layer.\"\"\"\n    assert k % 2 == 1, \"Only odd size kernels supported to avoid padding issues.\"\n    s, p, g, b = stride, (k - 1) // 2, groups, bias\n    return nn.Conv2d(w_in, w_out, k, stride=s, padding=p, groups=g, bias=b)", "self.qkv = nn.Conv2d(dim, head_dim * num_heads * 3, 1, stride=1, padding=0, bias=False)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Conv2d(self.head_dim * self.num_heads, dim, 1, stride=1, padding=0, bias=False)\n        self.proj_drop = nn.Dropout(proj_drop)", "class GlobalResponseNormMlp(nn.Module):\n    \"\"\" MLP w/ Global Response Norm (see grn.py), nn.Linear or 1x1 Conv2d", "def _init_weights(module: nn.Module, name: str, head_bias: float = 0., flax=False):\n    if isinstance(module, nn.Linear):\n        if name.startswith('head'):\n            nn.init.zeros_(module.weight)\n            nn.init.constant_(module.bias, head_bias)\n        else:\n            if flax:\n                # Flax defaults\n                lecun_normal_(module.weight)\n                if module.bias is not None:\n                    nn.init.zeros_(module.bias)\n            else:\n                nn.init.xavier_uniform_(module.weight)\n                if module.bias is not None:\n                    if 'mlp' in name:\n                        nn.init.normal_(module.bias, std=1e-6)\n                    else:\n                        nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Conv2d):\n        lecun_normal_(module.weight)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, (nn.LayerNorm, nn.BatchNorm2d, nn.GroupNorm)):\n        nn.init.ones_(module.weight)\n        nn.init.zeros_(module.bias)\n    elif isinstance(module, (nn.RNN, nn.GRU, nn.LSTM)):\n        stdv = 1.0 / math.sqrt(module.hidden_size)\n        for weight in module.parameters():\n            nn.init.uniform_(weight, -stdv, stdv)\n    elif hasattr(module, 'init_weights'):\n        module.init_weights()", "# We add out_channels compared to raft.MaskPredictor\n    def __init__(self, *, in_channels: int, hidden_size: int, out_channels: int, multiplier: float = 0.25):\n        super(raft.MaskPredictor, self).__init__()\n        self.convrelu = Conv2dNormActivation(in_channels, hidden_size, norm_layer=None, kernel_size=3)\n        self.conv = nn.Conv2d(hidden_size, out_channels, kernel_size=1, padding=0)\n        self.multiplier = multiplier", "class OverlapPatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n    def __init__(self, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n        super().__init__()\n        patch_size = to_2tuple(patch_size)\n        assert max(patch_size) > stride, \"Set larger patch_size than stride\"\n        self.patch_size = patch_size\n        self.proj = nn.Conv2d(\n            in_chans, embed_dim, patch_size,\n            stride=stride, padding=(patch_size[0] // 2, patch_size[1] // 2))\n        self.norm = nn.LayerNorm(embed_dim)", "# SSD300 case - page 4, Fig 2 of SSD paper\n        extra = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.Conv2d(1024, 256, kernel_size=1),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(256, 512, kernel_size=3, padding=1, stride=2),  # conv8_2\n                    nn.ReLU(inplace=True),\n                ),\n                nn.Sequential(\n                    nn.Conv2d(512, 128, kernel_size=1),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=2),  # conv9_2\n                    nn.ReLU(inplace=True),\n                ),\n                nn.Sequential(\n                    nn.Conv2d(256, 128, kernel_size=1),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(128, 256, kernel_size=3),  # conv10_2\n                    nn.ReLU(inplace=True),\n                ),\n                nn.Sequential(\n                    nn.Conv2d(256, 128, kernel_size=1),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(128, 256, kernel_size=3),  # conv11_2\n                    nn.ReLU(inplace=True),\n                ),\n            ]\n        )\n        if highres:\n            # Additional layers for the SSD512 case. See page 11, footernote 5.\n            extra.append(\n                nn.Sequential(\n                    nn.Conv2d(256, 128, kernel_size=1),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(128, 256, kernel_size=4),  # conv12_2\n                    nn.ReLU(inplace=True),\n                )\n            )\n        _xavier_init(extra)", "self.scale_heads = []\n        for in_feature in self.in_features:\n            head_ops = []\n            head_length = max(\n                1,\n                # pyre-fixme[6]: For 1st argument expected `Union[bytes, complex,\n                #  float, int, generic, str]` but got `Optional[int]`.\n                int(np.log2(feature_strides[in_feature]) - np.log2(self.common_stride)),\n            )\n            for k in range(head_length):\n                conv = Conv2d(\n                    feature_channels[in_feature] if k == 0 else conv_dims,\n                    conv_dims,\n                    kernel_size=3,\n                    stride=1,\n                    padding=1,\n                    bias=not norm,\n                    norm=get_norm(norm, conv_dims),\n                    activation=F.relu,\n                )\n                weight_init.c2_msra_fill(conv)\n                head_ops.append(conv)\n                if feature_strides[in_feature] != self.common_stride:\n                    head_ops.append(\n                        nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)\n                    )\n            self.scale_heads.append(nn.Sequential(*head_ops))\n            self.add_module(in_feature, self.scale_heads[-1])\n        self.predictor = Conv2d(conv_dims, num_classes, kernel_size=1, stride=1, padding=0)\n        weight_init.c2_msra_fill(self.predictor)", "def __init__(self, in_channels, use_scale=True,  rd_ratio=1/8, rd_channels=None, rd_divisor=8, **kwargs):\n        super(NonLocalAttn, self).__init__()\n        if rd_channels is None:\n            rd_channels = make_divisible(in_channels * rd_ratio, divisor=rd_divisor)\n        self.scale = in_channels ** -0.5 if use_scale else 1.0\n        self.t = nn.Conv2d(in_channels, rd_channels, kernel_size=1, stride=1, bias=True)\n        self.p = nn.Conv2d(in_channels, rd_channels, kernel_size=1, stride=1, bias=True)\n        self.g = nn.Conv2d(in_channels, rd_channels, kernel_size=1, stride=1, bias=True)\n        self.z = nn.Conv2d(rd_channels, in_channels, kernel_size=1, stride=1, bias=True)\n        self.norm = nn.BatchNorm2d(in_channels)\n        self.reset_parameters()", "layers.append(torch.nn.Linear(in_dim, hidden_channels[-1], bias=bias))\n        layers.append(torch.nn.Dropout(dropout, **params))", "should_proj = stride != 1 or in_channels != out_channels\n        if should_proj:\n            proj = [nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=True)]\n            if stride == 2:\n                proj = [nn.AvgPool2d(kernel_size=3, stride=stride, padding=1)] + proj  # type: ignore\n            self.proj = nn.Sequential(*proj)\n        else:\n            self.proj = nn.Identity()  # type: ignore", "if self.expand_ratio == 1:\n            self.inverted_bottleneck = None\n        else:\n            self.inverted_bottleneck = nn.Sequential(\n                OrderedDict(\n                    [\n                        (\n                            \"conv\",\n                            nn.Conv2d(\n                                self.in_channels, feature_dim, 1, 1, 0, bias=False\n                            ),\n                        ),\n                        (\"bn\", nn.BatchNorm2d(feature_dim)),\n                        (\"act\", build_activation(self.act_func, inplace=True)),\n                    ]\n                )\n            )", "self.primary_conv = nn.Sequential(\n            nn.Conv2d(in_chs, init_chs, kernel_size, stride, kernel_size // 2, bias=False),\n            nn.BatchNorm2d(init_chs),\n            nn.ReLU(inplace=True) if relu else nn.Identity(),\n        )", "self.act = nn.GELU()\n        self.proj = nn.Conv2d(dim, dim, kernel_size=1)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.focal_layers = nn.ModuleList()", "self.fc6 = nn.Linear(in_channels, representation_size)\n        self.fc7 = nn.Linear(representation_size, representation_size)", "# def divs(n, m=None):\n#     m = m or n // 2\n#     if m == 1:\n#         return [1]\n#     if n % m == 0:\n#         return [m] + divs(n, m - 1)\n#     return divs(n, m - 1)\n#\n#\n# class FlexiPatchEmbed(nn.Module):\n#     \"\"\" 2D Image to Patch Embedding w/ Flexible Patch sizes (FlexiViT)\n#     FIXME WIP\n#     \"\"\"\n#     def __init__(\n#             self,\n#             img_size=240,\n#             patch_size=16,\n#             in_chans=3,\n#             embed_dim=768,\n#             base_img_size=240,\n#             base_patch_size=32,\n#             norm_layer=None,\n#             flatten=True,\n#             bias=True,\n#     ):\n#         super().__init__()\n#         self.img_size = to_2tuple(img_size)\n#         self.patch_size = to_2tuple(patch_size)\n#         self.num_patches = 0\n#\n#         # full range for 240 = (5, 6, 8, 10, 12, 14, 15, 16, 20, 24, 30, 40, 48)\n#         self.seqhw = (6, 8, 10, 12, 14, 15, 16, 20, 24, 30)\n#\n#         self.base_img_size = to_2tuple(base_img_size)\n#         self.base_patch_size = to_2tuple(base_patch_size)\n#         self.base_grid_size = tuple([i // p for i, p in zip(self.base_img_size, self.base_patch_size)])\n#         self.base_num_patches = self.base_grid_size[0] * self.base_grid_size[1]\n#\n#         self.flatten = flatten\n#         self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=self.patch_size, stride=self.patch_size, bias=bias)\n#         self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n#\n#     def forward(self, x):\n#         B, C, H, W = x.shape\n#\n#         if self.patch_size == self.base_patch_size:\n#             weight = self.proj.weight\n#         else:\n#             weight = resample_patch_embed(self.proj.weight, self.patch_size)\n#         patch_size = self.patch_size\n#         x = F.conv2d(x, weight, bias=self.proj.bias, stride=patch_size)\n#         if self.flatten:\n#             x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC\n#         x = self.norm(x)\n#         return x", "def reset_parameters(self):\n        for name, m in self.named_modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(\n                    m.weight, mode='fan_out', nonlinearity='relu')\n                if len(list(m.parameters())) > 1:\n                    nn.init.constant_(m.bias, 0.0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 0)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.GroupNorm):\n                nn.init.constant_(m.weight, 0)\n                nn.init.constant_(m.bias, 0)", "num_branches = self.num_branches\n        num_in_chs = self.num_in_chs\n        fuse_layers = []\n        for i in range(num_branches if self.multi_scale_output else 1):\n            fuse_layer = []\n            for j in range(num_branches):\n                if j > i:\n                    fuse_layer.append(nn.Sequential(\n                        nn.Conv2d(num_in_chs[j], num_in_chs[i], 1, 1, 0, bias=False),\n                        nn.BatchNorm2d(num_in_chs[i], momentum=_BN_MOMENTUM),\n                        nn.Upsample(scale_factor=2 ** (j - i), mode='nearest')))\n                elif j == i:\n                    fuse_layer.append(nn.Identity())\n                else:\n                    conv3x3s = []\n                    for k in range(i - j):\n                        if k == i - j - 1:\n                            num_out_chs_conv3x3 = num_in_chs[i]\n                            conv3x3s.append(nn.Sequential(\n                                nn.Conv2d(num_in_chs[j], num_out_chs_conv3x3, 3, 2, 1, bias=False),\n                                nn.BatchNorm2d(num_out_chs_conv3x3, momentum=_BN_MOMENTUM)\n                            ))\n                        else:\n                            num_out_chs_conv3x3 = num_in_chs[j]\n                            conv3x3s.append(nn.Sequential(\n                                nn.Conv2d(num_in_chs[j], num_out_chs_conv3x3, 3, 2, 1, bias=False),\n                                nn.BatchNorm2d(num_out_chs_conv3x3, momentum=_BN_MOMENTUM),\n                                nn.ReLU(False)\n                            ))\n                    fuse_layer.append(nn.Sequential(*conv3x3s))\n            fuse_layers.append(nn.ModuleList(fuse_layer))", "self.point_linear = nn.Sequential(\n            OrderedDict(\n                [\n                    (\"conv\", nn.Conv2d(feature_dim, out_channels, 1, 1, 0, bias=False)),\n                    (\"bn\", nn.BatchNorm2d(out_channels)),\n                ]\n            )\n        )", "default_cfgs = generate_default_cfgs({\n    # weights in nn.Linear mode\n    'levit_128s.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n    ),\n    'levit_128.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n    ),\n    'levit_192.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n    ),\n    'levit_256.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n    ),\n    'levit_384.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n    ),", "self.shortcut_proj_attn = nn.Linear(dim, dim_out) if proj_needed and expand_attn else None\n        if stride_q and prod(stride_q) > 1:\n            kernel_skip = [s + 1 if s > 1 else s for s in stride_q]\n            stride_skip = stride_q\n            padding_skip = [int(skip // 2) for skip in kernel_skip]\n            self.shortcut_pool_attn = nn.MaxPool2d(kernel_skip, stride_skip, padding_skip)\n        else:\n            self.shortcut_pool_attn = None", "self.fc1 = nn.Linear(in_features, hidden_features, bias=bias[0])\n        self.act = act_layer()\n        self.drop1 = nn.Dropout(drop_probs[0])\n        if gate_layer is not None:\n            assert hidden_features % 2 == 0\n            self.gate = gate_layer(hidden_features)\n            hidden_features = hidden_features // 2  # FIXME base reduction on gate property?\n        else:\n            self.gate = nn.Identity()\n        self.norm = norm_layer(hidden_features) if norm_layer is not None else nn.Identity()\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias[1])\n        self.drop2 = nn.Dropout(drop_probs[1])", "def conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)", "gc = int(in_chs * branch_ratio)  # channel numbers of a convolution branch\n        square_padding = get_padding(square_kernel_size, dilation=dilation)\n        band_padding = get_padding(band_kernel_size, dilation=dilation)\n        self.dwconv_hw = nn.Conv2d(\n            gc, gc, square_kernel_size,\n            padding=square_padding, dilation=dilation, groups=gc)\n        self.dwconv_w = nn.Conv2d(\n            gc, gc, (1, band_kernel_size),\n            padding=(0, band_padding), dilation=(1, dilation), groups=gc)\n        self.dwconv_h = nn.Conv2d(\n            gc, gc, (band_kernel_size, 1),\n            padding=(band_padding, 0), dilation=(dilation, 1), groups=gc)\n        self.split_indexes = (in_chs - 3 * gc, gc, gc, gc)", "def init_weights(m):\n    \"\"\"Performs ResNet-style weight initialization.\"\"\"\n    if isinstance(m, nn.Conv2d):\n        # Note that there is no bias due to BN\n        fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(mean=0.0, std=np.sqrt(2.0 / fan_out))\n    elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1.0)\n        m.bias.data.zero_()\n    elif isinstance(m, nn.Linear):\n        m.weight.data.normal_(mean=0.0, std=0.01)\n        m.bias.data.zero_()", "def __init__(self, nc=80, anchors=(), ch=(), inplace=True):\n        \"\"\"Initializes YOLOv5 detection layer with specified classes, anchors, channels, and inplace operations.\"\"\"\n        super().__init__()\n        self.nc = nc  # number of classes\n        self.no = nc + 5  # number of outputs per anchor\n        self.nl = len(anchors)  # number of detection layers\n        self.na = len(anchors[0]) // 2  # number of anchors\n        self.grid = [torch.empty(0) for _ in range(self.nl)]  # init grid\n        self.anchor_grid = [torch.empty(0) for _ in range(self.nl)]  # init anchor grid\n        self.register_buffer(\"anchors\", torch.tensor(anchors).float().view(self.nl, -1, 2))  # shape(nl,na,2)\n        self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, 1) for x in ch)  # output conv\n        self.inplace = inplace  # use inplace ops (e.g. slice assignment)", "def __init__(self, in_channels: int, out_channels: int):\n        super().__init__()\n        self.p6 = nn.Conv2d(in_channels, out_channels, 3, 2, 1)\n        self.p7 = nn.Conv2d(out_channels, out_channels, 3, 2, 1)\n        for module in [self.p6, self.p7]:\n            nn.init.kaiming_uniform_(module.weight, a=1)\n            nn.init.constant_(module.bias, 0)\n        self.use_P5 = in_channels == out_channels", "if inference_mode:\n            self.reparam_conv = nn.Conv2d(\n                in_channels=in_chs,\n                out_channels=out_chs,\n                kernel_size=kernel_size,\n                stride=stride,\n                dilation=dilation,\n                groups=groups,\n                bias=True,\n            )\n        else:\n            self.reparam_conv = None\n            use_ident = in_chs == out_chs and stride == 1 and dilation[0] == dilation[1]\n            self.identity = layers.norm_act(out_chs, apply_act=False) if use_ident else None\n            self.conv_kxk = layers.conv_norm_act(\n                in_chs, out_chs, kernel_size,\n                stride=stride, dilation=dilation[0], groups=groups, drop_layer=drop_block, apply_act=False,\n            )\n            self.conv_1x1 = layers.conv_norm_act(in_chs, out_chs, 1, stride=stride, groups=groups, apply_act=False)\n            self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. and use_ident else nn.Identity()", "class DenseTransition(nn.Sequential):\n    def __init__(\n            self,\n            num_input_features,\n            num_output_features,\n            norm_layer=BatchNormAct2d,\n            aa_layer=None,\n    ):\n        super(DenseTransition, self).__init__()\n        self.add_module('norm', norm_layer(num_input_features))\n        self.add_module('conv', nn.Conv2d(\n            num_input_features, num_output_features, kernel_size=1, stride=1, bias=False))\n        if aa_layer is not None:\n            self.add_module('pool', aa_layer(num_output_features, stride=2))\n        else:\n            self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))", "def __init__(\n            self,\n            cfg: List[Any],\n            num_classes: int = 1000,\n            in_chans: int = 3,\n            output_stride: int = 32,\n            mlp_ratio: float = 1.0,\n            act_layer: Type[nn.Module] = nn.ReLU,\n            conv_layer: Type[nn.Module] = nn.Conv2d,\n            norm_layer: Optional[Type[nn.Module]] = None,\n            global_pool: str = 'avg',\n            drop_rate: float = 0.,\n    ) -> None:\n        super(VGG, self).__init__()\n        assert output_stride == 32\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        self.grad_checkpointing = False\n        self.use_norm = norm_layer is not None\n        self.feature_info = []", "Unlike the official impl, this one allows choice of 1 or 2, 1x1 conv can be faster with appropriate\n    choice of LayerNorm impl, however as model size increases the tradeoffs appear to change and nn.Linear\n    is a better choice. This was observed with PyTorch 1.10 on 3090 GPU, it could change over time & w/ different HW.\n    \"\"\"", "self.cls_subnet = nn.Sequential(*cls_subnet)\n        self.bbox_subnet = nn.Sequential(*bbox_subnet)\n        self.cls_score = nn.Conv2d(\n            conv_dims[-1], num_anchors * num_classes, kernel_size=3, stride=1, padding=1\n        )\n        self.bbox_pred = nn.Conv2d(\n            conv_dims[-1], num_anchors * 4, kernel_size=3, stride=1, padding=1\n        )", "def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=1,\n            padding=0,\n            norm_layer=None,\n    ):\n        super().__init__()\n        self.norm = norm_layer(in_channels) if norm_layer else nn.Identity()\n        self.conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding\n        )", "def test_param_groups_layer_decay_with_end_decay():\n    model = torch.nn.Sequential(\n        torch.nn.Linear(10, 5),\n        torch.nn.ReLU(),\n        torch.nn.Linear(5, 2)\n    )", "padding = kernel_size // 2 if padding is None else padding\n        self.conv = nn.Conv2d(\n            in_channels, mid_chs, kernel_size, stride, padding, dilation,\n            groups=groups * radix, bias=bias, **kwargs)\n        self.bn0 = norm_layer(mid_chs) if norm_layer else nn.Identity()\n        self.drop = drop_layer() if drop_layer is not None else nn.Identity()\n        self.act0 = act_layer(inplace=True)\n        self.fc1 = nn.Conv2d(out_channels, attn_chs, 1, groups=groups)\n        self.bn1 = norm_layer(attn_chs) if norm_layer else nn.Identity()\n        self.act1 = act_layer(inplace=True)\n        self.fc2 = nn.Conv2d(attn_chs, mid_chs, 1, groups=groups)\n        self.rsoftmax = RadixSoftmax(radix, groups)", "def __init__(self, dim: int, norm_layer: Callable[..., nn.Module] = nn.LayerNorm):\n        super().__init__()\n        _log_api_usage_once(self)\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n        self.norm = norm_layer(2 * dim)  # difference", "self.conv1 = nn.Conv2d(in_chans, 32, 3, 2, 0, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.act1 = nn.ReLU(inplace=True)", "self.cls_subnet = nn.Sequential(*cls_subnet)\n        self.cls_score = nn.Conv2d(\n            cur_channels, num_anchors * num_classes, kernel_size=3, stride=1, padding=1\n        )\n        modules_list = [self.cls_subnet, self.cls_score]", "self.conv_norm_relus = []\n        for k, conv_dim in enumerate(conv_dims):\n            conv = Conv2d(\n                self._output_size[0],\n                conv_dim,\n                kernel_size=3,\n                padding=1,\n                bias=not conv_norm,\n                norm=get_norm(conv_norm, conv_dim),\n                activation=nn.ReLU(),\n            )\n            self.add_module(\"conv{}\".format(k + 1), conv)\n            self.conv_norm_relus.append(conv)\n            self._output_size = (conv_dim, self._output_size[1], self._output_size[2])", "@register_model(name=\"quantized_mobilenet_v2\")\n@handle_legacy_interface(\n    weights=(\n        \"pretrained\",\n        lambda kwargs: (\n            MobileNet_V2_QuantizedWeights.IMAGENET1K_QNNPACK_V1\n            if kwargs.get(\"quantize\", False)\n            else MobileNet_V2_Weights.IMAGENET1K_V1\n        ),\n    )\n)\ndef mobilenet_v2(\n    *,\n    weights: Optional[Union[MobileNet_V2_QuantizedWeights, MobileNet_V2_Weights]] = None,\n    progress: bool = True,\n    quantize: bool = False,\n    **kwargs: Any,\n) -> QuantizableMobileNetV2:\n    \"\"\"\n    Constructs a MobileNetV2 architecture from\n    `MobileNetV2: Inverted Residuals and Linear Bottlenecks\n    <https://arxiv.org/abs/1801.04381>`_.", "class ASPPPooling(nn.Sequential):\n    def __init__(self, in_channels: int, out_channels: int) -> None:\n        super().__init__(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )", "if isinstance(self.head, ClNormMlpClassifierHead) and isinstance(self.head.fc, nn.Linear):\n            self.head.fc.weight.data.mul_(head_init_scale)\n            self.head.fc.bias.data.mul_(head_init_scale)", "# Classifier head\n        self.num_features = self.head_hidden_size = embed_dims[-1]\n        self.norm = norm_layer_cl(self.num_features)\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n        # assuming model is always distilled (valid for current checkpoints, will split def if that changes)\n        self.head_dist = nn.Linear(embed_dims[-1], num_classes) if num_classes > 0 else nn.Identity()\n        self.distilled_training = False  # must set this True to train w/ distillation token", "def _normal_init(conv: nn.Module):\n    for layer in conv.modules():\n        if isinstance(layer, nn.Conv2d):\n            torch.nn.init.normal_(layer.weight, mean=0.0, std=0.03)\n            if layer.bias is not None:\n                torch.nn.init.constant_(layer.bias, 0.0)", "self.fcs = []\n        for k, fc_dim in enumerate(fc_dims):\n            if k == 0:\n                self.add_module(\"flatten\", nn.Flatten())\n            fc = nn.Linear(int(np.prod(self._output_size)), fc_dim)\n            self.add_module(\"fc{}\".format(k + 1), fc)\n            self.add_module(\"fc_relu{}\".format(k + 1), nn.ReLU())\n            self.fcs.append(fc)\n            self._output_size = fc_dim", "self.m = nn.ModuleList(\n            [nn.Conv2d(c1, int(c_), k, s, k // 2, groups=math.gcd(c1, int(c_)), bias=False) for k, c_ in zip(k, c_)]\n        )\n        self.bn = nn.BatchNorm2d(c2)\n        self.act = nn.SiLU()", "# classification head\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()", "# pre-normalization similar to transformer layers\n        self.mlp_layer = nn.Sequential(\n            nn.LayerNorm(in_channels),\n            nn.Linear(in_channels, in_channels * mlp_ratio),\n            activation_layer(),\n            nn.Linear(in_channels * mlp_ratio, in_channels),\n            nn.Dropout(mlp_dropout),\n        )", "for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()", "# Head + Pooling\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        num_pooled_chs = self.num_features * self.global_pool.feat_mult()\n        if head_norm:\n            # mobilenet-v4 post-pooling PW conv is followed by a norm+act layer\n            self.conv_head = create_conv2d(num_pooled_chs, self.head_hidden_size, 1, padding=pad_type)  # never bias\n            self.norm_head = norm_act_layer(self.head_hidden_size)\n            self.act2 = nn.Identity()\n        else:\n            # mobilenet-v3 and others only have an activation after final PW conv\n            self.conv_head = create_conv2d(num_pooled_chs, self.head_hidden_size, 1, padding=pad_type, bias=head_bias)\n            self.norm_head = nn.Identity()\n            self.act2 = act_layer(inplace=True)\n        self.flatten = nn.Flatten(1) if global_pool else nn.Identity()  # don't flatten if pooling disabled\n        self.classifier = Linear(self.head_hidden_size, num_classes) if num_classes > 0 else nn.Identity()", "self.conv2 = Conv2d(\n            out_channels,\n            out_channels,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias=False,\n            norm=get_norm(norm, out_channels),\n        )", "def __init__(\n            self,\n            in_channels,\n            out_channels,\n            norm_layer=None,\n    ):\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=7,\n            stride=4,\n            padding=2\n        )\n        self.norm = norm_layer(out_channels) if norm_layer else nn.Identity()", "class FCNHead(nn.Sequential):\n    def __init__(self, in_channels: int, channels: int) -> None:\n        inter_channels = in_channels // 4\n        layers = [\n            nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n            nn.BatchNorm2d(inter_channels),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Conv2d(inter_channels, channels, 1),\n        ]", "class EseModule(nn.Module):\n    def __init__(self, chs):\n        super().__init__()\n        self.conv = nn.Conv2d(\n            chs,\n            chs,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n        )\n        self.sigmoid = nn.Sigmoid()", "for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.trunc_normal_(m.weight, std=0.02)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)", "self.conv2 = nn.Conv2d(32, 64, 3, bias=False)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.act2 = nn.ReLU(inplace=True)", "self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias)\n        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()", "self.fc = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()", "def create_conv2d_pad(in_chs, out_chs, kernel_size, **kwargs):\n    padding = kwargs.pop('padding', '')\n    kwargs.setdefault('bias', False)\n    padding, is_dynamic = get_padding_value(padding, kernel_size, **kwargs)\n    if is_dynamic:\n        if _USE_EXPORT_CONV and is_exportable():\n            # older PyTorch ver needed this to export same padding reasonably\n            assert not is_scriptable()  # Conv2DSameExport does not work with jit\n            return Conv2dSameExport(in_chs, out_chs, kernel_size, **kwargs)\n        else:\n            return Conv2dSame(in_chs, out_chs, kernel_size, **kwargs)\n    else:\n        return nn.Conv2d(in_chs, out_chs, kernel_size, padding=padding, **kwargs)", "def __init__(self, hidden_dim=32, dim=768, temperature=10000):\n        super().__init__()\n        self.token_projection = nn.Conv2d(hidden_dim * 2, dim, kernel_size=1)\n        self.scale = 2 * math.pi\n        self.temperature = temperature\n        self.hidden_dim = hidden_dim\n        self.dim = dim\n        self.eps = 1e-6", "def fuse_model(self, is_qat: Optional[bool] = None) -> None:\n        for idx in range(len(self.conv)):\n            if type(self.conv[idx]) is nn.Conv2d:\n                _fuse_modules(self.conv, [str(idx), str(idx + 1)], is_qat, inplace=True)", "for name, m in model.named_modules():\n        if isinstance(m, nn.Conv2d):\n            prune.l1_unstructured(m, name=\"weight\", amount=amount)  # prune\n            prune.remove(m, \"weight\")  # make permanent\n    LOGGER.info(f\"Model pruned to {sparsity(model):.3g} global sparsity\")", "self.fix_init_weight()\n        if isinstance(self.head, nn.Linear):\n            trunc_normal_(self.head.weight, std=.02)\n            self.head.weight.data.mul_(head_init_scale)\n            self.head.bias.data.mul_(head_init_scale)", "self.proj = nn.Conv2d(\n            dim,\n            dim,\n            kernel_size=k,\n            stride=1,\n            padding=k // 2,\n            groups=dim,\n        )\n        self.act = nn.GELU() if act else nn.Identity()", "class MaskRCNNPredictor(nn.Sequential):\n    def __init__(self, in_channels, dim_reduced, num_classes):\n        super().__init__(\n            OrderedDict(\n                [\n                    (\"conv5_mask\", nn.ConvTranspose2d(in_channels, dim_reduced, 2, 2, 0)),\n                    (\"relu\", nn.ReLU(inplace=True)),\n                    (\"mask_fcn_logits\", nn.Conv2d(dim_reduced, num_classes, 1, 1, 0)),\n                ]\n            )\n        )", "heads_layers: OrderedDict[str, nn.Module] = OrderedDict()\n        if representation_size is None:\n            heads_layers[\"head\"] = nn.Linear(hidden_dim, num_classes)\n        else:\n            heads_layers[\"pre_logits\"] = nn.Linear(hidden_dim, representation_size)\n            heads_layers[\"act\"] = nn.Tanh()\n            heads_layers[\"head\"] = nn.Linear(representation_size, num_classes)", "# model initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n            if isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)", "# building first layer\n        stem_chs = make_divisible(16 * width, 4)\n        self.conv_stem = nn.Conv2d(in_chans, stem_chs, 3, 2, 1, bias=False)\n        self.feature_info.append(dict(num_chs=stem_chs, reduction=2, module=f'conv_stem'))\n        self.bn1 = nn.BatchNorm2d(stem_chs)\n        self.act1 = nn.ReLU(inplace=True)\n        prev_chs = stem_chs", "def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            self.head.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n            self.head.flatten = nn.Flatten(1) if global_pool else nn.Identity()\n        if num_classes > 0:\n            if self.use_mlp_head:\n                final = MlpHead(self.num_features, num_classes, drop_rate=self.drop_rate)\n            else:\n                final = nn.Linear(self.num_features, num_classes)\n        else:\n            final = nn.Identity()\n        self.head.fc = final", "self.q = nn.Linear(embed_dim, embed_dim, bias=qkv_bias)\n        self.kv = nn.Linear(embed_dim, embed_dim * 2, bias=qkv_bias)\n        if qk_norm:\n            qk_norm_layer = norm_layer or nn.LayerNorm\n            self.q_norm = qk_norm_layer(self.head_dim)\n            self.k_norm = qk_norm_layer(self.head_dim)\n        else:\n            self.q_norm = nn.Identity()\n            self.k_norm = nn.Identity()\n        self.proj = nn.Linear(embed_dim, embed_dim)\n        self.proj_drop = nn.Dropout(drop)", "use_fc_norm = self.global_pool == 'avg'\n        self.norm = nn.Identity() if use_fc_norm else norm_layer(embed_dim)\n        self.fc_norm = norm_layer(embed_dim) if use_fc_norm else nn.Identity()\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()", "class ASPPPooling(nn.Sequential):\n    def __init__(self, in_channels, out_channels):\n        super(ASPPPooling, self).__init__(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            nn.GroupNorm(32, out_channels),\n            nn.ReLU(),\n        )", "self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.dropout = nn.Dropout(p=dropout)\n        self.fc = nn.Linear(1024, num_classes)", "self.conv = nn.Conv2d(layers[3], layers[4], kernel_size=1)", "def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.trunc_normal_(m.weight, std=0.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)", "if self.share_conv:\n            self.fpn_conv = nn.Conv2d(\n                in_channels=out_channels, out_channels=out_channels, kernel_size=3, padding=1\n            )\n        else:\n            self.fpn_conv = nn.ModuleList()\n            for _ in range(self.n_out_features):\n                self.fpn_conv.append(\n                    nn.Conv2d(\n                        in_channels=out_channels,\n                        out_channels=out_channels,\n                        kernel_size=3,\n                        padding=1,\n                    )\n                )", "def __init__(self, inplanes, planes, groups, reduction, stride=1, downsample=None):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes * 2)\n        self.conv2 = nn.Conv2d(\n            planes * 2, planes * 4, kernel_size=3, stride=stride,\n            padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes * 4)\n        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride", "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=dilation,\n        groups=groups,\n        bias=False,\n        dilation=dilation,\n    )", "self.fc1 = nn.Linear(in_features * self.global_pool.feat_mult(), hidden_features, bias=bias)\n        self.act = act_layer()\n        self.norm = norm_layer(hidden_features)\n        self.fc2 = nn.Linear(hidden_features, num_classes, bias=bias)\n        self.drop = nn.Dropout(drop)", "self.features = nn.Sequential(*layers)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=dropout, inplace=True),\n            nn.Linear(lastconv_output_channels, num_classes),\n        )", "# Introduce reparam conv\n        self.reparam_conv = nn.Conv2d(\n            self.dim,\n            self.dim_out,\n            kernel_size=self.spatial_shape,\n            stride=1,\n            padding=int(self.spatial_shape[0] // 2),\n            groups=self.groups,\n            bias=True,\n        )\n        self.reparam_conv.weight.data = w_final\n        self.reparam_conv.bias.data = b_final", "def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            assert global_pool in ('', 'avg', 'token')\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()", "self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)", "self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim)", "# Copyright (c) Facebook, Inc. and its affiliates.\nfrom .batch_norm import FrozenBatchNorm2d, get_norm, NaiveSyncBatchNorm, CycleBatchNormList\nfrom .deform_conv import DeformConv, ModulatedDeformConv\nfrom .mask_ops import paste_masks_in_image\nfrom .nms import batched_nms, batched_nms_rotated, nms, nms_rotated\nfrom .roi_align import ROIAlign, roi_align\nfrom .roi_align_rotated import ROIAlignRotated, roi_align_rotated\nfrom .shape_spec import ShapeSpec\nfrom .wrappers import (\n    BatchNorm2d,\n    Conv2d,\n    ConvTranspose2d,\n    cat,\n    interpolate,\n    Linear,\n    nonzero_tuple,\n    cross_entropy,\n    empty_input_loss_func_wrapper,\n    shapes_to_tensor,\n    move_device_like,\n)\nfrom .blocks import CNNBlockBase, DepthwiseSeparableConv2d\nfrom .aspp import ASPP\nfrom .losses import ciou_loss, diou_loss", "# see https://github.com/google-research/maxvit/blob/da76cf0d8a6ec668cc31b399c4126186da7da944/maxvit/models/maxvit.py#L1137-L1158\n        # for why there is Linear -> Tanh -> Linear\n        self.classifier = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            nn.LayerNorm(block_channels[-1]),\n            nn.Linear(block_channels[-1], block_channels[-1]),\n            nn.Tanh(),\n            nn.Linear(block_channels[-1], num_classes, bias=False),\n        )", "# shortcut\n        if in_chs == out_chs and self.stride == 1:\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(\n                    in_chs, in_chs, dw_kernel_size, stride=stride,\n                    padding=(dw_kernel_size-1)//2, groups=in_chs, bias=False),\n                nn.BatchNorm2d(in_chs),\n                nn.Conv2d(in_chs, out_chs, 1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_chs),\n            )", "def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)", "# feed forward network\n        self.ffn = nn.Sequential(\n            nn.Linear(dim_model * 2, dim_model * 2, bias=False),\n            nn.ReLU(),\n            nn.Linear(dim_model * 2, dim_model, bias=False),\n        )", "self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim, bias=proj_bias)\n        self.proj_drop = nn.Dropout(proj_drop)", "for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.normal_(m.bias, std=1e-6)", "self.norm1_proj = norm_layer(num_pixel * inner_dim)\n        self.proj = nn.Linear(num_pixel * inner_dim, embed_dim)\n        self.norm2_proj = norm_layer(embed_dim)", "self.qkv = nn.Linear(dim, self.key_attn_dim * 2 + self.val_attn_dim)\n        self.proj = nn.Linear(self.val_attn_dim, dim)", "cfg.update(**kwargs)\n        stem_width = cfg['stem_width']\n        self.conv1 = nn.Conv2d(in_chans, stem_width, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(stem_width, momentum=_BN_MOMENTUM)\n        self.act1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(stem_width, 64, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(64, momentum=_BN_MOMENTUM)\n        self.act2 = nn.ReLU(inplace=True)", "self.proj = nn.Linear(dim, dim)", "if dim != dim_out:\n            self.proj = nn.Linear(dim, dim_out)", "# Custom change: Replaces a couple (reduction conv + pooling) by one conv\n        self.reduction_pooling_conv = nn.ModuleList()\n        for i in range(self.n_out_features):\n            self.reduction_pooling_conv.append(\n                nn.Sequential(\n                    nn.Conv2d(sum(in_channels), out_channels, kernel_size=2**i, stride=2**i),\n                    nn.BatchNorm2d(out_channels, momentum=0.1),\n                    nn.ReLU(inplace=True),\n                )\n            )", "self.kernel_sizes = []\n        for k in range(self.focal_level):\n            kernel_size = self.focal_factor * k + self.focal_window\n            self.focal_layers.append(nn.Sequential(\n                nn.Conv2d(dim, dim, kernel_size=kernel_size, groups=dim, padding=kernel_size // 2, bias=False),\n                nn.GELU(),\n            ))\n            self.kernel_sizes.append(kernel_size)\n        self.norm = norm_layer(dim) if self.use_post_norm else nn.Identity()", "if not shared_base:\n            self.residual_block: nn.Module = nn.Identity()\n            self.conv = nn.Conv2d(base_dim, output_dim, kernel_size=1)\n        else:\n            # If we share base encoder weight for Feature and Context Encoder\n            # we need to add residual block with InstanceNorm2d and change the kernel size for conv layer\n            # see: https://github.com/princeton-vl/RAFT-Stereo/blob/main/core/raft_stereo.py#L35-L37\n            self.residual_block = block(base_dim, base_dim, norm_layer=nn.InstanceNorm2d, stride=1)\n            self.conv = nn.Conv2d(base_dim, output_dim, kernel_size=3, padding=1)", "class DlaTree(nn.Module):\n    def __init__(\n            self,\n            levels,\n            block,\n            in_channels,\n            out_channels,\n            stride=1,\n            dilation=1,\n            cardinality=1,\n            base_width=64,\n            level_root=False,\n            root_dim=0,\n            root_kernel_size=1,\n            root_shortcut=False,\n    ):\n        super(DlaTree, self).__init__()\n        if root_dim == 0:\n            root_dim = 2 * out_channels\n        if level_root:\n            root_dim += in_channels\n        self.downsample = nn.MaxPool2d(stride, stride=stride) if stride > 1 else nn.Identity()\n        self.project = nn.Identity()\n        cargs = dict(dilation=dilation, cardinality=cardinality, base_width=base_width)\n        if levels == 1:\n            self.tree1 = block(in_channels, out_channels, stride, **cargs)\n            self.tree2 = block(out_channels, out_channels, 1, **cargs)\n            if in_channels != out_channels:\n                # NOTE the official impl/weights have  project layers in levels > 1 case that are never\n                # used, I've moved the project layer here to avoid wasted params but old checkpoints will\n                # need strict=False while loading.\n                self.project = nn.Sequential(\n                    nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n                    nn.BatchNorm2d(out_channels))\n            self.root = DlaRoot(root_dim, out_channels, root_kernel_size, root_shortcut)\n        else:\n            cargs.update(dict(root_kernel_size=root_kernel_size, root_shortcut=root_shortcut))\n            self.tree1 = DlaTree(\n                levels - 1,\n                block,\n                in_channels,\n                out_channels,\n                stride,\n                root_dim=0,\n                **cargs,\n            )\n            self.tree2 = DlaTree(\n                levels - 1,\n                block,\n                out_channels,\n                out_channels,\n                root_dim=root_dim + out_channels,\n                **cargs,\n            )\n            self.root = None\n        self.level_root = level_root\n        self.root_dim = root_dim\n        self.levels = levels", "self.conv1 = torch.nn.Conv2d(\n            in_features, in_features, kernel_size=kernel_size, padding=padding, groups=in_features)\n        self.act = act_layer()\n        self.bn = nn.BatchNorm2d(in_features)\n        self.conv2 = torch.nn.Conv2d(\n            in_features, out_features, kernel_size=kernel_size, padding=padding, groups=out_features)", "self.q = nn.Linear(dim, dim, bias=qkv_bias)\n        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)", "def prune(model, amount=0.3):\n    \"\"\"Prunes Conv2d layers in a model to a specified sparsity using L1 unstructured pruning.\"\"\"\n    import torch.nn.utils.prune as prune", "if isinstance(module, nn.Linear):\n        extra_bias = weight @ extra_bias\n        weight.mul_(extra_weight.view(1, weight.size(1)).expand_as(weight))\n    elif isinstance(module, nn.Conv2d):\n        assert weight.shape[2] == 1 and weight.shape[3] == 1\n        weight = weight.reshape(weight.shape[0], weight.shape[1])\n        extra_bias = weight @ extra_bias\n        weight.mul_(extra_weight.view(1, weight.size(1)).expand_as(weight))\n        weight = weight.reshape(weight.shape[0], weight.shape[1], 1, 1)\n    bias.add_(extra_bias)", "from detectron2.config import configurable\nfrom detectron2.layers import Conv2d, ConvTranspose2d, cat, interpolate\nfrom detectron2.structures import Instances, heatmaps_to_keypoints\nfrom detectron2.utils.events import get_event_storage\nfrom detectron2.utils.registry import Registry", "\"\"\" Conv2d w/ Same Padding", "from detectron2.config import configurable\nfrom detectron2.layers import ASPP, Conv2d, DepthwiseSeparableConv2d, ShapeSpec, get_norm\nfrom detectron2.modeling import SEM_SEG_HEADS_REGISTRY", "# default init_weights for conv(msra) and norm in ConvModule\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, a=1)\n                nn.init.constant_(m.bias, 0)", "def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)", "class BasicConv2d(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, **kwargs: Any) -> None:\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)", "@staticmethod\n    def _fuse_bn(\n        conv: nn.Conv2d, bn: nn.BatchNorm2d\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Method to fuse batchnorm layer with conv layer.", "class ScaledStdConv2dSame(nn.Conv2d):\n    \"\"\"Conv2d layer with Scaled Weight Standardization and Tensorflow-like SAME padding support", "self.to_qkv = nn.Linear(feat_dim, self.n_heads * self.head_dim * 3)\n        self.scale_factor = feat_dim**-0.5", "for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n                stddev = m.stddev if hasattr(m, 'stddev') else 0.1\n                trunc_normal_(m.weight, std=stddev)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)", "def lf(x):\n            \"\"\"Linear learning rate scheduler function with decay calculated by epoch proportion.\"\"\"\n            return (1 - x / epochs) * (1.0 - hyp[\"lrf\"]) + hyp[\"lrf\"]  # linear", "final_layer = nn.Sequential(\n            nn.Conv2d(\n                in_channels=self.head_channels[3] * head_block_type.expansion, out_channels=self.num_features,\n                kernel_size=1, stride=1, padding=0, bias=conv_bias),\n            nn.BatchNorm2d(self.num_features, momentum=_BN_MOMENTUM),\n            nn.ReLU(inplace=True)\n        )", "self.features = nn.Sequential(*layers)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.classifier = nn.Sequential(\n            nn.Linear(lastconv_output_channels, last_channel),\n            nn.Hardswish(inplace=True),\n            nn.Dropout(p=dropout, inplace=True),\n            nn.Linear(last_channel, num_classes),\n        )", "for n, m in self.named_modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')", "def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)", "if not self.decoder_only:\n            self.predictor = Conv2d(\n                decoder_channels[0], num_classes, kernel_size=1, stride=1, padding=0\n            )\n            nn.init.normal_(self.predictor.weight, 0, 0.001)\n            nn.init.constant_(self.predictor.bias, 0)", "# First convolution\n        self.features = nn.Sequential(\n            OrderedDict(\n                [\n                    (\"conv0\", nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n                    (\"norm0\", nn.BatchNorm2d(num_init_features)),\n                    (\"relu0\", nn.ReLU(inplace=True)),\n                    (\"pool0\", nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n                ]\n            )\n        )", "self.fc1 = nn.Conv2d(in_features, hidden_features, kernel_size=1, bias=bias[0])\n        self.norm = norm_layer(hidden_features) if norm_layer else nn.Identity()\n        self.act = act_layer()\n        self.drop = nn.Dropout(drop)\n        self.fc2 = nn.Conv2d(hidden_features, out_features, kernel_size=1, bias=bias[1])", "def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.trunc_normal_(m.weight, std=0.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)", "self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n            self.register_buffer('k_bias', torch.zeros(all_head_dim), persistent=False)\n            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n        else:\n            self.q_bias = None\n            self.k_bias = None\n            self.v_bias = None", "# Classifier Head\n        if global_pool == 'map':\n            self.attn_pool = AttentionPoolLatent(\n                self.embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                norm_layer=norm_layer,\n                act_layer=act_layer,\n            )\n        else:\n            self.attn_pool = None\n        self.fc_norm = norm_layer(embed_dim) if final_norm and use_fc_norm else nn.Identity()\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()", "self.predictor = Conv2d(cur_channels, num_classes, kernel_size=1, stride=1, padding=0)", "@register_model()\n@handle_legacy_interface(\n    weights=(\"pretrained\", SSDLite320_MobileNet_V3_Large_Weights.COCO_V1),\n    weights_backbone=(\"pretrained_backbone\", MobileNet_V3_Large_Weights.IMAGENET1K_V1),\n)\ndef ssdlite320_mobilenet_v3_large(\n    *,\n    weights: Optional[SSDLite320_MobileNet_V3_Large_Weights] = None,\n    progress: bool = True,\n    num_classes: Optional[int] = None,\n    weights_backbone: Optional[MobileNet_V3_Large_Weights] = MobileNet_V3_Large_Weights.IMAGENET1K_V1,\n    trainable_backbone_layers: Optional[int] = None,\n    norm_layer: Optional[Callable[..., nn.Module]] = None,\n    **kwargs: Any,\n) -> SSD:\n    \"\"\"SSDlite model architecture with input size 320x320 and a MobileNetV3 Large backbone, as\n    described at `Searching for MobileNetV3 <https://arxiv.org/abs/1905.02244>`__ and\n    `MobileNetV2: Inverted Residuals and Linear Bottlenecks <https://arxiv.org/abs/1801.04381>`__.", "if init_weights:\n            for m in self.modules():\n                if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n                    torch.nn.init.trunc_normal_(m.weight, mean=0.0, std=0.01, a=-2, b=2)\n                elif isinstance(m, nn.BatchNorm2d):\n                    nn.init.constant_(m.weight, 1)\n                    nn.init.constant_(m.bias, 0)", "for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.trunc_normal_(m.weight, std=0.02)\n                if isinstance(m, nn.Linear) and m.bias is not None:\n                    nn.init.constant_(m.bias, 0.0)\n            elif isinstance(m, nn.LayerNorm):\n                if m.weight is not None:\n                    nn.init.constant_(m.weight, 1.0)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0.0)\n            elif isinstance(m, PositionalEncoding):\n                for weights in m.parameters():\n                    nn.init.trunc_normal_(weights, std=0.02)", "def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            assert global_pool in ('token', 'avg')\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()", "def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, multi_conv=False):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n        if multi_conv:\n            if patch_size[0] == 12:\n                self.proj = nn.Sequential(\n                    nn.Conv2d(in_chans, embed_dim // 4, kernel_size=7, stride=4, padding=3),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(embed_dim // 4, embed_dim // 2, kernel_size=3, stride=3, padding=0),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=3, stride=1, padding=1),\n                )\n            elif patch_size[0] == 16:\n                self.proj = nn.Sequential(\n                    nn.Conv2d(in_chans, embed_dim // 4, kernel_size=7, stride=4, padding=3),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(embed_dim // 4, embed_dim // 2, kernel_size=3, stride=2, padding=1),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=3, stride=2, padding=1),\n                )\n        else:\n            self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)", "if downsample_block or stride == 1:\n            self.downsample = nn.Identity()\n        else:\n            self.downsample = nn.Sequential(\n                norm_layer(in_chs),\n                nn.Conv2d(in_chs, out_chs, kernel_size=2, stride=2, bias=conv_bias)\n            )\n            in_chs = out_chs", "class ConvNorm(torch.nn.Sequential):\n    def __init__(self, in_chs, out_chs, ks=1, stride=1, pad=0, dilation=1, groups=1, bn_weight_init=1):\n        super().__init__()\n        self.conv = nn.Conv2d(in_chs, out_chs, ks, stride, pad, dilation, groups, bias=False)\n        self.bn = nn.BatchNorm2d(out_chs)\n        torch.nn.init.constant_(self.bn.weight, bn_weight_init)\n        torch.nn.init.constant_(self.bn.bias, 0)", "def init_weight_jax(\n        module: nn.Module,\n        name: str,\n        head_bias: float = 0.,\n        classifier_name: str = 'head',\n):\n    if isinstance(module, nn.Linear):\n        if name.startswith(classifier_name):\n            nn.init.zeros_(module.weight)\n            nn.init.constant_(module.bias, head_bias)\n        else:\n            nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                nn.init.normal_(module.bias, std=1e-6) if 'mlp' in name else nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Conv2d):\n        lecun_normal_(module.weight)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif hasattr(module, 'init_weights'):\n        module.init_weights()", "self.conv2 = nn.Conv2d(\n            first_planes, width, kernel_size=3, stride=1 if use_aa else stride,\n            padding=first_dilation, dilation=first_dilation, groups=cardinality, bias=False)\n        self.bn2 = norm_layer(width)\n        self.drop_block = drop_block() if drop_block is not None else nn.Identity()\n        self.act2 = act_layer(inplace=True)\n        self.aa = create_aa(aa_layer, channels=width, stride=stride, enable=use_aa)", "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\nfrom timm.layers import SelectAdaptivePool2d, Linear, LayerType, PadType, create_conv2d, get_norm_act_layer\nfrom ._builder import build_model_with_cfg, pretrained_cfg_for_features\nfrom ._efficientnet_blocks import SqueezeExcite\nfrom ._efficientnet_builder import BlockArgs, EfficientNetBuilder, decode_arch_def, efficientnet_init_weights, \\\n    round_channels, resolve_bn_args, resolve_act_layer, BN_EPS_TF_DEFAULT\nfrom ._features import FeatureInfo, FeatureHooks, feature_take_indices\nfrom ._manipulate import checkpoint_seq, checkpoint\nfrom ._registry import generate_default_cfgs, register_model, register_model_deprecations", "def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)", "transition_layers = []\n        for i in range(num_branches_cur):\n            if i < num_branches_pre:\n                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n                    transition_layers.append(\n                        nn.Sequential(\n                            nn.Conv2d(\n                                num_channels_pre_layer[i],\n                                num_channels_cur_layer[i],\n                                3,\n                                1,\n                                1,\n                                bias=False,\n                            ),\n                            nn.BatchNorm2d(num_channels_cur_layer[i]),\n                            nn.ReLU(inplace=True),\n                        )\n                    )\n                else:\n                    transition_layers.append(None)\n            else:\n                conv3x3s = []\n                for j in range(i + 1 - num_branches_pre):\n                    inchannels = num_channels_pre_layer[-1]\n                    outchannels = (\n                        num_channels_cur_layer[i] if j == i - num_branches_pre else inchannels\n                    )\n                    conv3x3s.append(\n                        nn.Sequential(\n                            nn.Conv2d(inchannels, outchannels, 3, 2, 1, bias=False),\n                            nn.BatchNorm2d(outchannels),\n                            nn.ReLU(inplace=True),\n                        )\n                    )\n                transition_layers.append(nn.Sequential(*conv3x3s))", "def __init__(self, c1, k=1, s=1, r=16):\n        \"\"\"Initializes MetaAconC with params: channel_in (c1), kernel size (k=1), stride (s=1), reduction (r=16).\"\"\"\n        super().__init__()\n        c2 = max(r, c1 // r)\n        self.p1 = nn.Parameter(torch.randn(1, c1, 1, 1))\n        self.p2 = nn.Parameter(torch.randn(1, c1, 1, 1))\n        self.fc1 = nn.Conv2d(c1, c2, k, s, bias=True)\n        self.fc2 = nn.Conv2d(c2, c1, k, s, bias=True)\n        # self.bn1 = nn.BatchNorm2d(c2)\n        # self.bn2 = nn.BatchNorm2d(c1)", "class Stem(nn.Module):\n    \"\"\"\n    Stem implemented by a layer of convolution.\n    Conv2d params constant across all models.\n    \"\"\"", "self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n        self.rel_pos = rel_pos_cls(num_heads=num_heads) if rel_pos_cls else None\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)", "self.conv3 = Conv2d(\n            bottleneck_channels,\n            out_channels,\n            kernel_size=1,\n            bias=False,\n            norm=get_norm(norm, out_channels),\n        )", "class ASPP(nn.Module):\n    def __init__(self, in_channels: int, atrous_rates: Sequence[int], out_channels: int = 256) -> None:\n        super().__init__()\n        modules = []\n        modules.append(\n            nn.Sequential(nn.Conv2d(in_channels, out_channels, 1, bias=False), nn.BatchNorm2d(out_channels), nn.ReLU())\n        )", "def _initialize_weights(self) -> None:\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)", "self.norm = norm_layer(in_chs)\n        self.even_k = kernel_size % 2 == 0\n        self.conv = nn.Conv2d(\n            in_chs,\n            out_chs,\n            kernel_size=kernel_size,\n            stride=2,\n            padding=0 if self.even_k else kernel_size // 2,\n        )", "self.cheap_operation = nn.Sequential(\n            nn.Conv2d(init_chs, new_chs, dw_size, 1, dw_size//2, groups=init_chs, bias=False),\n            nn.BatchNorm2d(new_chs),\n            # nn.ReLU(inplace=True) if relu else nn.Identity(),\n        )\n        self.relu = nn.ReLU(inplace=False) if relu else nn.Identity()", "def reset_classifier(self, num_classes: int, global_pool: str = 'avg'):\n        self.num_classes = num_classes\n        # cannot meaningfully change pooling of efficient head after creation\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.flatten = nn.Flatten(1) if global_pool else nn.Identity()  # don't flatten if pooling disabled\n        self.classifier = Linear(self.head_hidden_size, num_classes) if num_classes > 0 else nn.Identity()", "self.global_pool = SelectAdaptivePool2d(pool_type=pool_type)\n        if hidden_size is not None:\n            self.num_features = hidden_size\n            last_conv = nn.Conv2d(\n                in_features,\n                hidden_size,\n                kernel_size=1,\n                stride=1,\n                padding=0,\n                bias=False,\n            )\n            act = nn.ReLU()\n            if use_lab:\n                lab = LearnableAffineBlock()\n                self.last_conv = nn.Sequential(last_conv, act, lab)\n            else:\n                self.last_conv = nn.Sequential(last_conv, act)\n        else:\n            self.last_conv = nn.Identity()", "class BasicConv2d(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, **kwargs: Any) -> None:\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)", "class UnusedParamTest(unittest.TestCase):\n    def test_unused(self):\n        class TestMod(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.fc1 = nn.Linear(10, 10)\n                self.t = nn.Linear(10, 10)", "self.stem = nn.Sequential(\n            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n            norm_layer(dims[0])\n        )", "# Performs ResNet-style weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # Note that there is no bias due to BN\n                fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                nn.init.normal_(m.weight, mean=0.0, std=math.sqrt(2.0 / fan_out))\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, mean=0.0, std=0.01)\n                nn.init.zeros_(m.bias)", "self.norm = norm_layer(embed_dim)\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()", "class DlaRoot(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, shortcut):\n        super(DlaRoot, self).__init__()\n        self.conv = nn.Conv2d(\n            in_channels, out_channels, 1, stride=1, bias=False, padding=(kernel_size - 1) // 2)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.shortcut = shortcut", "# stem\n        assert stem_type in ('patch', 'overlap', 'overlap_tiered')\n        if stem_type == 'patch':\n            # NOTE: this stem is a minimal form of ViT PatchEmbed, as used in SwinTransformer w/ patch_size = 4\n            self.stem = nn.Sequential(\n                nn.Conv2d(in_chans, num_init_features, kernel_size=patch_size, stride=patch_size, bias=conv_bias),\n                norm_layer(num_init_features),\n            )\n            stem_stride = patch_size\n        else:\n            mid_chs = make_divisible(num_init_features // 2) if 'tiered' in stem_type else num_init_features\n            self.stem = nn.Sequential(\n                nn.Conv2d(in_chans, mid_chs, kernel_size=3, stride=2, padding=1, bias=conv_bias),\n                nn.Conv2d(mid_chs, num_init_features, kernel_size=3, stride=2, padding=1, bias=conv_bias),\n                norm_layer(num_init_features),\n            )\n            stem_stride = 4", "self.loss_weight = loss_weight\n        use_bias = norm == \"\"\n        # `head` is additional transform before predictor\n        if self.use_depthwise_separable_conv:\n            # We use a single 5x5 DepthwiseSeparableConv2d to replace\n            # 2 3x3 Conv2d since they have the same receptive field.\n            self.head = DepthwiseSeparableConv2d(\n                decoder_channels[0],\n                head_channels,\n                kernel_size=5,\n                padding=2,\n                norm1=norm,\n                activation1=F.relu,\n                norm2=norm,\n                activation2=F.relu,\n            )\n        else:\n            self.head = nn.Sequential(\n                Conv2d(\n                    decoder_channels[0],\n                    decoder_channels[0],\n                    kernel_size=3,\n                    padding=1,\n                    bias=use_bias,\n                    norm=get_norm(norm, decoder_channels[0]),\n                    activation=F.relu,\n                ),\n                Conv2d(\n                    decoder_channels[0],\n                    head_channels,\n                    kernel_size=3,\n                    padding=1,\n                    bias=use_bias,\n                    norm=get_norm(norm, head_channels),\n                    activation=F.relu,\n                ),\n            )\n            weight_init.c2_xavier_fill(self.head[0])\n            weight_init.c2_xavier_fill(self.head[1])\n        self.predictor = Conv2d(head_channels, num_classes, kernel_size=1)\n        nn.init.normal_(self.predictor.weight, 0, 0.001)\n        nn.init.constant_(self.predictor.bias, 0)", "self.qkv = nn.Linear(dim, 3 * dim_out)\n        self.proj = nn.Linear(dim_out, dim_out)", "# Final convolution is initialized differently from the rest\n        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=dropout), final_conv, nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1))\n        )", "self.bbox_reg = nn.Conv2d(in_channels, num_anchors * 4, kernel_size=3, stride=1, padding=1)\n        self.bbox_ctrness = nn.Conv2d(in_channels, num_anchors * 1, kernel_size=3, stride=1, padding=1)\n        for layer in [self.bbox_reg, self.bbox_ctrness]:\n            torch.nn.init.normal_(layer.weight, std=0.01)\n            torch.nn.init.zeros_(layer.bias)", "super().__init__(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            groups,\n            norm_layer,\n            activation_layer,\n            dilation,\n            inplace,\n            bias,\n            torch.nn.Conv2d,\n        )", "@torch.no_grad()\n    def fuse(self):\n        bn, linear = self.bn, self.linear\n        w = bn.weight / (bn.running_var + bn.eps)**0.5\n        b = bn.bias - self.bn.running_mean * \\\n            self.bn.weight / (bn.running_var + bn.eps)**0.5\n        w = linear.weight * w[None, :]\n        if linear.bias is None:\n            b = b @ self.linear.weight.T\n        else:\n            b = (linear.weight @ b[:, None]).view(-1) + self.linear.bias\n        m = torch.nn.Linear(w.size(1), w.size(0))\n        m.weight.data.copy_(w)\n        m.bias.data.copy_(b)\n        return m", "class ChannelAttn(nn.Module):\n    \"\"\" Original CBAM channel attention module, currently avg + max pool variant only.\n    \"\"\"\n    def __init__(\n            self, channels, rd_ratio=1./16, rd_channels=None, rd_divisor=1,\n            act_layer=nn.ReLU, gate_layer='sigmoid', mlp_bias=False):\n        super(ChannelAttn, self).__init__()\n        if not rd_channels:\n            rd_channels = make_divisible(channels * rd_ratio, rd_divisor, round_limit=0.)\n        self.fc1 = nn.Conv2d(channels, rd_channels, 1, bias=mlp_bias)\n        self.act = act_layer(inplace=True)\n        self.fc2 = nn.Conv2d(rd_channels, channels, 1, bias=mlp_bias)\n        self.gate = create_act_layer(gate_layer)", "self.conv1 = nn.Conv2d(\n            inplanes, first_planes, kernel_size=3, stride=1 if use_aa else stride, padding=first_dilation,\n            dilation=first_dilation, bias=False)\n        self.bn1 = norm_layer(first_planes)\n        self.drop_block = drop_block() if drop_block is not None else nn.Identity()\n        self.act1 = act_layer(inplace=True)\n        self.aa = create_aa(aa_layer, channels=first_planes, stride=stride, enable=use_aa)", "self.act = nn.ReLU()\n        self.path_1 = nn.Sequential()\n        self.path_1.add_module('avgpool', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_1.add_module('conv', nn.Conv2d(self.stem_size, self.num_channels // 2, 1, stride=1, bias=False))", "def __init__(self, in_channels, num_classes, conv_block=None):\n        super(InceptionAux, self).__init__()\n        conv_block = conv_block or ConvNormAct\n        self.conv0 = conv_block(in_channels, 128, kernel_size=1)\n        self.conv1 = conv_block(128, 768, kernel_size=5)\n        self.conv1.stddev = 0.01\n        self.fc = Linear(768, num_classes)\n        self.fc.stddev = 0.001", "def __init__(self, dim: int, norm_layer: Callable[..., nn.Module] = nn.LayerNorm):\n        super().__init__()\n        _log_api_usage_once(self)\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n        self.norm = norm_layer(4 * dim)", "self.conv_dw = nn.Conv2d(in_chs, mid_chs, 3, 1, 1, groups=in_chs, bias=bias)\n        self.act = act_layer()\n        self.se = attn_layer(mid_chs, **attn_kwargs)\n        self.conv_pw = nn.Conv2d(mid_chs, out_chs, 1, 1, 0, bias=bias)", "if idx == len(self.in_features) - 1:\n                # ASPP module\n                if train_size is not None:\n                    train_h, train_w = train_size\n                    encoder_stride = in_strides[-1]\n                    if train_h % encoder_stride or train_w % encoder_stride:\n                        raise ValueError(\"Crop size need to be divisible by encoder stride.\")\n                    pool_h = train_h // encoder_stride\n                    pool_w = train_w // encoder_stride\n                    pool_kernel_size = (pool_h, pool_w)\n                else:\n                    pool_kernel_size = None\n                project_conv = ASPP(\n                    in_channel,\n                    aspp_channels,\n                    aspp_dilations,\n                    norm=norm,\n                    activation=F.relu,\n                    pool_kernel_size=pool_kernel_size,\n                    dropout=aspp_dropout,\n                    use_depthwise_separable_conv=use_depthwise_separable_conv,\n                )\n                fuse_conv = None\n            else:\n                project_conv = Conv2d(\n                    in_channel,\n                    project_channels[idx],\n                    kernel_size=1,\n                    bias=use_bias,\n                    norm=get_norm(norm, project_channels[idx]),\n                    activation=F.relu,\n                )\n                weight_init.c2_xavier_fill(project_conv)\n                if use_depthwise_separable_conv:\n                    # We use a single 5x5 DepthwiseSeparableConv2d to replace\n                    # 2 3x3 Conv2d since they have the same receptive field,\n                    # proposed in :paper:`Panoptic-DeepLab`.\n                    fuse_conv = DepthwiseSeparableConv2d(\n                        project_channels[idx] + decoder_channels[idx + 1],\n                        decoder_channels[idx],\n                        kernel_size=5,\n                        padding=2,\n                        norm1=norm,\n                        activation1=F.relu,\n                        norm2=norm,\n                        activation2=F.relu,\n                    )\n                else:\n                    fuse_conv = nn.Sequential(\n                        Conv2d(\n                            project_channels[idx] + decoder_channels[idx + 1],\n                            decoder_channels[idx],\n                            kernel_size=3,\n                            padding=1,\n                            bias=use_bias,\n                            norm=get_norm(norm, decoder_channels[idx]),\n                            activation=F.relu,\n                        ),\n                        Conv2d(\n                            decoder_channels[idx],\n                            decoder_channels[idx],\n                            kernel_size=3,\n                            padding=1,\n                            bias=use_bias,\n                            norm=get_norm(norm, decoder_channels[idx]),\n                            activation=F.relu,\n                        ),\n                    )\n                    weight_init.c2_xavier_fill(fuse_conv[0])\n                    weight_init.c2_xavier_fill(fuse_conv[1])", "cls_subnet = []\n        bbox_subnet = []\n        for in_channels, out_channels in zip(\n            [input_shape[0].channels] + list(conv_dims), conv_dims\n        ):\n            cls_subnet.append(\n                nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n            )\n            if norm:\n                cls_subnet.append(get_norm(norm, out_channels))\n            cls_subnet.append(nn.ReLU())\n            bbox_subnet.append(\n                nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n            )\n            if norm:\n                bbox_subnet.append(get_norm(norm, out_channels))\n            bbox_subnet.append(nn.ReLU())", "def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            assert global_pool in ('avg', '')\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()", "class Linear(nn.Linear):\n    r\"\"\"Applies a linear transformation to the incoming data: :math:`y = xA^T + b`", "self.qkv = nn.Conv2d(dim, dim_attn * 3, 1, bias=bias)\n        self.rel_pos = rel_pos_cls(num_heads=self.num_heads) if rel_pos_cls else None\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Conv2d(dim_attn, dim_out, 1, bias=bias)\n        self.proj_drop = nn.Dropout(proj_drop)", "def _init_weights(module, name=None, head_init_scale=1.0):\n    if isinstance(module, nn.Conv2d):\n        trunc_normal_(module.weight, std=.02)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Linear):\n        trunc_normal_(module.weight, std=.02)\n        nn.init.zeros_(module.bias)\n        if name and 'head.' in name:\n            module.weight.data.mul_(head_init_scale)\n            module.bias.data.mul_(head_init_scale)", "# building first layer\n        stem_chs = make_divisible(16 * width, 4)\n        self.conv_stem = nn.Conv2d(in_chans, stem_chs, 3, 2, 1, bias=False)\n        self.feature_info.append(dict(num_chs=stem_chs, reduction=2, module=f'conv_stem'))\n        self.bn1 = nn.BatchNorm2d(stem_chs)\n        self.act1 = nn.ReLU(inplace=True)\n        prev_chs = stem_chs", "def reset(self, num_classes: int, pool_type: Optional[str] = None):\n        if pool_type is not None:\n            self.global_pool = SelectAdaptivePool2d(pool_type=pool_type)\n            self.flatten = nn.Flatten(1) if pool_type else nn.Identity()\n        self.use_conv = self.global_pool.is_identity()\n        linear_layer = partial(nn.Conv2d, kernel_size=1) if self.use_conv else nn.Linear\n        if self.hidden_size:\n            if ((isinstance(self.pre_logits.fc, nn.Conv2d) and not self.use_conv) or\n                    (isinstance(self.pre_logits.fc, nn.Linear) and self.use_conv)):\n                with torch.no_grad():\n                    new_fc = linear_layer(self.in_features, self.hidden_size)\n                    new_fc.weight.copy_(self.pre_logits.fc.weight.reshape(new_fc.weight.shape))\n                    new_fc.bias.copy_(self.pre_logits.fc.bias)\n                    self.pre_logits.fc = new_fc\n        self.fc = linear_layer(self.num_features, num_classes) if num_classes > 0 else nn.Identity()", "self.conv1 = nn.Conv2d(inplanes, mid_planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_planes)\n        self.conv2 = nn.Conv2d(\n            mid_planes, mid_planes, kernel_size=3,\n            stride=stride, padding=dilation, bias=False, dilation=dilation, groups=cardinality)\n        self.bn2 = nn.BatchNorm2d(mid_planes)\n        self.conv3 = nn.Conv2d(mid_planes, outplanes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(outplanes)\n        self.relu = nn.ReLU(inplace=True)", "class CREStereo(nn.Module):\n    \"\"\"\n    Implements CREStereo from the `\"Practical Stereo Matching via Cascaded Recurrent Network\n    With Adaptive Correlation\" <https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Practical_Stereo_Matching_via_Cascaded_Recurrent_Network_With_Adaptive_Correlation_CVPR_2022_paper.pdf>`_ paper.\n    Args:\n        feature_encoder (raft.FeatureEncoder): Raft-like Feature Encoder module extract low-level features from inputs.\n        update_block (raft.UpdateBlock): Raft-like Update Block which recursively refines a flow-map.\n        flow_head (raft.FlowHead): Raft-like Flow Head which predics a flow-map from some inputs.\n        self_attn_block (LocalFeatureTransformer): A Local Feature Transformer that performs self attention on the two feature maps.\n        cross_attn_block (LocalFeatureTransformer): A Local Feature Transformer that performs cross attention between the two feature maps\n            used in the Adaptive Group Correlation module.\n        feature_downsample_rates (List[int]): The downsample rates used to build a feature pyramid from the outputs of the `feature_encoder`. Default: [2, 4]\n        correlation_groups (int): In how many groups should the features be split when computer per-pixel correlation. Defaults 4.\n        search_window_1d (Tuple[int, int]): The alternate search window size in the x and y directions for the 1D case. Defaults to (1, 9).\n        search_dilate_1d (Tuple[int, int]): The dilation used in the `search_window_1d` when selecting pixels. Similar to `nn.Conv2d` dilate. Defaults to (1, 1).\n        search_window_2d (Tuple[int, int]): The alternate search window size in the x and y directions for the 2D case. Defaults to (3, 3).\n        search_dilate_2d (Tuple[int, int]): The dilation used in the `search_window_2d` when selecting pixels. Similar to `nn.Conv2d` dilate. Defaults to (1, 1).\n    \"\"\"", "def __init__(self, in_chs, out_chs, kernel_size=3, stride=2, padding=None, norm_layer=nn.BatchNorm2d):\n        super().__init__()\n        if padding is None:\n            padding = kernel_size // 2\n        self.conv = nn.Conv2d(in_chs, out_chs, kernel_size=kernel_size, stride=stride, padding=padding)\n        self.norm = norm_layer(out_chs)", "def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            assert global_pool in ('', 'avg', 'avgmax', 'max', 'token', 'map')\n            if global_pool == 'map' and self.attn_pool is None:\n                assert False, \"Cannot currently add attention pooling in reset_classifier().\"\n            elif global_pool != 'map' and self.attn_pool is not None:\n                self.attn_pool = None  # remove attention pooling\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()", "self.conv1 = ConvNormAct(in_channels, groups, 1, act_layer=act_layer, norm_layer=norm_layer)\n        self.conv_p = nn.Conv2d(groups, block_size * block_size * groups, kernel_size=(block_size, 1))\n        self.conv_q = nn.Conv2d(groups, block_size * block_size * groups, kernel_size=(1, block_size))\n        self.conv2 = ConvNormAct(in_channels, in_channels, 1, act_layer=act_layer, norm_layer=norm_layer)\n        self.block_size = block_size\n        self.groups = groups\n        self.in_channels = in_channels", "self.layers = nn.ModuleList()\n        for k in range(num_heads):\n            layer = nn.Linear(in_features, out_features, self.bias)\n            self.layers.append(layer)", "# Building blocks of SSDlite as described in section 6.2 of MobileNetV2 paper\ndef _prediction_block(\n    in_channels: int, out_channels: int, kernel_size: int, norm_layer: Callable[..., nn.Module]\n) -> nn.Sequential:\n    return nn.Sequential(\n        # 3x3 depthwise with stride 1 and padding 1\n        Conv2dNormActivation(\n            in_channels,\n            in_channels,\n            kernel_size=kernel_size,\n            groups=in_channels,\n            norm_layer=norm_layer,\n            activation_layer=nn.ReLU6,\n        ),\n        # 1x1 projetion to output channels\n        nn.Conv2d(in_channels, out_channels, 1),\n    )", "self.in_features = in_features\n        self.out_features = out_features\n        self.spatial_conv = spatial_conv\n        if self.spatial_conv:\n            if group < 2:  # net setting\n                hidden_features = in_features * 5 // 6\n            else:\n                hidden_features = in_features * 2\n        self.hidden_features = hidden_features\n        self.group = group\n        self.conv1 = nn.Conv2d(in_features, hidden_features, 1, stride=1, padding=0, bias=False)\n        self.act1 = act_layer()\n        self.drop1 = nn.Dropout(drop_probs[0])\n        if self.spatial_conv:\n            self.conv2 = nn.Conv2d(\n                hidden_features, hidden_features, 3, stride=1, padding=1, groups=self.group, bias=False)\n            self.act2 = act_layer()\n        else:\n            self.conv2 = None\n            self.act2 = None\n        self.conv3 = nn.Conv2d(hidden_features, out_features, 1, stride=1, padding=0, bias=False)\n        self.drop3 = nn.Dropout(drop_probs[1])", "self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()", "from detectron2.config import CfgNode\nfrom detectron2.layers import Conv2d", "def fuse(self):\n        \"\"\"Fuses Conv2d() and BatchNorm2d() layers in the model to improve inference speed.\"\"\"\n        LOGGER.info(\"Fusing layers... \")\n        for m in self.model.modules():\n            if isinstance(m, (Conv, DWConv)) and hasattr(m, \"bn\"):\n                m.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv\n                delattr(m, \"bn\")  # remove batchnorm\n                m.forward = m.forward_fuse  # update forward\n        self.info()\n        return self", "class KeypointRCNNHeads(nn.Sequential):\n    def __init__(self, in_channels, layers):\n        d = []\n        next_feature = in_channels\n        for out_channels in layers:\n            d.append(nn.Conv2d(next_feature, out_channels, 3, stride=1, padding=1))\n            d.append(nn.ReLU(inplace=True))\n            next_feature = out_channels\n        super().__init__(*d)\n        for m in self.children():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n                nn.init.constant_(m.bias, 0)", "def find_modules(model, mclass=nn.Conv2d):\n    \"\"\"Finds and returns list of layer indices in `model.module_list` matching the specified `mclass`.\"\"\"\n    return [i for i, m in enumerate(model.module_list) if isinstance(m, mclass)]", "self.rel_pos = RelPosBias(window_size=window_size, num_heads=num_heads)\n        if self.use_global:\n            self.qkv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n        else:\n            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)", "for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.trunc_normal_(m.weight, std=0.02)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)", "conv = []\n        for _ in range(num_convs):\n            conv.append(nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1))\n            conv.append(norm_layer(in_channels))\n            conv.append(nn.ReLU())\n        self.conv = nn.Sequential(*conv)", "self.model = nn.Sequential(\n            nn.Linear(128, 400),\n            nn.ReLU(),\n            nn.Linear(400, 400),\n            nn.ReLU(),\n            nn.Linear(400, 400),\n            nn.ReLU(),\n            nn.Linear(400, 1),\n        )\n        if pretrained:\n            # load pretrained model\n            fname = download_url(\n                \"https://raw.githubusercontent.com/han-cai/files/master/ofa/acc_predictor.pth\"\n            )\n            self.model.load_state_dict(\n                torch.load(fname, map_location=torch.device(\"cpu\"))\n            )\n        self.model = self.model.to(self.device)", "class Fire(nn.Module):\n    def __init__(self, inplanes: int, squeeze_planes: int, expand1x1_planes: int, expand3x3_planes: int) -> None:\n        super().__init__()\n        self.inplanes = inplanes\n        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)", "Args:\n            in_chs: Number of input channels.\n            hidden_channels: Number of channels after expansion. Default: None\n            out_chs: Number of output channels. Default: None\n            act_layer: Activation layer. Default: ``GELU``\n            drop: Dropout rate. Default: ``0.0``.\n        \"\"\"\n        super().__init__()\n        out_chs = out_chs or in_chs\n        hidden_channels = hidden_channels or in_chs\n        self.conv = ConvNormAct(\n            in_chs,\n            out_chs,\n            kernel_size=7,\n            groups=in_chs,\n            apply_act=False,\n        )\n        self.fc1 = nn.Conv2d(in_chs, hidden_channels, kernel_size=1)\n        self.act = act_layer()\n        self.fc2 = nn.Conv2d(hidden_channels, out_chs, kernel_size=1)\n        self.drop = nn.Dropout(drop)\n        self.apply(self._init_weights)", "class StdConv2dSame(nn.Conv2d):\n    \"\"\"Conv2d with Weight Standardization. TF compatible SAME padding. Used for ViT Hybrid model.", "Args:\n            cfg (CfgNode): configuration options\n            input_channels (int): number of input channels\n        \"\"\"\n        super(DensePoseV1ConvXHead, self).__init__()\n        # fmt: off\n        hidden_dim           = cfg.MODEL.ROI_DENSEPOSE_HEAD.CONV_HEAD_DIM\n        kernel_size          = cfg.MODEL.ROI_DENSEPOSE_HEAD.CONV_HEAD_KERNEL\n        self.n_stacked_convs = cfg.MODEL.ROI_DENSEPOSE_HEAD.NUM_STACKED_CONVS\n        # fmt: on\n        pad_size = kernel_size // 2\n        n_channels = input_channels\n        for i in range(self.n_stacked_convs):\n            layer = Conv2d(n_channels, hidden_dim, kernel_size, stride=1, padding=pad_size)\n            layer_name = self._get_layer_name(i)\n            self.add_module(layer_name, layer)\n            n_channels = hidden_dim\n        self.n_out_channels = n_channels\n        initialize_module_params(self)", "class _DenseLayer(nn.Module):\n    def __init__(\n        self, num_input_features: int, growth_rate: int, bn_size: int, drop_rate: float, memory_efficient: bool = False\n    ) -> None:\n        super().__init__()\n        self.norm1 = nn.BatchNorm2d(num_input_features)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)", "class ASPPConv(nn.Sequential):\n    def __init__(self, in_channels: int, out_channels: int, dilation: int) -> None:\n        modules = [\n            nn.Conv2d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        ]\n        super().__init__(*modules)", "def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride", "def set_input_size(\n            self,\n            img_size: Optional[Union[int, Tuple[int, int]]] = None,\n            patch_size: Optional[Union[int, Tuple[int, int]]] = None,\n            feature_size: Optional[Union[int, Tuple[int, int]]] = None,\n            feature_ratio: Optional[Union[int, Tuple[int, int]]] = None,\n            feature_dim: Optional[int] = None,\n    ):\n        assert img_size is not None or patch_size is not None\n        img_size = img_size or self.img_size\n        new_patch_size = None\n        if patch_size is not None:\n            new_patch_size = to_2tuple(patch_size)\n        if new_patch_size is not None and new_patch_size != self.patch_size:\n            assert isinstance(self.proj, nn.Conv2d), 'HybridEmbed must have a projection layer to change patch size.'\n            with torch.no_grad():\n                new_proj = nn.Conv2d(\n                    self.proj.in_channels,\n                    self.proj.out_channels,\n                    kernel_size=new_patch_size,\n                    stride=new_patch_size,\n                    bias=self.proj.bias is not None,\n                )\n                new_proj.weight.copy_(resample_patch_embed(self.proj.weight, new_patch_size, verbose=True))\n                if self.proj.bias is not None:\n                    new_proj.bias.copy_(self.proj.bias)\n                self.proj = new_proj\n            patch_size = new_patch_size\n        patch_size = patch_size or self.patch_size", "self.proj_l = nn.Linear(num_heads, num_heads)\n        self.proj_w = nn.Linear(num_heads, num_heads)", "def __init__(\n            self,\n            dim: int,\n            out_dim: Optional[int] = None,\n            norm_layer: Callable = nn.LayerNorm,\n    ):\n        \"\"\"\n        Args:\n            dim: Number of input channels.\n            out_dim: Number of output channels (or 2 * dim if None)\n            norm_layer: Normalization layer.\n        \"\"\"\n        super().__init__()\n        self.dim = dim\n        self.out_dim = out_dim or 2 * dim\n        self.norm = norm_layer(4 * dim)\n        self.reduction = nn.Linear(4 * dim, self.out_dim, bias=False)", "class SqueezeExciteCl(nn.Module):\n    \"\"\" SE Module as defined in original SE-Nets with a few additions\n    Additions include:\n        * divisor can be specified to keep channels % div == 0 (default: 8)\n        * reduction channels can be specified directly by arg (if rd_channels is set)\n        * reduction channels can be specified by float rd_ratio (default: 1/16)\n        * global max pooling can be added to the squeeze aggregation\n        * customizable activation, normalization, and gate layer\n    \"\"\"\n    def __init__(\n            self, channels, rd_ratio=1. / 16, rd_channels=None, rd_divisor=8,\n            bias=True, act_layer=nn.ReLU, gate_layer='sigmoid'):\n        super().__init__()\n        if not rd_channels:\n            rd_channels = make_divisible(channels * rd_ratio, rd_divisor, round_limit=0.)\n        self.fc1 = nn.Linear(channels, rd_channels, bias=bias)\n        self.act = create_act_layer(act_layer, inplace=True)\n        self.fc2 = nn.Linear(rd_channels, channels, bias=bias)\n        self.gate = create_act_layer(gate_layer)", "self.conv1 = Conv2d(\n            in_channels,\n            bottleneck_channels,\n            kernel_size=1,\n            stride=stride_1x1,\n            bias=False,\n            norm=get_norm(norm, bottleneck_channels),\n        )", "# Depth-wise convolution\n        if self.stride > 1:\n            self.conv_dw = nn.Conv2d(\n                mid_chs, mid_chs, dw_kernel_size, stride=stride,\n                padding=(dw_kernel_size-1)//2, groups=mid_chs, bias=False)\n            self.bn_dw = nn.BatchNorm2d(mid_chs)\n        else:\n            self.conv_dw = None\n            self.bn_dw = None", "@torch.no_grad()\n    def fuse(self):\n        bn, l = self._modules.values()\n        w = bn.weight / (bn.running_var + bn.eps) ** 0.5\n        b = bn.bias - self.bn.running_mean * self.bn.weight / (bn.running_var + bn.eps) ** 0.5\n        w = l.weight * w[None, :]\n        if l.bias is None:\n            b = b @ self.l.weight.T\n        else:\n            b = (l.weight @ b[:, None]).view(-1) + self.l.bias\n        m = nn.Linear(w.size(1), w.size(0), device=l.weight.device)\n        m.weight.data.copy_(w)\n        m.bias.data.copy_(b)\n        return m", "self.inplanes = 64\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            # each element in the tuple indicates if we should replace\n            # the 2x2 stride with a dilated convolution instead\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(\n                \"replace_stride_with_dilation should be None \"\n                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n            )\n        self.groups = groups\n        self.base_width = width_per_group\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)", "dense_stage_layers.append(norm_layer(num_features))\n                dense_stage_layers.append(\n                    nn.Conv2d(num_features, compressed_num_features, kernel_size=k_size, stride=stride, padding=0)\n                )\n                num_features = compressed_num_features", "self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)", "self.center_loss_weight = center_loss_weight\n        self.offset_loss_weight = offset_loss_weight\n        use_bias = norm == \"\"\n        # center prediction\n        # `head` is additional transform before predictor\n        self.center_head = nn.Sequential(\n            Conv2d(\n                decoder_channels[0],\n                decoder_channels[0],\n                kernel_size=3,\n                padding=1,\n                bias=use_bias,\n                norm=get_norm(norm, decoder_channels[0]),\n                activation=F.relu,\n            ),\n            Conv2d(\n                decoder_channels[0],\n                head_channels,\n                kernel_size=3,\n                padding=1,\n                bias=use_bias,\n                norm=get_norm(norm, head_channels),\n                activation=F.relu,\n            ),\n        )\n        weight_init.c2_xavier_fill(self.center_head[0])\n        weight_init.c2_xavier_fill(self.center_head[1])\n        self.center_predictor = Conv2d(head_channels, 1, kernel_size=1)\n        nn.init.normal_(self.center_predictor.weight, 0, 0.001)\n        nn.init.constant_(self.center_predictor.bias, 0)", "def _create_pool(\n        num_features: int,\n        num_classes: int,\n        pool_type: str = 'avg',\n        use_conv: bool = False,\n        input_fmt: Optional[str] = None,\n):\n    flatten_in_pool = not use_conv  # flatten when we use a Linear layer after pooling\n    if not pool_type:\n        flatten_in_pool = False  # disable flattening if pooling is pass-through (no pooling)\n    global_pool = SelectAdaptivePool2d(\n        pool_type=pool_type,\n        flatten=flatten_in_pool,\n        input_fmt=input_fmt,\n    )\n    num_pooled_features = num_features * global_pool.feat_mult()\n    return global_pool, num_pooled_features", "for layer in self.conv.children():\n            if isinstance(layer, nn.Conv2d):\n                torch.nn.init.normal_(layer.weight, std=0.01)\n                torch.nn.init.constant_(layer.bias, 0)", "super().__init__(*blocks)\n        for layer in self.modules():\n            if isinstance(layer, nn.Conv2d):\n                nn.init.kaiming_normal_(layer.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n                if layer.bias is not None:\n                    nn.init.zeros_(layer.bias)", "def __init__(self, in_channels=3, out_channels=128, norm=\"BN\"):\n        \"\"\"\n        Args:\n            norm (str or callable): norm after the first conv layer.\n                See :func:`layers.get_norm` for supported format.\n        \"\"\"\n        super().__init__(in_channels, out_channels, 4)\n        self.in_channels = in_channels\n        self.conv1 = Conv2d(\n            in_channels,\n            out_channels // 2,\n            kernel_size=3,\n            stride=2,\n            padding=1,\n            bias=False,\n            norm=get_norm(norm, out_channels // 2),\n        )\n        self.conv2 = Conv2d(\n            out_channels // 2,\n            out_channels // 2,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias=False,\n            norm=get_norm(norm, out_channels // 2),\n        )\n        self.conv3 = Conv2d(\n            out_channels // 2,\n            out_channels,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias=False,\n            norm=get_norm(norm, out_channels),\n        )\n        weight_init.c2_msra_fill(self.conv1)\n        weight_init.c2_msra_fill(self.conv2)\n        weight_init.c2_msra_fill(self.conv3)", "def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)", "# Classifier module\n        self.head = nn.Sequential(\n            nn.Dropout(dropout, inplace=True),\n            nn.Linear(block_setting[-1].output_channels, num_classes),\n        )", "self.proj = nn.Conv2d(\n            in_chans, embed_dim, kernel_size=kernel_size, stride=stride, padding=padding\n        )", "def reset_classifier(self, num_classes: int, global_pool: str = 'avg'):\n        self.num_classes = num_classes\n        # NOTE: cannot meaningfully change pooling of efficient head after creation\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.flatten = nn.Flatten(1) if global_pool else nn.Identity()  # don't flatten if pooling disabled\n        self.classifier = Linear(self.head_hidden_size, num_classes) if num_classes > 0 else nn.Identity()", "def __init__(\n            self,\n            dim,\n            expansion_ratio=8 / 3,\n            kernel_size=7,\n            conv_ratio=1.0,\n            ls_init_value=None,\n            norm_layer=LayerNorm,\n            act_layer=nn.GELU,\n            drop_path=0.,\n            **kwargs\n    ):\n        super().__init__()\n        self.norm = norm_layer(dim)\n        hidden = int(expansion_ratio * dim)\n        self.fc1 = nn.Linear(dim, hidden * 2)\n        self.act = act_layer()\n        conv_channels = int(conv_ratio * dim)\n        self.split_indices = (hidden, hidden - conv_channels, conv_channels)\n        self.conv = nn.Conv2d(\n            conv_channels,\n            conv_channels,\n            kernel_size=kernel_size,\n            padding=kernel_size // 2,\n            groups=conv_channels\n        )\n        self.fc2 = nn.Linear(hidden, dim)\n        self.ls = LayerScale(dim) if ls_init_value is not None else nn.Identity()\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()", "self.dropout = nn.Dropout(drop_rate)\n        self.flatten = nn.Flatten(1) if pool_type else nn.Identity()  # don't flatten if pooling disabled\n        self.fc = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()", "def _init_conv(module, name, scheme=''):\n    if isinstance(module, nn.Conv2d):\n        fan_out = module.kernel_size[0] * module.kernel_size[1] * module.out_channels\n        fan_out //= module.groups\n        nn.init.normal_(module.weight, 0, math.sqrt(2.0 / fan_out))\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)", "Modified by Ross Wightman to fix some issues with factorization dims for non nn.Linear layers", "def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n        self.head_dist = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()", "class SqueezeNet(nn.Module):\n    def __init__(self, version: str = \"1_0\", num_classes: int = 1000, dropout: float = 0.5) -> None:\n        super().__init__()\n        _log_api_usage_once(self)\n        self.num_classes = num_classes\n        if version == \"1_0\":\n            self.features = nn.Sequential(\n                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n                Fire(96, 16, 64, 64),\n                Fire(128, 16, 64, 64),\n                Fire(128, 32, 128, 128),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n                Fire(256, 32, 128, 128),\n                Fire(256, 48, 192, 192),\n                Fire(384, 48, 192, 192),\n                Fire(384, 64, 256, 256),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n                Fire(512, 64, 256, 256),\n            )\n        elif version == \"1_1\":\n            self.features = nn.Sequential(\n                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n                Fire(64, 16, 64, 64),\n                Fire(128, 16, 64, 64),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n                Fire(128, 32, 128, 128),\n                Fire(256, 32, 128, 128),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n                Fire(256, 48, 192, 192),\n                Fire(384, 48, 192, 192),\n                Fire(384, 64, 256, 256),\n                Fire(512, 64, 256, 256),\n            )\n        else:\n            # FIXME: Is this needed? SqueezeNet should only be called from the\n            # FIXME: squeezenet1_x() functions\n            # FIXME: This checking is not done for the other models\n            raise ValueError(f\"Unsupported SqueezeNet version {version}: 1_0 or 1_1 expected\")", "self.norm1 = norm_layer(dim)\n        if dim != dim_out:\n            self.do_expand = True\n            if use_expand_proj:\n                self.proj = nn.Linear(dim, dim_out)\n            else:\n                assert dim_out == dim * 2\n                self.proj = None\n        else:\n            self.do_expand = False\n            self.proj = None\n        self.attn = MaskUnitAttention(\n            dim,\n            dim_out,\n            heads,\n            q_stride,\n            window_size,\n            use_mask_unit_attn\n        )\n        self.ls1 = LayerScale(dim_out, init_values=init_values) if init_values is not None else nn.Identity()\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0 else nn.Identity()", "# building last several layers\n        self.num_features = prev_chs\n        self.head_hidden_size = out_chs = 1280\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.conv_head = nn.Conv2d(prev_chs, out_chs, 1, 1, 0, bias=True)\n        self.act2 = nn.ReLU(inplace=True)\n        self.flatten = nn.Flatten(1) if global_pool else nn.Identity()  # don't flatten if pooling disabled\n        self.classifier = Linear(out_chs, num_classes) if num_classes > 0 else nn.Identity()", "\"\"\" Image to Patch Embedding using Conv2d", "def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()", "def __init__(self, alpha: float, num_classes: int = 1000, dropout: float = 0.2) -> None:\n        super().__init__()\n        _log_api_usage_once(self)\n        if alpha <= 0.0:\n            raise ValueError(f\"alpha should be greater than 0.0 instead of {alpha}\")\n        self.alpha = alpha\n        self.num_classes = num_classes\n        depths = _get_depths(alpha)\n        layers = [\n            # First layer: regular conv.\n            nn.Conv2d(3, depths[0], 3, padding=1, stride=2, bias=False),\n            nn.BatchNorm2d(depths[0], momentum=_BN_MOMENTUM),\n            nn.ReLU(inplace=True),\n            # Depthwise separable, no skip.\n            nn.Conv2d(depths[0], depths[0], 3, padding=1, stride=1, groups=depths[0], bias=False),\n            nn.BatchNorm2d(depths[0], momentum=_BN_MOMENTUM),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(depths[0], depths[1], 1, padding=0, stride=1, bias=False),\n            nn.BatchNorm2d(depths[1], momentum=_BN_MOMENTUM),\n            # MNASNet blocks: stacks of inverted residuals.\n            _stack(depths[1], depths[2], 3, 2, 3, 3, _BN_MOMENTUM),\n            _stack(depths[2], depths[3], 5, 2, 3, 3, _BN_MOMENTUM),\n            _stack(depths[3], depths[4], 5, 2, 6, 3, _BN_MOMENTUM),\n            _stack(depths[4], depths[5], 3, 1, 6, 2, _BN_MOMENTUM),\n            _stack(depths[5], depths[6], 5, 2, 6, 4, _BN_MOMENTUM),\n            _stack(depths[6], depths[7], 3, 1, 6, 1, _BN_MOMENTUM),\n            # Final mapping to classifier input.\n            nn.Conv2d(depths[7], 1280, 1, padding=0, stride=1, bias=False),\n            nn.BatchNorm2d(1280, momentum=_BN_MOMENTUM),\n            nn.ReLU(inplace=True),\n        ]\n        self.layers = nn.Sequential(*layers)\n        self.classifier = nn.Sequential(nn.Dropout(p=dropout, inplace=True), nn.Linear(1280, num_classes))", "# stem net\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(Bottleneck, 64, 4)", "class NormMlpClassifierHead(nn.Module):\n    \"\"\" A Pool -> Norm -> Mlp Classifier Head for '2D' NCHW tensors\n    \"\"\"\n    def __init__(\n            self,\n            in_features: int,\n            num_classes: int,\n            hidden_size: Optional[int] = None,\n            pool_type: str = 'avg',\n            drop_rate: float = 0.,\n            norm_layer: Union[str, Callable] = 'layernorm2d',\n            act_layer: Union[str, Callable] = 'tanh',\n    ):\n        \"\"\"\n        Args:\n            in_features: The number of input features.\n            num_classes:  The number of classes for the final classifier layer (output).\n            hidden_size: The hidden size of the MLP (pre-logits FC layer) if not None.\n            pool_type: Global pooling type, pooling disabled if empty string ('').\n            drop_rate: Pre-classifier dropout rate.\n            norm_layer: Normalization layer type.\n            act_layer: MLP activation layer type (only used if hidden_size is not None).\n        \"\"\"\n        super().__init__()\n        self.in_features = in_features\n        self.hidden_size = hidden_size\n        self.num_features = in_features\n        self.use_conv = not pool_type\n        norm_layer = get_norm_layer(norm_layer)\n        act_layer = get_act_layer(act_layer)\n        linear_layer = partial(nn.Conv2d, kernel_size=1) if self.use_conv else nn.Linear", "class StdConv2d(nn.Conv2d):\n    \"\"\"Conv2d with Weight Standardization. Used for BiT ResNet-V2 models.", "class ScaledStdConv2d(nn.Conv2d):\n    \"\"\"Conv2d layer with Scaled Weight Standardization.", "# offset prediction\n        # `head` is additional transform before predictor\n        if self.use_depthwise_separable_conv:\n            # We use a single 5x5 DepthwiseSeparableConv2d to replace\n            # 2 3x3 Conv2d since they have the same receptive field.\n            self.offset_head = DepthwiseSeparableConv2d(\n                decoder_channels[0],\n                head_channels,\n                kernel_size=5,\n                padding=2,\n                norm1=norm,\n                activation1=F.relu,\n                norm2=norm,\n                activation2=F.relu,\n            )\n        else:\n            self.offset_head = nn.Sequential(\n                Conv2d(\n                    decoder_channels[0],\n                    decoder_channels[0],\n                    kernel_size=3,\n                    padding=1,\n                    bias=use_bias,\n                    norm=get_norm(norm, decoder_channels[0]),\n                    activation=F.relu,\n                ),\n                Conv2d(\n                    decoder_channels[0],\n                    head_channels,\n                    kernel_size=3,\n                    padding=1,\n                    bias=use_bias,\n                    norm=get_norm(norm, head_channels),\n                    activation=F.relu,\n                ),\n            )\n            weight_init.c2_xavier_fill(self.offset_head[0])\n            weight_init.c2_xavier_fill(self.offset_head[1])\n        self.offset_predictor = Conv2d(head_channels, 2, kernel_size=1)\n        nn.init.normal_(self.offset_predictor.weight, 0, 0.001)\n        nn.init.constant_(self.offset_predictor.bias, 0)", "from detectron2.config import configurable\nfrom detectron2.data import MetadataCatalog\nfrom detectron2.layers import Conv2d, DepthwiseSeparableConv2d, ShapeSpec, get_norm\nfrom detectron2.modeling import (\n    META_ARCH_REGISTRY,\n    SEM_SEG_HEADS_REGISTRY,\n    build_backbone,\n    build_sem_seg_head,\n)\nfrom detectron2.modeling.postprocessing import sem_seg_postprocess\nfrom detectron2.projects.deeplab import DeepLabV3PlusHead\nfrom detectron2.projects.deeplab.loss import DeepLabCE\nfrom detectron2.structures import BitMasks, ImageList, Instances\nfrom detectron2.utils.registry import Registry", "from .batch_norm import get_norm\nfrom .blocks import DepthwiseSeparableConv2d\nfrom .wrappers import Conv2d", "layers: list[nn.Module] = []\n        # split image into non-overlapping patches\n        layers.append(\n            nn.Sequential(\n                nn.Conv2d(\n                    3, embed_dim, kernel_size=(patch_size[0], patch_size[1]), stride=(patch_size[0], patch_size[1])\n                ),\n                Permute([0, 2, 3, 1]),\n                norm_layer(embed_dim),\n            )\n        )", "# Official init from torch repo.\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.constant_(m.bias, 0)", "def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            self.pooling = SelectAdaptivePool2d(pool_type=global_pool, flatten=True)\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()", "self.norm = norm_layer(in_features)\n        if hidden_size:\n            self.pre_logits = nn.Sequential(OrderedDict([\n                ('fc', nn.Linear(in_features, hidden_size)),\n                ('act', act_layer()),\n                ('norm', norm_layer(hidden_size))\n            ]))\n            self.num_features = hidden_size\n        else:\n            self.num_features = in_features\n            self.pre_logits = nn.Identity()", "def _init_weights(self, m: nn.Module) -> None:\n        \"\"\"Init. for classification\"\"\"\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=0.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)", "def __init__(self, in_channels, out_channels, in_feature=\"res5\"):\n        super().__init__()\n        self.num_levels = 2\n        self.in_feature = in_feature\n        self.p6 = nn.Conv2d(in_channels, out_channels, 3, 2, 1)\n        self.p7 = nn.Conv2d(out_channels, out_channels, 3, 2, 1)\n        for module in [self.p6, self.p7]:\n            weight_init.c2_xavier_fill(module)", "output_channels = self._stage_out_channels[-1]\n        self.conv5 = nn.Sequential(\n            nn.Conv2d(input_channels, output_channels, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(output_channels),\n            nn.ReLU(inplace=True),\n        )", "# Classifier head\n        self.feature_info = [dict(num_chs=embed_dim, reduction=0, module='head')]\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()", "self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=kernel_size, stride=stride, padding=padding)\n        self.norm = norm_layer(in_chans) # affine over C", "def init_models(net, model_init=\"he_fout\"):\n    \"\"\"\n    Conv2d,\n    BatchNorm2d, BatchNorm1d, GroupNorm\n    Linear,\n    \"\"\"\n    if isinstance(net, list):\n        for sub_net in net:\n            init_models(sub_net, model_init)\n        return\n    for m in net.modules():\n        if isinstance(m, nn.Conv2d):\n            if model_init == \"he_fout\":\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2.0 / n))\n            elif model_init == \"he_fin\":\n                n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n                m.weight.data.normal_(0, math.sqrt(2.0 / n))\n            else:\n                raise NotImplementedError\n            if m.bias is not None:\n                m.bias.data.zero_()\n        elif type(m) in [nn.BatchNorm2d, nn.BatchNorm1d, nn.GroupNorm]:\n            m.weight.data.fill_(1)\n            m.bias.data.zero_()\n        elif isinstance(m, nn.Linear):\n            stdv = 1.0 / math.sqrt(m.weight.size(1))\n            m.weight.data.uniform_(-stdv, stdv)\n            if m.bias is not None:\n                m.bias.data.zero_()", "if 'visual.class_embedding' in state_dict:\n        state_dict = _convert_openai_clip(state_dict, model)\n    elif 'module.visual.class_embedding' in state_dict:\n        state_dict = _convert_openai_clip(state_dict, model, prefix='module.visual.')\n    elif \"mask_token\" in state_dict:\n        state_dict = _convert_dinov2(state_dict, model)\n    elif \"encoder\" in state_dict:\n        # IJEPA, vit in an 'encoder' submodule\n        state_dict = state_dict['encoder']\n        prefix = 'module.'\n    elif 'visual.trunk.pos_embed' in state_dict or 'visual.trunk.blocks.0.norm1.weight' in state_dict:\n        # OpenCLIP model with timm vision encoder\n        prefix = 'visual.trunk.'\n        if 'visual.head.proj.weight' in state_dict and isinstance(model.head, nn.Linear):\n            # remap final nn.Linear if it exists outside of the timm .trunk (ie in visual.head.proj)\n            out_dict['head.weight'] = state_dict['visual.head.proj.weight']\n            out_dict['head.bias'] = torch.zeros(state_dict['visual.head.proj.weight'].shape[0])\n    elif 'preprocessor.patchifier.proj.weight' in state_dict:\n        state_dict = _convert_aimv2(state_dict, model)", "class CustomModel(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.linear = nn.Linear(10, 10)", "self.act = nn.ReLU()\n        self.path_1 = nn.Sequential()\n        self.path_1.add_module('avgpool', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_1.add_module('conv', nn.Conv2d(in_chs_left, out_chs_left, 1, stride=1, bias=False))", "def __init__(self, in_channels: int, num_anchors: int, conv_depth=1) -> None:\n        super().__init__()\n        convs = []\n        for _ in range(conv_depth):\n            convs.append(Conv2dNormActivation(in_channels, in_channels, kernel_size=3, norm_layer=None))\n        self.conv = nn.Sequential(*convs)\n        self.cls_logits = nn.Conv2d(in_channels, num_anchors, kernel_size=1, stride=1)\n        self.bbox_pred = nn.Conv2d(in_channels, num_anchors * 4, kernel_size=1, stride=1)", "def reset(self, num_classes: Optional[int] = None, pool_type: Optional[str] = None):\n        # NOTE: this module is being used as a head, so need compatible reset()\n        if pool_type is not None:\n            assert pool_type in ('', 'token')\n            self.pool_type = pool_type\n        if num_classes is not None:\n            self.proj = nn.Linear(self.in_features, num_classes) if num_classes > 0 else nn.Identity()\n            self.out_features = num_classes if num_classes > 0 else self.embed_dim", "self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.attn_out_proj = nn.Linear(dim, dim, bias=proj_bias)", "def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(\n                    self.inplanes,\n                    planes * block.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                    bias=False,\n                ),\n                nn.BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n            )", "super().__init__()\n        self.mask_head = nn.Sequential(\n            Conv2dNormActivation(in_channels, hidden_size, norm_layer=None, kernel_size=3),\n            # https://arxiv.org/pdf/2003.12039.pdf (Annex section B) for the\n            # following convolution output size\n            nn.Conv2d(hidden_size, upsample_factor**2 * 9, 1, padding=0),\n        )", "def _init_weights(self, m):\n        if isinstance(m, (nn.Conv2d, nn.Linear)):\n            trunc_normal_(m.weight, std=.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)", "\"\"\" Create Conv2d Factory Method", "@torch.no_grad()\n    def fuse(self):\n        l, bn = self.linear, self.bn\n        w = bn.weight / (bn.running_var + bn.eps) ** 0.5\n        w = l.weight * w[:, None]\n        b = bn.bias - bn.running_mean * bn.weight / (bn.running_var + bn.eps) ** 0.5\n        m = nn.Linear(w.size(1), w.size(0))\n        m.weight.data.copy_(w)\n        m.bias.data.copy_(b)\n        return m", "def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            self.global_pool = global_pool\n        self.head = nn.Sequential(OrderedDict([\n            ('drop', nn.Dropout(self.drop_rate)),\n            ('fc', nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity())\n        ]))", "# Skip pooling with kernel and stride size of (1, 1, 1).\n        if prod(kernel_q) == 1 and prod(stride_q) == 1:\n            kernel_q = None\n        if prod(kernel_kv) == 1 and prod(stride_kv) == 1:\n            kernel_kv = None\n        self.mode = mode\n        self.unshared = mode == 'conv_unshared'\n        self.pool_q, self.pool_k, self.pool_v = None, None, None\n        self.norm_q, self.norm_k, self.norm_v = None, None, None\n        if mode in (\"avg\", \"max\"):\n            pool_op = nn.MaxPool2d if mode == \"max\" else nn.AvgPool2d\n            if kernel_q:\n                self.pool_q = pool_op(kernel_q, stride_q, padding_q)\n            if kernel_kv:\n                self.pool_k = pool_op(kernel_kv, stride_kv, padding_kv)\n                self.pool_v = pool_op(kernel_kv, stride_kv, padding_kv)\n        elif mode == \"conv\" or mode == \"conv_unshared\":\n            dim_conv = dim // num_heads if mode == \"conv\" else dim\n            if kernel_q:\n                self.pool_q = nn.Conv2d(\n                    dim_conv,\n                    dim_conv,\n                    kernel_q,\n                    stride=stride_q,\n                    padding=padding_q,\n                    groups=dim_conv,\n                    bias=False,\n                )\n                self.norm_q = norm_layer(dim_conv)\n            if kernel_kv:\n                self.pool_k = nn.Conv2d(\n                    dim_conv,\n                    dim_conv,\n                    kernel_kv,\n                    stride=stride_kv,\n                    padding=padding_kv,\n                    groups=dim_conv,\n                    bias=False,\n                )\n                self.norm_k = norm_layer(dim_conv)\n                self.pool_v = nn.Conv2d(\n                    dim_conv,\n                    dim_conv,\n                    kernel_kv,\n                    stride=stride_kv,\n                    padding=padding_kv,\n                    groups=dim_conv,\n                    bias=False,\n                )\n                self.norm_v = norm_layer(dim_conv)\n        else:\n            raise NotImplementedError(f\"Unsupported model {mode}\")", "def __init__(\n            self, block, layers, groups, reduction, drop_rate=0.2,\n            in_chans=3, inplanes=64, input_3x3=False, downsample_kernel_size=1,\n            downsample_padding=0, num_classes=1000, global_pool='avg'):\n        \"\"\"\n        Parameters\n        ----------\n        block (nn.Module): Bottleneck class.\n            - For SENet154: SEBottleneck\n            - For SE-ResNet models: SEResNetBottleneck\n            - For SE-ResNeXt models:  SEResNeXtBottleneck\n        layers (list of ints): Number of residual blocks for 4 layers of the\n            network (layer1...layer4).\n        groups (int): Number of groups for the 3x3 convolution in each\n            bottleneck block.\n            - For SENet154: 64\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models:  32\n        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n            - For all models: 16\n        dropout_p (float or None): Drop probability for the Dropout layer.\n            If `None` the Dropout layer is not used.\n            - For SENet154: 0.2\n            - For SE-ResNet models: None\n            - For SE-ResNeXt models: None\n        inplanes (int):  Number of input channels for layer1.\n            - For SENet154: 128\n            - For SE-ResNet models: 64\n            - For SE-ResNeXt models: 64\n        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n            a single 7x7 convolution in layer0.\n            - For SENet154: True\n            - For SE-ResNet models: False\n            - For SE-ResNeXt models: False\n        downsample_kernel_size (int): Kernel size for downsampling convolutions\n            in layer2, layer3 and layer4.\n            - For SENet154: 3\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models: 1\n        downsample_padding (int): Padding for downsampling convolutions in\n            layer2, layer3 and layer4.\n            - For SENet154: 1\n            - For SE-ResNet models: 0\n            - For SE-ResNeXt models: 0\n        num_classes (int): Number of outputs in `last_linear` layer.\n            - For all models: 1000\n        \"\"\"\n        super(SENet, self).__init__()\n        self.inplanes = inplanes\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        if input_3x3:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(in_chans, 64, 3, stride=2, padding=1, bias=False)),\n                ('bn1', nn.BatchNorm2d(64)),\n                ('relu1', nn.ReLU(inplace=True)),\n                ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1, bias=False)),\n                ('bn2', nn.BatchNorm2d(64)),\n                ('relu2', nn.ReLU(inplace=True)),\n                ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1, bias=False)),\n                ('bn3', nn.BatchNorm2d(inplanes)),\n                ('relu3', nn.ReLU(inplace=True)),\n            ]\n        else:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(\n                    in_chans, inplanes, kernel_size=7, stride=2, padding=3, bias=False)),\n                ('bn1', nn.BatchNorm2d(inplanes)),\n                ('relu1', nn.ReLU(inplace=True)),\n            ]\n        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n        # To preserve compatibility with Caffe weights `ceil_mode=True` is used instead of `padding=1`.\n        self.pool0 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n        self.feature_info = [dict(num_chs=inplanes, reduction=2, module='layer0')]\n        self.layer1 = self._make_layer(\n            block,\n            planes=64,\n            blocks=layers[0],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=1,\n            downsample_padding=0\n        )\n        self.feature_info += [dict(num_chs=64 * block.expansion, reduction=4, module='layer1')]\n        self.layer2 = self._make_layer(\n            block,\n            planes=128,\n            blocks=layers[1],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.feature_info += [dict(num_chs=128 * block.expansion, reduction=8, module='layer2')]\n        self.layer3 = self._make_layer(\n            block,\n            planes=256,\n            blocks=layers[2],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.feature_info += [dict(num_chs=256 * block.expansion, reduction=16, module='layer3')]\n        self.layer4 = self._make_layer(\n            block,\n            planes=512,\n            blocks=layers[3],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.feature_info += [dict(num_chs=512 * block.expansion, reduction=32, module='layer4')]\n        self.num_features = self.head_hidden_size = 512 * block.expansion\n        self.global_pool, self.last_linear = create_classifier(\n            self.num_features, self.num_classes, pool_type=global_pool)", "self.qkv_proj = nn.Conv2d(\n            in_channels=embed_dim,\n            out_channels=1 + (2 * embed_dim),\n            bias=bias,\n            kernel_size=1,\n        )\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.out_proj = nn.Conv2d(\n            in_channels=embed_dim,\n            out_channels=embed_dim,\n            bias=bias,\n            kernel_size=1,\n        )\n        self.out_drop = nn.Dropout(proj_drop)", "self.qkv = nn.Conv2d(dim, dim_attn * 3, 1, bias=bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Conv2d(dim_attn, dim_out, 1, bias=bias)\n        self.proj_drop = nn.Dropout(proj_drop)", "# init for classification\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)", "def init_weights_vit_timm(module: nn.Module, name: str = '') -> None:\n    \"\"\" ViT weight initialization, original timm impl (for reproducibility) \"\"\"\n    if isinstance(module, nn.Linear):\n        trunc_normal_(module.weight, std=.02)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif hasattr(module, 'init_weights'):\n        module.init_weights()", "# build modules\n        self.conv1 = nn.Sequential(\n            OrderedDict(\n                [\n                    (\n                        \"conv\",\n                        nn.Conv2d(self.in_channels, feature_dim, 1, 1, 0, bias=False),\n                    ),\n                    (\"bn\", nn.BatchNorm2d(feature_dim)),\n                    (\"act\", build_activation(self.act_func, inplace=True)),\n                ]\n            )\n        )", "for idx, layer_channels in enumerate(conv_dims, 1):\n            module = Conv2d(in_channels, layer_channels, 3, stride=1, padding=1)\n            self.add_module(\"conv_fcn{}\".format(idx), module)\n            self.add_module(\"conv_fcn_relu{}\".format(idx), nn.ReLU())\n            in_channels = layer_channels", "weights = np.load(checkpoint_path)\n    stem_conv_w = adapt_input_conv(\n        model.stem.conv.weight.shape[1], t2p(weights[f'{prefix}root_block/standardized_conv2d/kernel']))\n    model.stem.conv.weight.copy_(stem_conv_w)\n    model.norm.weight.copy_(t2p(weights[f'{prefix}group_norm/gamma']))\n    model.norm.bias.copy_(t2p(weights[f'{prefix}group_norm/beta']))\n    if isinstance(getattr(model.head, 'fc', None), nn.Conv2d) and \\\n            model.head.fc.weight.shape[0] == weights[f'{prefix}head/conv2d/kernel'].shape[-1]:\n        model.head.fc.weight.copy_(t2p(weights[f'{prefix}head/conv2d/kernel']))\n        model.head.fc.bias.copy_(t2p(weights[f'{prefix}head/conv2d/bias']))\n    for i, (sname, stage) in enumerate(model.stages.named_children()):\n        for j, (bname, block) in enumerate(stage.blocks.named_children()):\n            cname = 'standardized_conv2d'\n            block_prefix = f'{prefix}block{i + 1}/unit{j + 1:02d}/'\n            block.conv1.weight.copy_(t2p(weights[f'{block_prefix}a/{cname}/kernel']))\n            block.conv2.weight.copy_(t2p(weights[f'{block_prefix}b/{cname}/kernel']))\n            block.conv3.weight.copy_(t2p(weights[f'{block_prefix}c/{cname}/kernel']))\n            block.norm1.weight.copy_(t2p(weights[f'{block_prefix}a/group_norm/gamma']))\n            block.norm2.weight.copy_(t2p(weights[f'{block_prefix}b/group_norm/gamma']))\n            block.norm3.weight.copy_(t2p(weights[f'{block_prefix}c/group_norm/gamma']))\n            block.norm1.bias.copy_(t2p(weights[f'{block_prefix}a/group_norm/beta']))\n            block.norm2.bias.copy_(t2p(weights[f'{block_prefix}b/group_norm/beta']))\n            block.norm3.bias.copy_(t2p(weights[f'{block_prefix}c/group_norm/beta']))\n            if block.downsample is not None:\n                w = weights[f'{block_prefix}a/proj/{cname}/kernel']\n                block.downsample.conv.weight.copy_(t2p(w))", "self.fc = None\n        if with_fc:\n            if union == \"cat\":\n                self.fc = nn.Linear(2 * self.output_size, input_size)\n            elif union == \"add\":\n                self.fc = nn.Linear(self.output_size, input_size)\n            elif union == \"vertical\":\n                self.fc = nn.Linear(self.output_size, input_size)\n                self.with_horizontal = False\n            elif union == \"horizontal\":\n                self.fc = nn.Linear(self.output_size, input_size)\n                self.with_vertical = False\n            else:\n                raise ValueError(\"Unrecognized union: \" + union)\n        elif union == \"cat\":\n            pass\n            if 2 * self.output_size != input_size:\n                raise ValueError(f\"The output channel {2 * self.output_size} is different from the input channel {input_size}.\")\n        elif union == \"add\":\n            pass\n            if self.output_size != input_size:\n                raise ValueError(f\"The output channel {self.output_size} is different from the input channel {input_size}.\")\n        elif union == \"vertical\":\n            if self.output_size != input_size:\n                raise ValueError(f\"The output channel {self.output_size} is different from the input channel {input_size}.\")\n            self.with_horizontal = False\n        elif union == \"horizontal\":\n            if self.output_size != input_size:\n                raise ValueError(f\"The output channel {self.output_size} is different from the input channel {input_size}.\")\n            self.with_vertical = False\n        else:\n            raise ValueError(\"Unrecognized union: \" + union)", "self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()", "self.conv = nn.Conv2d(\n            in_channels, out_channels, kernel_size=patch_size,\n            stride=stride, padding=padding, bias=True)", "def _init_weights(module, name='', zero_init_last=False):\n    if isinstance(module, nn.Conv2d):\n        fan_out = module.kernel_size[0] * module.kernel_size[1] * module.out_channels\n        fan_out //= module.groups\n        module.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n        if module.bias is not None:\n            module.bias.data.zero_()\n    elif isinstance(module, nn.Linear):\n        nn.init.normal_(module.weight, mean=0.0, std=0.01)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.BatchNorm2d):\n        nn.init.ones_(module.weight)\n        nn.init.zeros_(module.bias)\n    elif hasattr(module, 'init_weights'):\n        module.init_weights(zero_init_last=zero_init_last)", "self.reduce_spatial_dim_conv = Conv2d(\n            conv_dim, conv_dim, kernel_size=2, stride=2, padding=0, bias=True, activation=F.relu\n        )\n        self.conv_layers.append(self.reduce_spatial_dim_conv)", "if inference_mode:\n            self.reparam_conv = nn.Conv2d(\n                self.dim,\n                self.dim_out,\n                kernel_size=self.spatial_shape,\n                stride=1,\n                padding=spatial_shape[0] // 2,\n                groups=self.groups,\n                bias=True,\n            )\n        else:\n            self.reparam_conv = None\n            self.pos_enc = nn.Conv2d(\n                self.dim,\n                self.dim_out,\n                spatial_shape,\n                1,\n                int(spatial_shape[0] // 2),\n                groups=self.groups,\n                bias=True,\n            )", "from detectron2.layers import (\n    CNNBlockBase,\n    Conv2d,\n    DeformConv,\n    ModulatedDeformConv,\n    ShapeSpec,\n    get_norm,\n)", "self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.use_rel_pos = use_rel_pos\n        if self.use_rel_pos:\n            assert rope is None\n            assert (\n                input_size is not None\n            ), \"Input size must be provided if using relative positional encoding.\"\n            # initialize relative positional embeddings\n            self.rel_pos_h = nn.Parameter(torch.zeros(\n                2 * input_size[0] - 1, self.head_dim))\n            self.rel_pos_w = nn.Parameter(torch.zeros(\n                2 * input_size[1] - 1, self.head_dim))\n        self.rope = rope", "from detectron2.config import configurable\nfrom detectron2.layers import Conv2d, ShapeSpec, get_norm\nfrom detectron2.structures import ImageList\nfrom detectron2.utils.registry import Registry", "def conv_nd(n: int) -> Type[nn.Module]:\n    \"\"\"\n    Returns a conv with nd (e.g., Conv2d for n=2). Work up to n=3.\n    If you wanted a 4d Hiera, you could probably just implement this for n=4. (no promises)\n    \"\"\"\n    return [nn.Identity, nn.Conv1d, nn.Conv2d, nn.Conv3d][n]", "self.conv_kxk = layers.conv_norm_act(\n            in_chs, in_chs, kernel_size=kernel_size,\n            stride=stride, groups=groups, dilation=dilation[0])\n        self.conv_1x1 = nn.Conv2d(in_chs, transformer_dim, kernel_size=1, bias=False)", "def __init__(self, inplanes, planes, groups, reduction, stride=1, downsample=None):\n        super(SEResNetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False, stride=stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride", "name, m = list((model.model if hasattr(model, \"model\") else model).named_children())[-1]  # last module\n    if isinstance(m, Classify):  # YOLOv5 Classify() head\n        if m.linear.out_features != n:\n            m.linear = nn.Linear(m.linear.in_features, n)\n    elif isinstance(m, nn.Linear):  # ResNet, EfficientNet\n        if m.out_features != n:\n            setattr(model, name, nn.Linear(m.in_features, n))\n    elif isinstance(m, nn.Sequential):\n        types = [type(x) for x in m]\n        if nn.Linear in types:\n            i = len(types) - 1 - types[::-1].index(nn.Linear)  # last nn.Linear index\n            if m[i].out_features != n:\n                m[i] = nn.Linear(m[i].in_features, n)\n        elif nn.Conv2d in types:\n            i = len(types) - 1 - types[::-1].index(nn.Conv2d)  # last nn.Conv2d index\n            if m[i].out_channels != n:\n                m[i] = nn.Conv2d(m[i].in_channels, n, m[i].kernel_size, m[i].stride, bias=m[i].bias is not None)", "self.fc1_g = nn.Linear(in_features, hidden_features, bias=bias[0])\n        self.fc1_x = nn.Linear(in_features, hidden_features, bias=bias[0])\n        self.act = act_layer()\n        self.drop1 = nn.Dropout(drop_probs[0])\n        self.norm = norm_layer(hidden_features) if norm_layer is not None else nn.Identity()\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias[1])\n        self.drop2 = nn.Dropout(drop_probs[1])", "self.norm = norm_layer(in_features)\n        self.w0 = nn.Linear(in_features, hidden_features, bias=bias)\n        self.act = create_act_layer(act_layer)\n        self.w1 = nn.Linear(in_features, hidden_features, bias=bias)\n        self.w2 = nn.Linear(hidden_features, in_features, bias=bias)", "self.norm2 = nn.BatchNorm2d(bn_size * growth_rate)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)", "self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)", "def _init_weights(self, module, name, scheme='vit'):\n        # note Conv2d left as default init\n        if scheme == 'vit':\n            if isinstance(module, nn.Linear):\n                nn.init.xavier_uniform_(module.weight)\n                if module.bias is not None:\n                    if 'mlp' in name:\n                        nn.init.normal_(module.bias, std=1e-6)\n                    else:\n                        nn.init.zeros_(module.bias)\n        else:\n            if isinstance(module, nn.Linear):\n                nn.init.normal_(module.weight, std=.02)\n                if module.bias is not None:\n                    nn.init.zeros_(module.bias)", "num_features = embed_dim * 2 ** (len(depths) - 1)\n        self.norm = norm_layer(num_features)\n        self.permute = Permute([0, 3, 1, 2])  # B H W C -> B C H W\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.flatten = nn.Flatten(1)\n        self.head = nn.Linear(num_features, num_classes)", "def __init__(\n        self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.0\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)", "self.q = nn.Linear(dim, dim_out, bias=qkv_bias)\n        self.k = nn.Linear(dim, dim_out, bias=qkv_bias)\n        self.v = nn.Linear(dim, dim_out, bias=qkv_bias)\n        self.proj = nn.Linear(dim_out, dim_out)", "# if using MlpHead, dropout is handled by MlpHead\n        if num_classes > 0:\n            if self.use_mlp_head:\n                # FIXME not actually returning mlp hidden state right now as pre-logits.\n                final = MlpHead(self.num_features, num_classes, drop_rate=self.drop_rate)\n                self.head_hidden_size = self.num_features\n            else:\n                final = nn.Linear(self.num_features, num_classes)\n                self.head_hidden_size = self.num_features\n        else:\n            final = nn.Identity()", "if dim != dim_out:\n            self.expand = nn.Conv2d(dim, dim_out, 1, bias=bias)\n        else:\n            self.expand = nn.Identity()", "def _init_weights(self, m):\n        if isinstance(m, (nn.Conv2d, nn.Linear)):\n            trunc_normal_(m.weight, std=.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)", "class ConvNorm(torch.nn.Sequential):\n    def __init__(self, in_chs, out_chs, ks=1, stride=1, pad=0, dilation=1, groups=1, bn_weight_init=1):\n        super().__init__()\n        self.conv = nn.Conv2d(in_chs, out_chs, ks, stride, pad, dilation, groups, bias=False)\n        self.bn = nn.BatchNorm2d(out_chs)\n        torch.nn.init.constant_(self.bn.weight, bn_weight_init)\n        torch.nn.init.constant_(self.bn.bias, 0)", "def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n                    downsample_kernel_size=1, downsample_padding=0):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(\n                    self.inplanes, planes * block.expansion, kernel_size=downsample_kernel_size,\n                    stride=stride, padding=downsample_padding, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )", "self.norm = nn.LayerNorm(dim)\n        self.qkv = nn.Linear(dim, num_heads * (self.val_dim + 2 * key_dim))\n        self.proj = nn.Linear(self.out_dim, dim)", "def reset(self, num_classes: Optional[int] = None, pool_type: Optional[str] = None):\n        # NOTE: this module is being used as a head, so need compatible reset()\n        if pool_type is not None:\n            assert pool_type in ('', 'token')\n            self.pool_type = pool_type\n        if num_classes is not None:\n            self.proj = nn.Linear(self.in_features, num_classes) if num_classes > 0 else nn.Identity()\n            self.out_features = num_classes if num_classes > 0 else self.embed_dim", "if in_channels != out_channels:\n            self.shortcut = Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size=1,\n                stride=stride,\n                bias=False,\n                norm=get_norm(norm, out_channels),\n            )\n        else:\n            self.shortcut = None", "Based on: `Pay Attention to MLPs` - https://arxiv.org/abs/2105.08050\n    \"\"\"\n    def __init__(self, dim, seq_len, norm_layer=nn.LayerNorm):\n        super().__init__()\n        gate_dim = dim // 2\n        self.norm = norm_layer(gate_dim)\n        self.proj = nn.Linear(seq_len, seq_len)", "@staticmethod\n    def depthwise_conv(\n        i: int, o: int, kernel_size: int, stride: int = 1, padding: int = 0, bias: bool = False\n    ) -> nn.Conv2d:\n        return nn.Conv2d(i, o, kernel_size, stride, padding, bias=bias, groups=i)", "self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)", "class SSDRegressionHead(SSDScoringHead):\n    def __init__(self, in_channels: list[int], num_anchors: list[int]):\n        bbox_reg = nn.ModuleList()\n        for channels, anchors in zip(in_channels, num_anchors):\n            bbox_reg.append(nn.Conv2d(channels, 4 * anchors, kernel_size=3, padding=1))\n        _xavier_init(bbox_reg)\n        super().__init__(bbox_reg, 4)", "def reset(self, num_classes: int, pool_type: Optional[str] = None):\n        if pool_type is not None:\n            assert pool_type, 'Cannot disable pooling'\n            self.global_pool = SelectAdaptivePool2d(pool_type=pool_type, flatten=True,)\n        if num_classes > 0:\n            self.classifier[-1] = nn.Linear(self.num_features, num_classes, bias=True)\n        else:\n            self.classifier[-1] = nn.Identity()", "from detectron2.layers import Conv2d, FrozenBatchNorm2d, get_norm\nfrom detectron2.modeling import BACKBONE_REGISTRY, ResNet, ResNetBlockBase\nfrom detectron2.modeling.backbone.resnet import BasicStem, BottleneckBlock, DeformBottleneckBlock", "class MyConv2d(nn.Conv2d):\n    \"\"\"\n    Conv2d with Weight Standardization\n    https://github.com/joe-siyuan-qiao/WeightStandardization\n    \"\"\"", "class LinearNorm(nn.Module):\n    def __init__(self, in_features, out_features, bn_weight_init=1):\n        super().__init__()\n        self.linear = nn.Linear(in_features, out_features, bias=False)\n        self.bn = nn.BatchNorm1d(out_features)", "def __init__(self, in_channels=3, out_channels=64, norm=\"BN\"):\n        \"\"\"\n        Args:\n            norm (str or callable): norm after the first conv layer.\n                See :func:`layers.get_norm` for supported format.\n        \"\"\"\n        super().__init__(in_channels, out_channels, 4)\n        self.in_channels = in_channels\n        self.conv1 = Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=7,\n            stride=2,\n            padding=3,\n            bias=False,\n            norm=get_norm(norm, out_channels),\n        )\n        weight_init.c2_msra_fill(self.conv1)", "self.conv1 = nn.Conv2d(inplanes, mid_planes * scale, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_planes * scale)", "layers: list[nn.Module] = []\n        if expand_ratio != 1:\n            # pw\n            layers.append(\n                Conv2dNormActivation(inp, hidden_dim, kernel_size=1, norm_layer=norm_layer, activation_layer=nn.ReLU6)\n            )\n        layers.extend(\n            [\n                # dw\n                Conv2dNormActivation(\n                    hidden_dim,\n                    hidden_dim,\n                    stride=stride,\n                    groups=hidden_dim,\n                    norm_layer=norm_layer,\n                    activation_layer=nn.ReLU6,\n                ),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                norm_layer(oup),\n            ]\n        )\n        self.conv = nn.Sequential(*layers)\n        self.out_channels = oup\n        self._is_cn = stride > 1", "class _Transition(nn.Sequential):\n    def __init__(self, num_input_features: int, num_output_features: int) -> None:\n        super().__init__()\n        self.norm = nn.BatchNorm2d(num_input_features)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv = nn.Conv2d(num_input_features, num_output_features, kernel_size=1, stride=1, bias=False)\n        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)", "# qkv pooling\n        pool_padding = [k // 2 for k in pool_kernel]\n        dim_conv = dim_out // num_heads\n        self.pool_q = nn.Conv2d(\n            dim_conv,\n            dim_conv,\n            pool_kernel,\n            stride=stride_q,\n            padding=pool_padding,\n            groups=dim_conv,\n            bias=False,\n        )\n        self.norm_q = norm_layer(dim_conv)\n        self.pool_k = nn.Conv2d(\n            dim_conv,\n            dim_conv,\n            pool_kernel,\n            stride=stride_kv,\n            padding=pool_padding,\n            groups=dim_conv,\n            bias=False,\n        )\n        self.norm_k = norm_layer(dim_conv)\n        self.pool_v = nn.Conv2d(\n            dim_conv,\n            dim_conv,\n            pool_kernel,\n            stride=stride_kv,\n            padding=pool_padding,\n            groups=dim_conv,\n            bias=False,\n        )\n        self.norm_v = norm_layer(dim_conv)", "layers.extend(\n                [\n                    Conv2d(\n                        out_dim,\n                        out_channels,\n                        kernel_size=1,\n                        bias=use_bias,\n                        norm=get_norm(norm, out_channels),\n                    ),\n                    Conv2d(\n                        out_channels,\n                        out_channels,\n                        kernel_size=3,\n                        padding=1,\n                        bias=use_bias,\n                        norm=get_norm(norm, out_channels),\n                    ),\n                ]\n            )\n            layers = nn.Sequential(*layers)", "if inference_mode:\n            self.reparam_conv = nn.Conv2d(\n                self.dim,\n                self.dim,\n                kernel_size=self.kernel_size,\n                stride=1,\n                padding=self.kernel_size // 2,\n                groups=self.dim,\n                bias=True,\n            )\n        else:\n            self.reparam_conv = None\n            self.norm = MobileOneBlock(\n                dim,\n                dim,\n                kernel_size,\n                group_size=1,\n                use_act=False,\n                use_scale_branch=False,\n                num_conv_branches=0,\n            )\n            self.mixer = MobileOneBlock(\n                dim,\n                dim,\n                kernel_size,\n                group_size=1,\n                use_act=False,\n            )\n            if layer_scale_init_value is not None:\n                self.layer_scale = LayerScale2d(dim, layer_scale_init_value)\n            else:\n                self.layer_scale = nn.Identity()", "def __init__(\n            self,\n            dim: int,\n            out_dim: Optional[int] = None,\n            norm_layer: Type[nn.Module] = nn.LayerNorm\n    ):\n        \"\"\"\n        Args:\n            dim (int): Number of input channels.\n            out_dim (int): Number of output channels (or 2 * dim if None)\n            norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n        \"\"\"\n        super().__init__()\n        self.dim = dim\n        self.out_dim = out_dim or 2 * dim\n        self.reduction = nn.Linear(4 * dim, self.out_dim, bias=False)\n        self.norm = norm_layer(self.out_dim)", "class GELUTanh(nn.Module):\n    \"\"\"Applies the Gaussian Error Linear Units function (w/ dummy inplace arg)\n    \"\"\"\n    def __init__(self, inplace: bool = False):\n        super(GELUTanh, self).__init__()", "self.stem = nn.Sequential(\n            nn.Conv2d(in_chans, dim, kernel_size=patch_size, stride=patch_size),\n            act_layer(),\n            nn.BatchNorm2d(dim)\n        )\n        self.blocks = nn.Sequential(\n            *[nn.Sequential(\n                    Residual(nn.Sequential(\n                        nn.Conv2d(dim, dim, kernel_size, groups=dim, padding=\"same\"),\n                        act_layer(),\n                        nn.BatchNorm2d(dim)\n                    )),\n                    nn.Conv2d(dim, dim, kernel_size=1),\n                    act_layer(),\n                    nn.BatchNorm2d(dim)\n            ) for i in range(depth)]\n        )\n        self.pooling = SelectAdaptivePool2d(pool_type=global_pool, flatten=True)\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(dim, num_classes) if num_classes > 0 else nn.Identity()", "def __init__(self, inplanes, planes, groups, reduction, stride=1, downsample=None, base_width=4):\n        super(SEResNeXtBottleneck, self).__init__()\n        width = math.floor(planes * (base_width / 64)) * groups\n        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False, stride=1)\n        self.bn1 = nn.BatchNorm2d(width)\n        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride, padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(width)\n        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride", "class ConvNeXtBlock(nn.Module):\n    \"\"\" ConvNeXt Block\n    There are two equivalent implementations:\n      (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n      (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back", "if stride == 2:\n            self.pool = nn.AvgPool2d((2, 2), stride=2, ceil_mode=True, count_include_pad=False)\n            self.conv = nn.Conv2d(in_chs, out_chs, kernel_size=1, stride=1, bias=False)\n            self.norm = norm_layer(out_chs)\n        elif in_chs != out_chs:\n            self.pool = nn.Identity()\n            self.conv = nn.Conv2d(in_chs, out_chs, kernel_size=1, stride=1, bias=False)\n            self.norm = norm_layer(out_chs)\n        else:\n            self.pool = nn.Identity()\n            self.conv = nn.Identity()\n            self.norm = nn.Identity()", "# Depth-wise convolution\n        if self.stride > 1:\n            self.conv_dw = nn.Conv2d(\n                mid_chs, mid_chs, dw_kernel_size, stride=stride,\n                padding=(dw_kernel_size-1)//2, groups=mid_chs, bias=False)\n            self.bn_dw = nn.BatchNorm2d(mid_chs)\n        else:\n            self.conv_dw = None\n            self.bn_dw = None", "assert stem_type in ('patch', 'overlap')\n        if stem_type == 'patch':\n            self.stem = nn.Sequential(\n                nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4, bias=conv_bias),\n                norm_layer(dims[0]),\n            )\n        else:\n            self.stem = nn.Sequential(\n                nn.Conv2d(in_chans, dims[0], kernel_size=9, stride=4, padding=9 // 2, bias=conv_bias),\n                norm_layer(dims[0]),\n            )", "class TestModule(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.conv = torch.nn.Conv2d(3, 1, 3)\n                self.leaf_module = LeafModule()", "def conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution + batch norm\"\"\"\n    return torch.nn.Sequential(\n        nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False),\n        nn.BatchNorm2d(out_planes)\n    )", "class SSDClassificationHead(SSDScoringHead):\n    def __init__(self, in_channels: list[int], num_anchors: list[int], num_classes: int):\n        cls_logits = nn.ModuleList()\n        for channels, anchors in zip(in_channels, num_anchors):\n            cls_logits.append(nn.Conv2d(channels, num_classes * anchors, kernel_size=3, padding=1))\n        _xavier_init(cls_logits)\n        super().__init__(cls_logits, num_classes)", "if dim != dim_out:\n            self.expand = nn.Conv2d(dim, dim_out, 1, bias=bias) # 1x1 conv\n        else:\n            self.expand = nn.Identity()", "self.conv3 = Conv2d(bottleneck_channels, out_channels, 1, bias=False)\n        self.norm3 = get_norm(norm, out_channels)", "def _create_fc(num_features, num_classes, use_conv=False):\n    if num_classes <= 0:\n        fc = nn.Identity()  # pass-through (no classifier)\n    elif use_conv:\n        fc = nn.Conv2d(num_features, num_classes, 1, bias=True)\n    else:\n        fc = nn.Linear(num_features, num_classes, bias=True)\n    return fc", "if out_channels != in_channels or strides != 1:\n            self.skip = nn.Conv2d(in_channels, out_channels, 1, stride=strides, bias=False)\n            self.skipbn = nn.BatchNorm2d(out_channels)\n        else:\n            self.skip = None", "self.prediction = nn.Linear(fc_dims[-1], output_dim)\n        # use normal distribution initialization for mask prediction layer\n        nn.init.normal_(self.prediction.weight, std=0.001)\n        nn.init.constant_(self.prediction.bias, 0)", "def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()", "def __init__(\n            self,\n            img_size=224,\n            stem_conv=False,\n            stem_stride=1,\n            patch_size=8,\n            in_chans=3,\n            hidden_dim=64,\n            embed_dim=384,\n    ):\n        super().__init__()\n        assert patch_size in [4, 8, 16]\n        if stem_conv:\n            self.conv = nn.Sequential(\n                nn.Conv2d(in_chans, hidden_dim, kernel_size=7, stride=stem_stride, padding=3, bias=False),  # 112x112\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=1, bias=False),  # 112x112\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=1, bias=False),  # 112x112\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            self.conv = None", "@torch.jit.ignore\n    def init_weights(self, zero_init_last: bool = True):\n        for n, m in self.named_modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n        if zero_init_last:\n            for m in self.modules():\n                if hasattr(m, 'zero_init_last'):\n                    m.zero_init_last()", "# building last several layers\n        self.num_features = prev_chs\n        self.head_hidden_size = out_chs = 1280\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.conv_head = nn.Conv2d(prev_chs, out_chs, 1, 1, 0, bias=True)\n        self.act2 = nn.ReLU(inplace=True)\n        self.flatten = nn.Flatten(1) if global_pool else nn.Identity()  # don't flatten if pooling disabled\n        self.classifier = Linear(out_chs, num_classes) if num_classes > 0 else nn.Identity()", "self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim)", "self.project: Optional[nn.Module] = None\n        if cnf.input_channels != cnf.output_channels:\n            self.project = nn.Linear(cnf.input_channels, cnf.output_channels)", "def _init_weights(module, name='', zero_init_last=False):\n    if isinstance(module, nn.Conv2d):\n        fan_out = module.kernel_size[0] * module.kernel_size[1] * module.out_channels\n        fan_out //= module.groups\n        module.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n        if module.bias is not None:\n            module.bias.data.zero_()\n    elif isinstance(module, nn.Linear):\n        nn.init.normal_(module.weight, mean=0.0, std=0.01)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif zero_init_last and hasattr(module, 'zero_init_last'):\n        module.zero_init_last()", "self.qkv = nn.Linear(dim, attn_dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(attn_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)", "\"\"\" Conv2d + BN + Act", "if version == 1 and not self.alpha == 1.0:\n            # In the initial version of the model (v1), stem was fixed-size.\n            # All other layer configurations were the same. This will patch\n            # the model so that it's identical to v1. Model with alpha 1.0 is\n            # unaffected.\n            depths = _get_depths(self.alpha)\n            v1_stem = [\n                nn.Conv2d(3, 32, 3, padding=1, stride=2, bias=False),\n                nn.BatchNorm2d(32, momentum=_BN_MOMENTUM),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(32, 32, 3, padding=1, stride=1, groups=32, bias=False),\n                nn.BatchNorm2d(32, momentum=_BN_MOMENTUM),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(32, 16, 1, padding=0, stride=1, bias=False),\n                nn.BatchNorm2d(16, momentum=_BN_MOMENTUM),\n                _stack(16, depths[2], 3, 2, 3, 3, _BN_MOMENTUM),\n            ]\n            for idx, layer in enumerate(v1_stem):\n                self.layers[idx] = layer", "class NormLinear(torch.nn.Sequential):\n    def __init__(self, in_features, out_features, bias=True, std=0.02, drop=0.):\n        super().__init__()\n        self.bn = nn.BatchNorm1d(in_features)\n        self.drop = nn.Dropout(drop)\n        self.linear = nn.Linear(in_features, out_features, bias=bias)", "self.v = nn.Linear(dim, dim, bias=qkv_bias)\n        self.attn = nn.Linear(dim, kernel_size ** 4 * num_heads)", "def init_weights_vit_jax(module: nn.Module, name: str = '', head_bias: float = 0.0) -> None:\n    \"\"\" ViT weight initialization, matching JAX (Flax) impl \"\"\"\n    if isinstance(module, nn.Linear):\n        if name.startswith('head'):\n            nn.init.zeros_(module.weight)\n            nn.init.constant_(module.bias, head_bias)\n        else:\n            nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                nn.init.normal_(module.bias, std=1e-6) if 'mlp' in name else nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Conv2d):\n        lecun_normal_(module.weight)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif hasattr(module, 'init_weights'):\n        module.init_weights()", "def make_layers(cfg: list[Union[str, int]], batch_norm: bool = False) -> nn.Sequential:\n    layers: list[nn.Module] = []\n    in_channels = 3\n    for v in cfg:\n        if v == \"M\":\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        else:\n            v = cast(int, v)\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    return nn.Sequential(*layers)", "def _init_conv(module, name, scheme=''):\n    if isinstance(module, nn.Conv2d):\n        if scheme == 'normal':\n            nn.init.normal_(module.weight, std=.02)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        elif scheme == 'trunc_normal':\n            trunc_normal_tf_(module.weight, std=.02)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        elif scheme == 'xavier_normal':\n            nn.init.xavier_normal_(module.weight)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        else:\n            # efficientnet like\n            fan_out = module.kernel_size[0] * module.kernel_size[1] * module.out_channels\n            fan_out //= module.groups\n            nn.init.normal_(module.weight, 0, math.sqrt(2.0 / fan_out))\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)", "if num_classes is not None:\n            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n            self.linear = nn.Linear(curr_channels, num_classes)", "def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_tf_(m.weight, std=0.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0.0)", "self.qkv = nn.Linear(dim, 3*dim, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)", "def conv_bn(in_chs, out_chs, k=3, stride=1, padding=None, dilation=1):\n    if padding is None:\n        padding = ((stride - 1) + dilation * (k - 1)) // 2\n    return nn.Sequential(\n        nn.Conv2d(in_chs, out_chs, k, stride, padding=padding, dilation=dilation, bias=False),\n        nn.BatchNorm2d(out_chs),\n        nn.ReLU(inplace=True)\n    )", "Args:\n            in_chs: Number of input image channels.\n            out_chs: Number of linear projection output channels.\n            stride: Downsample stride.\n            overlap: Use overlapping convolutions if True.\n            norm_layer: Normalization layer.\n        \"\"\"\n        super().__init__()\n        self.stride = stride\n        padding = 0\n        kernel_size = stride\n        if overlap:\n            assert stride in (2, 4)\n            if stride == 4:\n                kernel_size, padding = 7, 2\n            elif stride == 2:\n                kernel_size, padding = 3, 1\n        self.proj = nn.Conv2d(in_chs, out_chs, kernel_size=kernel_size, stride=stride, padding=padding)\n        self.norm = norm_layer(out_chs) if norm_layer is not None else nn.Identity()", "# switching to 768 initial embeddings\n        decoder_embedding = 768 if decoder_embedding < 0 else decoder_embedding\n        self.embed_standart = nn.Linear(initial_num_features, decoder_embedding)", "self.qkv = nn.Linear(dim, dim * 3, bias=False)\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(dim))\n            self.register_buffer('k_bias', torch.zeros(dim), persistent=False)\n            self.v_bias = nn.Parameter(torch.zeros(dim))\n        else:\n            self.q_bias = None\n            self.k_bias = None\n            self.v_bias = None\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.softmax = nn.Softmax(dim=-1)", "self.conv2d = nn.Conv2d(384, 1088, kernel_size=1, stride=1)\n        self.act = nn.ReLU()", "# Initialization\n        for modules in modules_list:\n            for layer in modules.modules():\n                if isinstance(layer, nn.Conv2d):\n                    torch.nn.init.normal_(layer.weight, mean=0, std=0.01)\n                    torch.nn.init.constant_(layer.bias, 0)", "self.num_prefix_tokens = 2\n        self.dist_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n        self.pos_embed = nn.Parameter(\n            torch.zeros(1, self.patch_embed.num_patches + self.num_prefix_tokens, self.embed_dim))\n        self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if self.num_classes > 0 else nn.Identity()\n        self.distilled_training = False  # must set this True to train w/ distillation token", "# FIXME not clear if this stride behaviour is what the paper intended\n        # Also, the paper mentions using a 3D conv for dealing with the blocking/gather, and leaving\n        # data in unfolded block form. I haven't wrapped my head around how that'd look.\n        self.q = nn.Conv2d(dim, self.dim_out_qk, 1, stride=self.block_stride, bias=qkv_bias)\n        self.kv = nn.Conv2d(dim, self.dim_out_qk + self.dim_out_v, 1, bias=qkv_bias)", "def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)", "def __init__(\n            self,\n            in_chs=96,\n            out_chs=198,\n            norm_layer=LayerNorm,\n    ):\n        super().__init__()\n        self.norm = norm_layer(in_chs)\n        self.conv = nn.Conv2d(\n            in_chs,\n            out_chs,\n            kernel_size=3,\n            stride=2,\n            padding=1\n        )", "from detectron2.config import configurable\nfrom detectron2.layers import Conv2d, ShapeSpec, cat\nfrom detectron2.structures import Boxes, ImageList, Instances, pairwise_iou\nfrom detectron2.utils.events import get_event_storage\nfrom detectron2.utils.memory import retry_if_cuda_oom\nfrom detectron2.utils.registry import Registry"]