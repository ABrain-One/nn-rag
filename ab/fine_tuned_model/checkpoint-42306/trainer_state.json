{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9998227264669386,
  "eval_steps": 500,
  "global_step": 42306,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0007090941322460556,
      "grad_norm": 0.793201744556427,
      "learning_rate": 0.00019995272538174253,
      "loss": 1.9336,
      "step": 10
    },
    {
      "epoch": 0.0014181882644921113,
      "grad_norm": 0.8548299670219421,
      "learning_rate": 0.0001999054507634851,
      "loss": 1.6481,
      "step": 20
    },
    {
      "epoch": 0.002127282396738167,
      "grad_norm": 0.8612930774688721,
      "learning_rate": 0.00019985817614522763,
      "loss": 1.477,
      "step": 30
    },
    {
      "epoch": 0.0028363765289842226,
      "grad_norm": 0.7039531469345093,
      "learning_rate": 0.00019981090152697018,
      "loss": 1.2224,
      "step": 40
    },
    {
      "epoch": 0.003545470661230278,
      "grad_norm": 0.7653282284736633,
      "learning_rate": 0.00019976362690871273,
      "loss": 1.122,
      "step": 50
    },
    {
      "epoch": 0.004254564793476334,
      "grad_norm": 1.0965416431427002,
      "learning_rate": 0.00019971635229045525,
      "loss": 1.0904,
      "step": 60
    },
    {
      "epoch": 0.0049636589257223895,
      "grad_norm": 1.1053794622421265,
      "learning_rate": 0.0001996690776721978,
      "loss": 0.8968,
      "step": 70
    },
    {
      "epoch": 0.005672753057968445,
      "grad_norm": 1.0096896886825562,
      "learning_rate": 0.00019962180305394035,
      "loss": 0.8773,
      "step": 80
    },
    {
      "epoch": 0.006381847190214501,
      "grad_norm": 0.9786328673362732,
      "learning_rate": 0.0001995745284356829,
      "loss": 0.8183,
      "step": 90
    },
    {
      "epoch": 0.007090941322460556,
      "grad_norm": 0.9007595181465149,
      "learning_rate": 0.00019952725381742543,
      "loss": 0.8051,
      "step": 100
    },
    {
      "epoch": 0.007800035454706612,
      "grad_norm": 0.8999798893928528,
      "learning_rate": 0.00019947997919916798,
      "loss": 0.6767,
      "step": 110
    },
    {
      "epoch": 0.008509129586952668,
      "grad_norm": 1.4061540365219116,
      "learning_rate": 0.00019943270458091053,
      "loss": 0.7636,
      "step": 120
    },
    {
      "epoch": 0.009218223719198724,
      "grad_norm": 1.1065741777420044,
      "learning_rate": 0.00019938542996265305,
      "loss": 0.6461,
      "step": 130
    },
    {
      "epoch": 0.009927317851444779,
      "grad_norm": 0.9951900839805603,
      "learning_rate": 0.0001993381553443956,
      "loss": 0.7286,
      "step": 140
    },
    {
      "epoch": 0.010636411983690836,
      "grad_norm": 0.8543180823326111,
      "learning_rate": 0.00019929088072613815,
      "loss": 0.6311,
      "step": 150
    },
    {
      "epoch": 0.01134550611593689,
      "grad_norm": 1.3052494525909424,
      "learning_rate": 0.0001992436061078807,
      "loss": 0.6049,
      "step": 160
    },
    {
      "epoch": 0.012054600248182947,
      "grad_norm": 1.3258410692214966,
      "learning_rate": 0.00019919633148962322,
      "loss": 0.6393,
      "step": 170
    },
    {
      "epoch": 0.012763694380429002,
      "grad_norm": 2.073603630065918,
      "learning_rate": 0.00019914905687136577,
      "loss": 0.5784,
      "step": 180
    },
    {
      "epoch": 0.013472788512675058,
      "grad_norm": 0.9939327836036682,
      "learning_rate": 0.00019910178225310832,
      "loss": 0.512,
      "step": 190
    },
    {
      "epoch": 0.014181882644921113,
      "grad_norm": 1.0717014074325562,
      "learning_rate": 0.00019905450763485084,
      "loss": 0.4948,
      "step": 200
    },
    {
      "epoch": 0.01489097677716717,
      "grad_norm": 1.3470021486282349,
      "learning_rate": 0.00019900723301659342,
      "loss": 0.4646,
      "step": 210
    },
    {
      "epoch": 0.015600070909413224,
      "grad_norm": 1.5810909271240234,
      "learning_rate": 0.00019895995839833594,
      "loss": 0.458,
      "step": 220
    },
    {
      "epoch": 0.01630916504165928,
      "grad_norm": 1.1155333518981934,
      "learning_rate": 0.00019891268378007847,
      "loss": 0.4242,
      "step": 230
    },
    {
      "epoch": 0.017018259173905335,
      "grad_norm": 0.9931291341781616,
      "learning_rate": 0.00019886540916182104,
      "loss": 0.4328,
      "step": 240
    },
    {
      "epoch": 0.01772735330615139,
      "grad_norm": 0.9246334433555603,
      "learning_rate": 0.00019881813454356357,
      "loss": 0.3907,
      "step": 250
    },
    {
      "epoch": 0.01843644743839745,
      "grad_norm": 0.9617415070533752,
      "learning_rate": 0.00019877085992530612,
      "loss": 0.3634,
      "step": 260
    },
    {
      "epoch": 0.019145541570643503,
      "grad_norm": 1.3214004039764404,
      "learning_rate": 0.00019872358530704867,
      "loss": 0.3777,
      "step": 270
    },
    {
      "epoch": 0.019854635702889558,
      "grad_norm": 1.390303373336792,
      "learning_rate": 0.0001986763106887912,
      "loss": 0.4385,
      "step": 280
    },
    {
      "epoch": 0.020563729835135613,
      "grad_norm": 0.7259579300880432,
      "learning_rate": 0.00019862903607053374,
      "loss": 0.4163,
      "step": 290
    },
    {
      "epoch": 0.02127282396738167,
      "grad_norm": 1.6467229127883911,
      "learning_rate": 0.00019858176145227626,
      "loss": 0.4445,
      "step": 300
    },
    {
      "epoch": 0.021981918099627726,
      "grad_norm": 1.1013151407241821,
      "learning_rate": 0.00019853448683401884,
      "loss": 0.4292,
      "step": 310
    },
    {
      "epoch": 0.02269101223187378,
      "grad_norm": 0.9453394412994385,
      "learning_rate": 0.00019848721221576136,
      "loss": 0.4043,
      "step": 320
    },
    {
      "epoch": 0.023400106364119835,
      "grad_norm": 1.3166333436965942,
      "learning_rate": 0.0001984399375975039,
      "loss": 0.3852,
      "step": 330
    },
    {
      "epoch": 0.024109200496365894,
      "grad_norm": 1.397927165031433,
      "learning_rate": 0.00019839266297924646,
      "loss": 0.4175,
      "step": 340
    },
    {
      "epoch": 0.02481829462861195,
      "grad_norm": 1.2934504747390747,
      "learning_rate": 0.00019834538836098898,
      "loss": 0.3851,
      "step": 350
    },
    {
      "epoch": 0.025527388760858003,
      "grad_norm": 1.0651768445968628,
      "learning_rate": 0.00019829811374273153,
      "loss": 0.3866,
      "step": 360
    },
    {
      "epoch": 0.026236482893104058,
      "grad_norm": 1.3693320751190186,
      "learning_rate": 0.00019825083912447408,
      "loss": 0.3585,
      "step": 370
    },
    {
      "epoch": 0.026945577025350116,
      "grad_norm": 1.457266092300415,
      "learning_rate": 0.00019820356450621663,
      "loss": 0.3286,
      "step": 380
    },
    {
      "epoch": 0.02765467115759617,
      "grad_norm": 2.0795555114746094,
      "learning_rate": 0.00019815628988795916,
      "loss": 0.3388,
      "step": 390
    },
    {
      "epoch": 0.028363765289842226,
      "grad_norm": 1.7747998237609863,
      "learning_rate": 0.0001981090152697017,
      "loss": 0.3989,
      "step": 400
    },
    {
      "epoch": 0.02907285942208828,
      "grad_norm": 1.1434431076049805,
      "learning_rate": 0.00019806174065144426,
      "loss": 0.2917,
      "step": 410
    },
    {
      "epoch": 0.02978195355433434,
      "grad_norm": 1.5389207601547241,
      "learning_rate": 0.00019801446603318678,
      "loss": 0.3627,
      "step": 420
    },
    {
      "epoch": 0.030491047686580394,
      "grad_norm": 1.300639271736145,
      "learning_rate": 0.00019796719141492935,
      "loss": 0.3221,
      "step": 430
    },
    {
      "epoch": 0.03120014181882645,
      "grad_norm": 0.7745596766471863,
      "learning_rate": 0.00019791991679667188,
      "loss": 0.3205,
      "step": 440
    },
    {
      "epoch": 0.03190923595107251,
      "grad_norm": 0.8482242226600647,
      "learning_rate": 0.0001978726421784144,
      "loss": 0.341,
      "step": 450
    },
    {
      "epoch": 0.03261833008331856,
      "grad_norm": 0.6732930541038513,
      "learning_rate": 0.00019782536756015698,
      "loss": 0.2864,
      "step": 460
    },
    {
      "epoch": 0.033327424215564616,
      "grad_norm": 1.3267600536346436,
      "learning_rate": 0.0001977780929418995,
      "loss": 0.3425,
      "step": 470
    },
    {
      "epoch": 0.03403651834781067,
      "grad_norm": 1.0371687412261963,
      "learning_rate": 0.00019773081832364205,
      "loss": 0.3123,
      "step": 480
    },
    {
      "epoch": 0.034745612480056726,
      "grad_norm": 1.1350866556167603,
      "learning_rate": 0.0001976835437053846,
      "loss": 0.3227,
      "step": 490
    },
    {
      "epoch": 0.03545470661230278,
      "grad_norm": 0.7760120034217834,
      "learning_rate": 0.00019763626908712712,
      "loss": 0.3423,
      "step": 500
    },
    {
      "epoch": 0.03616380074454884,
      "grad_norm": 0.7728914022445679,
      "learning_rate": 0.00019758899446886967,
      "loss": 0.3077,
      "step": 510
    },
    {
      "epoch": 0.0368728948767949,
      "grad_norm": 1.5368269681930542,
      "learning_rate": 0.0001975417198506122,
      "loss": 0.2937,
      "step": 520
    },
    {
      "epoch": 0.03758198900904095,
      "grad_norm": 0.901556670665741,
      "learning_rate": 0.00019749444523235477,
      "loss": 0.2896,
      "step": 530
    },
    {
      "epoch": 0.038291083141287006,
      "grad_norm": 3.071465492248535,
      "learning_rate": 0.0001974471706140973,
      "loss": 0.3225,
      "step": 540
    },
    {
      "epoch": 0.03900017727353306,
      "grad_norm": 1.8410004377365112,
      "learning_rate": 0.00019739989599583984,
      "loss": 0.3378,
      "step": 550
    },
    {
      "epoch": 0.039709271405779116,
      "grad_norm": 1.2589173316955566,
      "learning_rate": 0.0001973526213775824,
      "loss": 0.3117,
      "step": 560
    },
    {
      "epoch": 0.04041836553802517,
      "grad_norm": 1.1031163930892944,
      "learning_rate": 0.00019730534675932492,
      "loss": 0.3154,
      "step": 570
    },
    {
      "epoch": 0.041127459670271226,
      "grad_norm": 2.0168099403381348,
      "learning_rate": 0.00019725807214106747,
      "loss": 0.2949,
      "step": 580
    },
    {
      "epoch": 0.04183655380251729,
      "grad_norm": 1.1197401285171509,
      "learning_rate": 0.00019721079752281002,
      "loss": 0.2958,
      "step": 590
    },
    {
      "epoch": 0.04254564793476334,
      "grad_norm": 1.4875794649124146,
      "learning_rate": 0.00019716352290455257,
      "loss": 0.3303,
      "step": 600
    },
    {
      "epoch": 0.0432547420670094,
      "grad_norm": 1.7961788177490234,
      "learning_rate": 0.0001971162482862951,
      "loss": 0.3242,
      "step": 610
    },
    {
      "epoch": 0.04396383619925545,
      "grad_norm": 0.6595041155815125,
      "learning_rate": 0.00019706897366803764,
      "loss": 0.3404,
      "step": 620
    },
    {
      "epoch": 0.044672930331501506,
      "grad_norm": 1.2470208406448364,
      "learning_rate": 0.0001970216990497802,
      "loss": 0.3014,
      "step": 630
    },
    {
      "epoch": 0.04538202446374756,
      "grad_norm": 2.831173896789551,
      "learning_rate": 0.0001969744244315227,
      "loss": 0.3152,
      "step": 640
    },
    {
      "epoch": 0.046091118595993616,
      "grad_norm": 1.290031909942627,
      "learning_rate": 0.0001969271498132653,
      "loss": 0.2747,
      "step": 650
    },
    {
      "epoch": 0.04680021272823967,
      "grad_norm": 1.279561161994934,
      "learning_rate": 0.0001968798751950078,
      "loss": 0.298,
      "step": 660
    },
    {
      "epoch": 0.04750930686048573,
      "grad_norm": 2.4023661613464355,
      "learning_rate": 0.00019683260057675036,
      "loss": 0.3074,
      "step": 670
    },
    {
      "epoch": 0.04821840099273179,
      "grad_norm": 1.2128534317016602,
      "learning_rate": 0.0001967853259584929,
      "loss": 0.3098,
      "step": 680
    },
    {
      "epoch": 0.04892749512497784,
      "grad_norm": 1.6055822372436523,
      "learning_rate": 0.00019673805134023543,
      "loss": 0.3075,
      "step": 690
    },
    {
      "epoch": 0.0496365892572239,
      "grad_norm": 1.1544620990753174,
      "learning_rate": 0.00019669077672197798,
      "loss": 0.2926,
      "step": 700
    },
    {
      "epoch": 0.05034568338946995,
      "grad_norm": 1.5919976234436035,
      "learning_rate": 0.0001966435021037205,
      "loss": 0.2611,
      "step": 710
    },
    {
      "epoch": 0.051054777521716006,
      "grad_norm": 1.1348686218261719,
      "learning_rate": 0.00019659622748546306,
      "loss": 0.3125,
      "step": 720
    },
    {
      "epoch": 0.05176387165396206,
      "grad_norm": 1.0918036699295044,
      "learning_rate": 0.0001965489528672056,
      "loss": 0.2611,
      "step": 730
    },
    {
      "epoch": 0.052472965786208116,
      "grad_norm": 0.9908801913261414,
      "learning_rate": 0.00019650167824894813,
      "loss": 0.3051,
      "step": 740
    },
    {
      "epoch": 0.05318205991845418,
      "grad_norm": 2.130890130996704,
      "learning_rate": 0.0001964544036306907,
      "loss": 0.2904,
      "step": 750
    },
    {
      "epoch": 0.05389115405070023,
      "grad_norm": 0.7910012602806091,
      "learning_rate": 0.00019640712901243323,
      "loss": 0.2854,
      "step": 760
    },
    {
      "epoch": 0.05460024818294629,
      "grad_norm": 2.6579387187957764,
      "learning_rate": 0.00019635985439417578,
      "loss": 0.2621,
      "step": 770
    },
    {
      "epoch": 0.05530934231519234,
      "grad_norm": 1.5045433044433594,
      "learning_rate": 0.00019631257977591833,
      "loss": 0.2816,
      "step": 780
    },
    {
      "epoch": 0.0560184364474384,
      "grad_norm": 1.3829929828643799,
      "learning_rate": 0.00019626530515766085,
      "loss": 0.3176,
      "step": 790
    },
    {
      "epoch": 0.05672753057968445,
      "grad_norm": 0.8288218975067139,
      "learning_rate": 0.0001962180305394034,
      "loss": 0.2471,
      "step": 800
    },
    {
      "epoch": 0.057436624711930506,
      "grad_norm": 0.6359084248542786,
      "learning_rate": 0.00019617075592114595,
      "loss": 0.2607,
      "step": 810
    },
    {
      "epoch": 0.05814571884417656,
      "grad_norm": 0.744010329246521,
      "learning_rate": 0.0001961234813028885,
      "loss": 0.2848,
      "step": 820
    },
    {
      "epoch": 0.05885481297642262,
      "grad_norm": 0.8733288049697876,
      "learning_rate": 0.00019607620668463102,
      "loss": 0.3021,
      "step": 830
    },
    {
      "epoch": 0.05956390710866868,
      "grad_norm": 1.5550212860107422,
      "learning_rate": 0.00019602893206637357,
      "loss": 0.2601,
      "step": 840
    },
    {
      "epoch": 0.06027300124091473,
      "grad_norm": 1.2154916524887085,
      "learning_rate": 0.00019598165744811612,
      "loss": 0.3569,
      "step": 850
    },
    {
      "epoch": 0.06098209537316079,
      "grad_norm": 0.6036758422851562,
      "learning_rate": 0.00019593438282985865,
      "loss": 0.295,
      "step": 860
    },
    {
      "epoch": 0.06169118950540684,
      "grad_norm": 1.8986241817474365,
      "learning_rate": 0.00019588710821160122,
      "loss": 0.265,
      "step": 870
    },
    {
      "epoch": 0.0624002836376529,
      "grad_norm": 1.0784766674041748,
      "learning_rate": 0.00019583983359334375,
      "loss": 0.2895,
      "step": 880
    },
    {
      "epoch": 0.06310937776989896,
      "grad_norm": 2.2821438312530518,
      "learning_rate": 0.0001957925589750863,
      "loss": 0.2664,
      "step": 890
    },
    {
      "epoch": 0.06381847190214501,
      "grad_norm": 1.3035733699798584,
      "learning_rate": 0.00019574528435682882,
      "loss": 0.2934,
      "step": 900
    },
    {
      "epoch": 0.06452756603439107,
      "grad_norm": 0.7928126454353333,
      "learning_rate": 0.00019569800973857137,
      "loss": 0.2887,
      "step": 910
    },
    {
      "epoch": 0.06523666016663712,
      "grad_norm": 1.5824174880981445,
      "learning_rate": 0.00019565073512031392,
      "loss": 0.3533,
      "step": 920
    },
    {
      "epoch": 0.06594575429888318,
      "grad_norm": 1.2715685367584229,
      "learning_rate": 0.00019560346050205644,
      "loss": 0.2631,
      "step": 930
    },
    {
      "epoch": 0.06665484843112923,
      "grad_norm": 1.452602505683899,
      "learning_rate": 0.000195556185883799,
      "loss": 0.3121,
      "step": 940
    },
    {
      "epoch": 0.06736394256337529,
      "grad_norm": 1.3192753791809082,
      "learning_rate": 0.00019550891126554154,
      "loss": 0.264,
      "step": 950
    },
    {
      "epoch": 0.06807303669562134,
      "grad_norm": 1.139640212059021,
      "learning_rate": 0.00019546163664728406,
      "loss": 0.3037,
      "step": 960
    },
    {
      "epoch": 0.0687821308278674,
      "grad_norm": 0.879903256893158,
      "learning_rate": 0.00019541436202902664,
      "loss": 0.2675,
      "step": 970
    },
    {
      "epoch": 0.06949122496011345,
      "grad_norm": 1.0006853342056274,
      "learning_rate": 0.00019536708741076916,
      "loss": 0.3037,
      "step": 980
    },
    {
      "epoch": 0.0702003190923595,
      "grad_norm": 0.6497418880462646,
      "learning_rate": 0.0001953198127925117,
      "loss": 0.2947,
      "step": 990
    },
    {
      "epoch": 0.07090941322460556,
      "grad_norm": 1.133432149887085,
      "learning_rate": 0.00019527253817425426,
      "loss": 0.2632,
      "step": 1000
    },
    {
      "epoch": 0.07161850735685162,
      "grad_norm": 1.2833722829818726,
      "learning_rate": 0.00019522526355599679,
      "loss": 0.2899,
      "step": 1010
    },
    {
      "epoch": 0.07232760148909768,
      "grad_norm": 1.4593967199325562,
      "learning_rate": 0.00019517798893773933,
      "loss": 0.2721,
      "step": 1020
    },
    {
      "epoch": 0.07303669562134374,
      "grad_norm": 2.1897354125976562,
      "learning_rate": 0.00019513071431948188,
      "loss": 0.2892,
      "step": 1030
    },
    {
      "epoch": 0.0737457897535898,
      "grad_norm": 1.1114848852157593,
      "learning_rate": 0.00019508343970122443,
      "loss": 0.2668,
      "step": 1040
    },
    {
      "epoch": 0.07445488388583585,
      "grad_norm": 1.7540379762649536,
      "learning_rate": 0.00019503616508296696,
      "loss": 0.2435,
      "step": 1050
    },
    {
      "epoch": 0.0751639780180819,
      "grad_norm": 1.6595946550369263,
      "learning_rate": 0.0001949888904647095,
      "loss": 0.2503,
      "step": 1060
    },
    {
      "epoch": 0.07587307215032796,
      "grad_norm": 1.0685054063796997,
      "learning_rate": 0.00019494161584645206,
      "loss": 0.2629,
      "step": 1070
    },
    {
      "epoch": 0.07658216628257401,
      "grad_norm": 1.1935460567474365,
      "learning_rate": 0.00019489434122819458,
      "loss": 0.298,
      "step": 1080
    },
    {
      "epoch": 0.07729126041482007,
      "grad_norm": 1.1377061605453491,
      "learning_rate": 0.00019484706660993713,
      "loss": 0.3199,
      "step": 1090
    },
    {
      "epoch": 0.07800035454706612,
      "grad_norm": 2.587998151779175,
      "learning_rate": 0.00019479979199167968,
      "loss": 0.2828,
      "step": 1100
    },
    {
      "epoch": 0.07870944867931218,
      "grad_norm": 0.8861609697341919,
      "learning_rate": 0.00019475251737342223,
      "loss": 0.2964,
      "step": 1110
    },
    {
      "epoch": 0.07941854281155823,
      "grad_norm": 1.2256063222885132,
      "learning_rate": 0.00019470524275516475,
      "loss": 0.2704,
      "step": 1120
    },
    {
      "epoch": 0.08012763694380429,
      "grad_norm": 2.1910200119018555,
      "learning_rate": 0.0001946579681369073,
      "loss": 0.3111,
      "step": 1130
    },
    {
      "epoch": 0.08083673107605034,
      "grad_norm": 0.8197041153907776,
      "learning_rate": 0.00019461069351864985,
      "loss": 0.2718,
      "step": 1140
    },
    {
      "epoch": 0.0815458252082964,
      "grad_norm": 1.9719947576522827,
      "learning_rate": 0.00019456341890039237,
      "loss": 0.2913,
      "step": 1150
    },
    {
      "epoch": 0.08225491934054245,
      "grad_norm": 1.1873505115509033,
      "learning_rate": 0.00019451614428213492,
      "loss": 0.2684,
      "step": 1160
    },
    {
      "epoch": 0.0829640134727885,
      "grad_norm": 0.6588963270187378,
      "learning_rate": 0.00019446886966387747,
      "loss": 0.2627,
      "step": 1170
    },
    {
      "epoch": 0.08367310760503457,
      "grad_norm": 1.7660906314849854,
      "learning_rate": 0.00019442159504562,
      "loss": 0.257,
      "step": 1180
    },
    {
      "epoch": 0.08438220173728063,
      "grad_norm": 1.6933649778366089,
      "learning_rate": 0.00019437432042736257,
      "loss": 0.2815,
      "step": 1190
    },
    {
      "epoch": 0.08509129586952668,
      "grad_norm": 1.642093300819397,
      "learning_rate": 0.0001943270458091051,
      "loss": 0.2741,
      "step": 1200
    },
    {
      "epoch": 0.08580039000177274,
      "grad_norm": 0.7710999846458435,
      "learning_rate": 0.00019427977119084765,
      "loss": 0.346,
      "step": 1210
    },
    {
      "epoch": 0.0865094841340188,
      "grad_norm": 0.698076069355011,
      "learning_rate": 0.0001942324965725902,
      "loss": 0.2932,
      "step": 1220
    },
    {
      "epoch": 0.08721857826626485,
      "grad_norm": 0.9502787590026855,
      "learning_rate": 0.00019418522195433272,
      "loss": 0.2498,
      "step": 1230
    },
    {
      "epoch": 0.0879276723985109,
      "grad_norm": 1.9796011447906494,
      "learning_rate": 0.00019413794733607527,
      "loss": 0.2854,
      "step": 1240
    },
    {
      "epoch": 0.08863676653075696,
      "grad_norm": 0.7409440875053406,
      "learning_rate": 0.00019409067271781782,
      "loss": 0.2719,
      "step": 1250
    },
    {
      "epoch": 0.08934586066300301,
      "grad_norm": 0.7769677042961121,
      "learning_rate": 0.00019404339809956037,
      "loss": 0.2798,
      "step": 1260
    },
    {
      "epoch": 0.09005495479524907,
      "grad_norm": 1.2055917978286743,
      "learning_rate": 0.0001939961234813029,
      "loss": 0.2764,
      "step": 1270
    },
    {
      "epoch": 0.09076404892749512,
      "grad_norm": 1.048652172088623,
      "learning_rate": 0.00019394884886304544,
      "loss": 0.2783,
      "step": 1280
    },
    {
      "epoch": 0.09147314305974118,
      "grad_norm": 1.2595043182373047,
      "learning_rate": 0.000193901574244788,
      "loss": 0.2609,
      "step": 1290
    },
    {
      "epoch": 0.09218223719198723,
      "grad_norm": 1.0531580448150635,
      "learning_rate": 0.00019385429962653051,
      "loss": 0.2815,
      "step": 1300
    },
    {
      "epoch": 0.09289133132423329,
      "grad_norm": 0.7150720953941345,
      "learning_rate": 0.00019380702500827306,
      "loss": 0.2515,
      "step": 1310
    },
    {
      "epoch": 0.09360042545647934,
      "grad_norm": 1.3996539115905762,
      "learning_rate": 0.0001937597503900156,
      "loss": 0.274,
      "step": 1320
    },
    {
      "epoch": 0.09430951958872541,
      "grad_norm": 0.6486003398895264,
      "learning_rate": 0.00019371247577175816,
      "loss": 0.2471,
      "step": 1330
    },
    {
      "epoch": 0.09501861372097146,
      "grad_norm": 1.0747305154800415,
      "learning_rate": 0.00019366520115350069,
      "loss": 0.2388,
      "step": 1340
    },
    {
      "epoch": 0.09572770785321752,
      "grad_norm": 1.175173282623291,
      "learning_rate": 0.00019361792653524324,
      "loss": 0.273,
      "step": 1350
    },
    {
      "epoch": 0.09643680198546357,
      "grad_norm": 1.78445303440094,
      "learning_rate": 0.00019357065191698579,
      "loss": 0.2849,
      "step": 1360
    },
    {
      "epoch": 0.09714589611770963,
      "grad_norm": 0.9592330455780029,
      "learning_rate": 0.0001935233772987283,
      "loss": 0.2597,
      "step": 1370
    },
    {
      "epoch": 0.09785499024995568,
      "grad_norm": 0.8844307661056519,
      "learning_rate": 0.00019347610268047089,
      "loss": 0.277,
      "step": 1380
    },
    {
      "epoch": 0.09856408438220174,
      "grad_norm": 1.2234432697296143,
      "learning_rate": 0.0001934288280622134,
      "loss": 0.2588,
      "step": 1390
    },
    {
      "epoch": 0.0992731785144478,
      "grad_norm": 0.9837889671325684,
      "learning_rate": 0.00019338155344395593,
      "loss": 0.2613,
      "step": 1400
    },
    {
      "epoch": 0.09998227264669385,
      "grad_norm": 0.6992933750152588,
      "learning_rate": 0.0001933342788256985,
      "loss": 0.27,
      "step": 1410
    },
    {
      "epoch": 0.1006913667789399,
      "grad_norm": 1.057494878768921,
      "learning_rate": 0.00019328700420744103,
      "loss": 0.2594,
      "step": 1420
    },
    {
      "epoch": 0.10140046091118596,
      "grad_norm": 0.8464921712875366,
      "learning_rate": 0.00019323972958918358,
      "loss": 0.2765,
      "step": 1430
    },
    {
      "epoch": 0.10210955504343201,
      "grad_norm": 1.5695501565933228,
      "learning_rate": 0.00019319245497092613,
      "loss": 0.2806,
      "step": 1440
    },
    {
      "epoch": 0.10281864917567807,
      "grad_norm": 1.671340823173523,
      "learning_rate": 0.00019314518035266865,
      "loss": 0.2789,
      "step": 1450
    },
    {
      "epoch": 0.10352774330792412,
      "grad_norm": 0.9877613186836243,
      "learning_rate": 0.0001930979057344112,
      "loss": 0.2493,
      "step": 1460
    },
    {
      "epoch": 0.10423683744017018,
      "grad_norm": 0.6798184514045715,
      "learning_rate": 0.00019305063111615375,
      "loss": 0.2724,
      "step": 1470
    },
    {
      "epoch": 0.10494593157241623,
      "grad_norm": 0.866227924823761,
      "learning_rate": 0.0001930033564978963,
      "loss": 0.3224,
      "step": 1480
    },
    {
      "epoch": 0.1056550257046623,
      "grad_norm": 1.2459803819656372,
      "learning_rate": 0.00019295608187963883,
      "loss": 0.2617,
      "step": 1490
    },
    {
      "epoch": 0.10636411983690836,
      "grad_norm": 2.1125783920288086,
      "learning_rate": 0.00019290880726138138,
      "loss": 0.2676,
      "step": 1500
    },
    {
      "epoch": 0.10707321396915441,
      "grad_norm": 1.5917508602142334,
      "learning_rate": 0.00019286153264312393,
      "loss": 0.305,
      "step": 1510
    },
    {
      "epoch": 0.10778230810140046,
      "grad_norm": 2.703559398651123,
      "learning_rate": 0.00019281425802486645,
      "loss": 0.2577,
      "step": 1520
    },
    {
      "epoch": 0.10849140223364652,
      "grad_norm": 0.7905508279800415,
      "learning_rate": 0.000192766983406609,
      "loss": 0.2441,
      "step": 1530
    },
    {
      "epoch": 0.10920049636589257,
      "grad_norm": 0.9939122796058655,
      "learning_rate": 0.00019271970878835155,
      "loss": 0.2625,
      "step": 1540
    },
    {
      "epoch": 0.10990959049813863,
      "grad_norm": 1.3608746528625488,
      "learning_rate": 0.0001926724341700941,
      "loss": 0.2662,
      "step": 1550
    },
    {
      "epoch": 0.11061868463038468,
      "grad_norm": 1.2437385320663452,
      "learning_rate": 0.00019262515955183662,
      "loss": 0.2506,
      "step": 1560
    },
    {
      "epoch": 0.11132777876263074,
      "grad_norm": 2.213831901550293,
      "learning_rate": 0.00019257788493357917,
      "loss": 0.2579,
      "step": 1570
    },
    {
      "epoch": 0.1120368728948768,
      "grad_norm": 1.1146856546401978,
      "learning_rate": 0.00019253061031532172,
      "loss": 0.2518,
      "step": 1580
    },
    {
      "epoch": 0.11274596702712285,
      "grad_norm": 1.0240142345428467,
      "learning_rate": 0.00019248333569706424,
      "loss": 0.2735,
      "step": 1590
    },
    {
      "epoch": 0.1134550611593689,
      "grad_norm": 1.3948067426681519,
      "learning_rate": 0.00019243606107880682,
      "loss": 0.2716,
      "step": 1600
    },
    {
      "epoch": 0.11416415529161496,
      "grad_norm": 1.1953983306884766,
      "learning_rate": 0.00019238878646054934,
      "loss": 0.2502,
      "step": 1610
    },
    {
      "epoch": 0.11487324942386101,
      "grad_norm": 1.0285197496414185,
      "learning_rate": 0.00019234151184229186,
      "loss": 0.2549,
      "step": 1620
    },
    {
      "epoch": 0.11558234355610707,
      "grad_norm": 0.7585241794586182,
      "learning_rate": 0.00019229423722403444,
      "loss": 0.3482,
      "step": 1630
    },
    {
      "epoch": 0.11629143768835312,
      "grad_norm": 3.7883896827697754,
      "learning_rate": 0.00019224696260577696,
      "loss": 0.2368,
      "step": 1640
    },
    {
      "epoch": 0.11700053182059919,
      "grad_norm": 1.1329237222671509,
      "learning_rate": 0.00019219968798751951,
      "loss": 0.2674,
      "step": 1650
    },
    {
      "epoch": 0.11770962595284525,
      "grad_norm": 1.8846967220306396,
      "learning_rate": 0.00019215241336926206,
      "loss": 0.2776,
      "step": 1660
    },
    {
      "epoch": 0.1184187200850913,
      "grad_norm": 1.6655303239822388,
      "learning_rate": 0.0001921051387510046,
      "loss": 0.264,
      "step": 1670
    },
    {
      "epoch": 0.11912781421733735,
      "grad_norm": 1.5933881998062134,
      "learning_rate": 0.00019205786413274714,
      "loss": 0.2927,
      "step": 1680
    },
    {
      "epoch": 0.11983690834958341,
      "grad_norm": 1.7303941249847412,
      "learning_rate": 0.00019201058951448966,
      "loss": 0.3105,
      "step": 1690
    },
    {
      "epoch": 0.12054600248182946,
      "grad_norm": 2.589449167251587,
      "learning_rate": 0.00019196331489623224,
      "loss": 0.2587,
      "step": 1700
    },
    {
      "epoch": 0.12125509661407552,
      "grad_norm": 1.6207760572433472,
      "learning_rate": 0.00019191604027797476,
      "loss": 0.2631,
      "step": 1710
    },
    {
      "epoch": 0.12196419074632157,
      "grad_norm": 1.29201340675354,
      "learning_rate": 0.0001918687656597173,
      "loss": 0.2629,
      "step": 1720
    },
    {
      "epoch": 0.12267328487856763,
      "grad_norm": 0.7323301434516907,
      "learning_rate": 0.00019182149104145986,
      "loss": 0.2695,
      "step": 1730
    },
    {
      "epoch": 0.12338237901081368,
      "grad_norm": 1.4293895959854126,
      "learning_rate": 0.00019177421642320238,
      "loss": 0.2534,
      "step": 1740
    },
    {
      "epoch": 0.12409147314305974,
      "grad_norm": 0.8188483715057373,
      "learning_rate": 0.00019172694180494493,
      "loss": 0.2496,
      "step": 1750
    },
    {
      "epoch": 0.1248005672753058,
      "grad_norm": 0.8277511596679688,
      "learning_rate": 0.00019167966718668748,
      "loss": 0.2752,
      "step": 1760
    },
    {
      "epoch": 0.12550966140755185,
      "grad_norm": 0.5749657154083252,
      "learning_rate": 0.00019163239256843003,
      "loss": 0.2591,
      "step": 1770
    },
    {
      "epoch": 0.12621875553979792,
      "grad_norm": 1.720327377319336,
      "learning_rate": 0.00019158511795017255,
      "loss": 0.2656,
      "step": 1780
    },
    {
      "epoch": 0.12692784967204396,
      "grad_norm": 0.5802816152572632,
      "learning_rate": 0.0001915378433319151,
      "loss": 0.2851,
      "step": 1790
    },
    {
      "epoch": 0.12763694380429003,
      "grad_norm": 0.9241579174995422,
      "learning_rate": 0.00019149056871365765,
      "loss": 0.2991,
      "step": 1800
    },
    {
      "epoch": 0.12834603793653607,
      "grad_norm": 1.1456042528152466,
      "learning_rate": 0.00019144329409540018,
      "loss": 0.2758,
      "step": 1810
    },
    {
      "epoch": 0.12905513206878214,
      "grad_norm": 1.2688820362091064,
      "learning_rate": 0.00019139601947714275,
      "loss": 0.2956,
      "step": 1820
    },
    {
      "epoch": 0.12976422620102818,
      "grad_norm": 1.7101361751556396,
      "learning_rate": 0.00019134874485888528,
      "loss": 0.2851,
      "step": 1830
    },
    {
      "epoch": 0.13047332033327425,
      "grad_norm": 2.5800559520721436,
      "learning_rate": 0.0001913014702406278,
      "loss": 0.2979,
      "step": 1840
    },
    {
      "epoch": 0.1311824144655203,
      "grad_norm": 1.072624921798706,
      "learning_rate": 0.00019125419562237038,
      "loss": 0.2905,
      "step": 1850
    },
    {
      "epoch": 0.13189150859776635,
      "grad_norm": 1.9011839628219604,
      "learning_rate": 0.0001912069210041129,
      "loss": 0.2396,
      "step": 1860
    },
    {
      "epoch": 0.1326006027300124,
      "grad_norm": 1.314944863319397,
      "learning_rate": 0.00019115964638585545,
      "loss": 0.3007,
      "step": 1870
    },
    {
      "epoch": 0.13330969686225846,
      "grad_norm": 0.8556939959526062,
      "learning_rate": 0.00019111237176759797,
      "loss": 0.31,
      "step": 1880
    },
    {
      "epoch": 0.13401879099450453,
      "grad_norm": 3.053788900375366,
      "learning_rate": 0.00019106509714934052,
      "loss": 0.2769,
      "step": 1890
    },
    {
      "epoch": 0.13472788512675057,
      "grad_norm": 1.6534894704818726,
      "learning_rate": 0.00019101782253108307,
      "loss": 0.2801,
      "step": 1900
    },
    {
      "epoch": 0.13543697925899664,
      "grad_norm": 1.4080369472503662,
      "learning_rate": 0.0001909705479128256,
      "loss": 0.2754,
      "step": 1910
    },
    {
      "epoch": 0.13614607339124268,
      "grad_norm": 0.8427908420562744,
      "learning_rate": 0.00019092327329456817,
      "loss": 0.2573,
      "step": 1920
    },
    {
      "epoch": 0.13685516752348875,
      "grad_norm": 1.1091351509094238,
      "learning_rate": 0.0001908759986763107,
      "loss": 0.3079,
      "step": 1930
    },
    {
      "epoch": 0.1375642616557348,
      "grad_norm": 1.2822812795639038,
      "learning_rate": 0.00019082872405805324,
      "loss": 0.257,
      "step": 1940
    },
    {
      "epoch": 0.13827335578798086,
      "grad_norm": 1.3184417486190796,
      "learning_rate": 0.0001907814494397958,
      "loss": 0.2212,
      "step": 1950
    },
    {
      "epoch": 0.1389824499202269,
      "grad_norm": 0.865830659866333,
      "learning_rate": 0.00019073417482153832,
      "loss": 0.2447,
      "step": 1960
    },
    {
      "epoch": 0.13969154405247297,
      "grad_norm": 0.8691566586494446,
      "learning_rate": 0.00019068690020328087,
      "loss": 0.2955,
      "step": 1970
    },
    {
      "epoch": 0.140400638184719,
      "grad_norm": 1.0053280591964722,
      "learning_rate": 0.00019063962558502342,
      "loss": 0.2834,
      "step": 1980
    },
    {
      "epoch": 0.14110973231696508,
      "grad_norm": 0.7840780019760132,
      "learning_rate": 0.00019059235096676597,
      "loss": 0.2664,
      "step": 1990
    },
    {
      "epoch": 0.14181882644921112,
      "grad_norm": 0.6317345499992371,
      "learning_rate": 0.0001905450763485085,
      "loss": 0.2483,
      "step": 2000
    },
    {
      "epoch": 0.1425279205814572,
      "grad_norm": 0.4940699338912964,
      "learning_rate": 0.00019049780173025104,
      "loss": 0.2337,
      "step": 2010
    },
    {
      "epoch": 0.14323701471370323,
      "grad_norm": 0.775048553943634,
      "learning_rate": 0.0001904505271119936,
      "loss": 0.251,
      "step": 2020
    },
    {
      "epoch": 0.1439461088459493,
      "grad_norm": 0.7207909226417542,
      "learning_rate": 0.0001904032524937361,
      "loss": 0.2745,
      "step": 2030
    },
    {
      "epoch": 0.14465520297819537,
      "grad_norm": 1.693779468536377,
      "learning_rate": 0.0001903559778754787,
      "loss": 0.2649,
      "step": 2040
    },
    {
      "epoch": 0.1453642971104414,
      "grad_norm": 1.4893012046813965,
      "learning_rate": 0.0001903087032572212,
      "loss": 0.3061,
      "step": 2050
    },
    {
      "epoch": 0.14607339124268748,
      "grad_norm": 0.7390331029891968,
      "learning_rate": 0.00019026142863896373,
      "loss": 0.2711,
      "step": 2060
    },
    {
      "epoch": 0.14678248537493352,
      "grad_norm": 1.2441099882125854,
      "learning_rate": 0.00019021415402070628,
      "loss": 0.3155,
      "step": 2070
    },
    {
      "epoch": 0.1474915795071796,
      "grad_norm": 0.8691495060920715,
      "learning_rate": 0.00019016687940244883,
      "loss": 0.262,
      "step": 2080
    },
    {
      "epoch": 0.14820067363942563,
      "grad_norm": 1.3639897108078003,
      "learning_rate": 0.00019011960478419138,
      "loss": 0.2615,
      "step": 2090
    },
    {
      "epoch": 0.1489097677716717,
      "grad_norm": 0.6307128667831421,
      "learning_rate": 0.0001900723301659339,
      "loss": 0.2476,
      "step": 2100
    },
    {
      "epoch": 0.14961886190391774,
      "grad_norm": 1.3040242195129395,
      "learning_rate": 0.00019002505554767645,
      "loss": 0.2459,
      "step": 2110
    },
    {
      "epoch": 0.1503279560361638,
      "grad_norm": 0.9490776062011719,
      "learning_rate": 0.000189977780929419,
      "loss": 0.2536,
      "step": 2120
    },
    {
      "epoch": 0.15103705016840985,
      "grad_norm": 1.1302683353424072,
      "learning_rate": 0.00018993050631116153,
      "loss": 0.2698,
      "step": 2130
    },
    {
      "epoch": 0.15174614430065592,
      "grad_norm": 0.6912137866020203,
      "learning_rate": 0.0001898832316929041,
      "loss": 0.2547,
      "step": 2140
    },
    {
      "epoch": 0.15245523843290196,
      "grad_norm": 1.186445713043213,
      "learning_rate": 0.00018983595707464663,
      "loss": 0.2625,
      "step": 2150
    },
    {
      "epoch": 0.15316433256514803,
      "grad_norm": 1.020560383796692,
      "learning_rate": 0.00018978868245638918,
      "loss": 0.2839,
      "step": 2160
    },
    {
      "epoch": 0.15387342669739407,
      "grad_norm": 1.7530548572540283,
      "learning_rate": 0.00018974140783813173,
      "loss": 0.2631,
      "step": 2170
    },
    {
      "epoch": 0.15458252082964014,
      "grad_norm": 0.6586669087409973,
      "learning_rate": 0.00018969413321987425,
      "loss": 0.2528,
      "step": 2180
    },
    {
      "epoch": 0.1552916149618862,
      "grad_norm": 3.0322751998901367,
      "learning_rate": 0.0001896468586016168,
      "loss": 0.315,
      "step": 2190
    },
    {
      "epoch": 0.15600070909413224,
      "grad_norm": 0.8461542725563049,
      "learning_rate": 0.00018959958398335935,
      "loss": 0.2716,
      "step": 2200
    },
    {
      "epoch": 0.1567098032263783,
      "grad_norm": 1.1469186544418335,
      "learning_rate": 0.0001895523093651019,
      "loss": 0.2601,
      "step": 2210
    },
    {
      "epoch": 0.15741889735862435,
      "grad_norm": 1.5030549764633179,
      "learning_rate": 0.00018950503474684442,
      "loss": 0.2703,
      "step": 2220
    },
    {
      "epoch": 0.15812799149087042,
      "grad_norm": 0.6703240275382996,
      "learning_rate": 0.00018945776012858697,
      "loss": 0.2523,
      "step": 2230
    },
    {
      "epoch": 0.15883708562311646,
      "grad_norm": 0.9909358024597168,
      "learning_rate": 0.00018941048551032952,
      "loss": 0.28,
      "step": 2240
    },
    {
      "epoch": 0.15954617975536253,
      "grad_norm": 1.7155457735061646,
      "learning_rate": 0.00018936321089207204,
      "loss": 0.2687,
      "step": 2250
    },
    {
      "epoch": 0.16025527388760857,
      "grad_norm": 1.409775733947754,
      "learning_rate": 0.0001893159362738146,
      "loss": 0.2806,
      "step": 2260
    },
    {
      "epoch": 0.16096436801985464,
      "grad_norm": 2.051780939102173,
      "learning_rate": 0.00018926866165555714,
      "loss": 0.2566,
      "step": 2270
    },
    {
      "epoch": 0.16167346215210068,
      "grad_norm": 1.0114407539367676,
      "learning_rate": 0.00018922138703729967,
      "loss": 0.2794,
      "step": 2280
    },
    {
      "epoch": 0.16238255628434675,
      "grad_norm": 0.6785348057746887,
      "learning_rate": 0.00018917411241904222,
      "loss": 0.2751,
      "step": 2290
    },
    {
      "epoch": 0.1630916504165928,
      "grad_norm": 1.869268774986267,
      "learning_rate": 0.00018912683780078477,
      "loss": 0.2592,
      "step": 2300
    },
    {
      "epoch": 0.16380074454883886,
      "grad_norm": 1.8561577796936035,
      "learning_rate": 0.00018907956318252732,
      "loss": 0.2693,
      "step": 2310
    },
    {
      "epoch": 0.1645098386810849,
      "grad_norm": 0.7551804780960083,
      "learning_rate": 0.00018903228856426984,
      "loss": 0.2479,
      "step": 2320
    },
    {
      "epoch": 0.16521893281333097,
      "grad_norm": 2.3948168754577637,
      "learning_rate": 0.0001889850139460124,
      "loss": 0.2641,
      "step": 2330
    },
    {
      "epoch": 0.165928026945577,
      "grad_norm": 1.199001669883728,
      "learning_rate": 0.00018893773932775494,
      "loss": 0.264,
      "step": 2340
    },
    {
      "epoch": 0.16663712107782308,
      "grad_norm": 0.874065637588501,
      "learning_rate": 0.00018889046470949746,
      "loss": 0.2659,
      "step": 2350
    },
    {
      "epoch": 0.16734621521006915,
      "grad_norm": 2.3602943420410156,
      "learning_rate": 0.00018884319009124004,
      "loss": 0.2463,
      "step": 2360
    },
    {
      "epoch": 0.1680553093423152,
      "grad_norm": 1.022355556488037,
      "learning_rate": 0.00018879591547298256,
      "loss": 0.2505,
      "step": 2370
    },
    {
      "epoch": 0.16876440347456126,
      "grad_norm": 1.7378098964691162,
      "learning_rate": 0.0001887486408547251,
      "loss": 0.2842,
      "step": 2380
    },
    {
      "epoch": 0.1694734976068073,
      "grad_norm": 0.874998152256012,
      "learning_rate": 0.00018870136623646766,
      "loss": 0.2425,
      "step": 2390
    },
    {
      "epoch": 0.17018259173905337,
      "grad_norm": 2.5533130168914795,
      "learning_rate": 0.00018865409161821018,
      "loss": 0.2701,
      "step": 2400
    },
    {
      "epoch": 0.1708916858712994,
      "grad_norm": 1.494472622871399,
      "learning_rate": 0.00018860681699995273,
      "loss": 0.2632,
      "step": 2410
    },
    {
      "epoch": 0.17160078000354548,
      "grad_norm": 0.8966007828712463,
      "learning_rate": 0.00018855954238169528,
      "loss": 0.2582,
      "step": 2420
    },
    {
      "epoch": 0.17230987413579152,
      "grad_norm": 2.3985342979431152,
      "learning_rate": 0.00018851226776343783,
      "loss": 0.2934,
      "step": 2430
    },
    {
      "epoch": 0.1730189682680376,
      "grad_norm": 1.9367870092391968,
      "learning_rate": 0.00018846499314518036,
      "loss": 0.2497,
      "step": 2440
    },
    {
      "epoch": 0.17372806240028363,
      "grad_norm": 1.0653109550476074,
      "learning_rate": 0.0001884177185269229,
      "loss": 0.2578,
      "step": 2450
    },
    {
      "epoch": 0.1744371565325297,
      "grad_norm": 0.8647519946098328,
      "learning_rate": 0.00018837044390866546,
      "loss": 0.2396,
      "step": 2460
    },
    {
      "epoch": 0.17514625066477574,
      "grad_norm": 0.6560046672821045,
      "learning_rate": 0.00018832316929040798,
      "loss": 0.2502,
      "step": 2470
    },
    {
      "epoch": 0.1758553447970218,
      "grad_norm": 0.697385311126709,
      "learning_rate": 0.00018827589467215053,
      "loss": 0.2713,
      "step": 2480
    },
    {
      "epoch": 0.17656443892926785,
      "grad_norm": 1.2434662580490112,
      "learning_rate": 0.00018822862005389308,
      "loss": 0.2682,
      "step": 2490
    },
    {
      "epoch": 0.17727353306151392,
      "grad_norm": 0.7549688220024109,
      "learning_rate": 0.0001881813454356356,
      "loss": 0.2252,
      "step": 2500
    },
    {
      "epoch": 0.17798262719375998,
      "grad_norm": 0.8602234125137329,
      "learning_rate": 0.00018813407081737815,
      "loss": 0.266,
      "step": 2510
    },
    {
      "epoch": 0.17869172132600603,
      "grad_norm": 1.057240605354309,
      "learning_rate": 0.0001880867961991207,
      "loss": 0.279,
      "step": 2520
    },
    {
      "epoch": 0.1794008154582521,
      "grad_norm": 1.187013864517212,
      "learning_rate": 0.00018803952158086325,
      "loss": 0.2544,
      "step": 2530
    },
    {
      "epoch": 0.18010990959049814,
      "grad_norm": 0.6001466512680054,
      "learning_rate": 0.00018799224696260577,
      "loss": 0.2828,
      "step": 2540
    },
    {
      "epoch": 0.1808190037227442,
      "grad_norm": 0.7250998616218567,
      "learning_rate": 0.00018794497234434832,
      "loss": 0.2738,
      "step": 2550
    },
    {
      "epoch": 0.18152809785499024,
      "grad_norm": 2.3079469203948975,
      "learning_rate": 0.00018789769772609087,
      "loss": 0.2792,
      "step": 2560
    },
    {
      "epoch": 0.1822371919872363,
      "grad_norm": 1.350137710571289,
      "learning_rate": 0.0001878504231078334,
      "loss": 0.2376,
      "step": 2570
    },
    {
      "epoch": 0.18294628611948235,
      "grad_norm": 0.7468216419219971,
      "learning_rate": 0.00018780314848957597,
      "loss": 0.2698,
      "step": 2580
    },
    {
      "epoch": 0.18365538025172842,
      "grad_norm": 2.434629201889038,
      "learning_rate": 0.0001877558738713185,
      "loss": 0.2439,
      "step": 2590
    },
    {
      "epoch": 0.18436447438397446,
      "grad_norm": 1.3849151134490967,
      "learning_rate": 0.00018770859925306105,
      "loss": 0.2307,
      "step": 2600
    },
    {
      "epoch": 0.18507356851622053,
      "grad_norm": 1.0093735456466675,
      "learning_rate": 0.0001876613246348036,
      "loss": 0.2638,
      "step": 2610
    },
    {
      "epoch": 0.18578266264846657,
      "grad_norm": 0.9296581745147705,
      "learning_rate": 0.00018761405001654612,
      "loss": 0.2669,
      "step": 2620
    },
    {
      "epoch": 0.18649175678071264,
      "grad_norm": 1.2387136220932007,
      "learning_rate": 0.00018756677539828867,
      "loss": 0.3171,
      "step": 2630
    },
    {
      "epoch": 0.18720085091295868,
      "grad_norm": 1.8796069622039795,
      "learning_rate": 0.00018751950078003122,
      "loss": 0.2754,
      "step": 2640
    },
    {
      "epoch": 0.18790994504520475,
      "grad_norm": 2.1438047885894775,
      "learning_rate": 0.00018747222616177377,
      "loss": 0.2808,
      "step": 2650
    },
    {
      "epoch": 0.18861903917745082,
      "grad_norm": 4.811794281005859,
      "learning_rate": 0.0001874249515435163,
      "loss": 0.2929,
      "step": 2660
    },
    {
      "epoch": 0.18932813330969686,
      "grad_norm": 1.5412636995315552,
      "learning_rate": 0.00018737767692525884,
      "loss": 0.2815,
      "step": 2670
    },
    {
      "epoch": 0.19003722744194293,
      "grad_norm": 1.3918386697769165,
      "learning_rate": 0.0001873304023070014,
      "loss": 0.3001,
      "step": 2680
    },
    {
      "epoch": 0.19074632157418897,
      "grad_norm": 0.9082643389701843,
      "learning_rate": 0.0001872831276887439,
      "loss": 0.3144,
      "step": 2690
    },
    {
      "epoch": 0.19145541570643504,
      "grad_norm": 0.9718620181083679,
      "learning_rate": 0.00018723585307048646,
      "loss": 0.2681,
      "step": 2700
    },
    {
      "epoch": 0.19216450983868108,
      "grad_norm": 0.8356815576553345,
      "learning_rate": 0.000187188578452229,
      "loss": 0.241,
      "step": 2710
    },
    {
      "epoch": 0.19287360397092715,
      "grad_norm": 2.682049036026001,
      "learning_rate": 0.00018714130383397156,
      "loss": 0.2784,
      "step": 2720
    },
    {
      "epoch": 0.1935826981031732,
      "grad_norm": 1.2357947826385498,
      "learning_rate": 0.00018709402921571408,
      "loss": 0.2837,
      "step": 2730
    },
    {
      "epoch": 0.19429179223541926,
      "grad_norm": 0.818461000919342,
      "learning_rate": 0.00018704675459745663,
      "loss": 0.2569,
      "step": 2740
    },
    {
      "epoch": 0.1950008863676653,
      "grad_norm": 1.2226225137710571,
      "learning_rate": 0.00018699947997919918,
      "loss": 0.2465,
      "step": 2750
    },
    {
      "epoch": 0.19570998049991137,
      "grad_norm": 1.4628353118896484,
      "learning_rate": 0.0001869522053609417,
      "loss": 0.2555,
      "step": 2760
    },
    {
      "epoch": 0.1964190746321574,
      "grad_norm": 0.7720316648483276,
      "learning_rate": 0.00018690493074268426,
      "loss": 0.2573,
      "step": 2770
    },
    {
      "epoch": 0.19712816876440348,
      "grad_norm": 0.6387788653373718,
      "learning_rate": 0.0001868576561244268,
      "loss": 0.2568,
      "step": 2780
    },
    {
      "epoch": 0.19783726289664952,
      "grad_norm": 0.4949035346508026,
      "learning_rate": 0.00018681038150616933,
      "loss": 0.2744,
      "step": 2790
    },
    {
      "epoch": 0.1985463570288956,
      "grad_norm": 1.2327134609222412,
      "learning_rate": 0.0001867631068879119,
      "loss": 0.3048,
      "step": 2800
    },
    {
      "epoch": 0.19925545116114163,
      "grad_norm": 0.8336178064346313,
      "learning_rate": 0.00018671583226965443,
      "loss": 0.2373,
      "step": 2810
    },
    {
      "epoch": 0.1999645452933877,
      "grad_norm": 0.6447906494140625,
      "learning_rate": 0.00018666855765139698,
      "loss": 0.2348,
      "step": 2820
    },
    {
      "epoch": 0.20067363942563377,
      "grad_norm": 0.551474392414093,
      "learning_rate": 0.00018662128303313953,
      "loss": 0.2703,
      "step": 2830
    },
    {
      "epoch": 0.2013827335578798,
      "grad_norm": 0.6283109188079834,
      "learning_rate": 0.00018657400841488205,
      "loss": 0.2608,
      "step": 2840
    },
    {
      "epoch": 0.20209182769012587,
      "grad_norm": 0.9415434002876282,
      "learning_rate": 0.0001865267337966246,
      "loss": 0.2943,
      "step": 2850
    },
    {
      "epoch": 0.20280092182237192,
      "grad_norm": 0.9555026888847351,
      "learning_rate": 0.00018647945917836712,
      "loss": 0.2647,
      "step": 2860
    },
    {
      "epoch": 0.20351001595461798,
      "grad_norm": 1.244555950164795,
      "learning_rate": 0.0001864321845601097,
      "loss": 0.2645,
      "step": 2870
    },
    {
      "epoch": 0.20421911008686403,
      "grad_norm": 0.8582156300544739,
      "learning_rate": 0.00018638490994185222,
      "loss": 0.247,
      "step": 2880
    },
    {
      "epoch": 0.2049282042191101,
      "grad_norm": 0.6308354735374451,
      "learning_rate": 0.00018633763532359477,
      "loss": 0.2513,
      "step": 2890
    },
    {
      "epoch": 0.20563729835135613,
      "grad_norm": 0.6052433848381042,
      "learning_rate": 0.00018629036070533732,
      "loss": 0.2716,
      "step": 2900
    },
    {
      "epoch": 0.2063463924836022,
      "grad_norm": 2.0591466426849365,
      "learning_rate": 0.00018624308608707985,
      "loss": 0.2685,
      "step": 2910
    },
    {
      "epoch": 0.20705548661584824,
      "grad_norm": 0.6965278387069702,
      "learning_rate": 0.0001861958114688224,
      "loss": 0.2317,
      "step": 2920
    },
    {
      "epoch": 0.2077645807480943,
      "grad_norm": 1.0373324155807495,
      "learning_rate": 0.00018614853685056495,
      "loss": 0.2586,
      "step": 2930
    },
    {
      "epoch": 0.20847367488034035,
      "grad_norm": 0.6134164333343506,
      "learning_rate": 0.0001861012622323075,
      "loss": 0.2866,
      "step": 2940
    },
    {
      "epoch": 0.20918276901258642,
      "grad_norm": 1.7072770595550537,
      "learning_rate": 0.00018605398761405002,
      "loss": 0.2412,
      "step": 2950
    },
    {
      "epoch": 0.20989186314483246,
      "grad_norm": 2.672445297241211,
      "learning_rate": 0.00018600671299579257,
      "loss": 0.2581,
      "step": 2960
    },
    {
      "epoch": 0.21060095727707853,
      "grad_norm": 0.7742972373962402,
      "learning_rate": 0.00018595943837753512,
      "loss": 0.2443,
      "step": 2970
    },
    {
      "epoch": 0.2113100514093246,
      "grad_norm": 1.0225462913513184,
      "learning_rate": 0.00018591216375927764,
      "loss": 0.2315,
      "step": 2980
    },
    {
      "epoch": 0.21201914554157064,
      "grad_norm": 1.218682885169983,
      "learning_rate": 0.0001858648891410202,
      "loss": 0.2464,
      "step": 2990
    },
    {
      "epoch": 0.2127282396738167,
      "grad_norm": 0.9801955819129944,
      "learning_rate": 0.00018581761452276274,
      "loss": 0.2832,
      "step": 3000
    },
    {
      "epoch": 0.21343733380606275,
      "grad_norm": 1.4123388528823853,
      "learning_rate": 0.00018577033990450526,
      "loss": 0.2354,
      "step": 3010
    },
    {
      "epoch": 0.21414642793830882,
      "grad_norm": 1.7345587015151978,
      "learning_rate": 0.00018572306528624784,
      "loss": 0.2584,
      "step": 3020
    },
    {
      "epoch": 0.21485552207055486,
      "grad_norm": 1.1610444784164429,
      "learning_rate": 0.00018567579066799036,
      "loss": 0.2421,
      "step": 3030
    },
    {
      "epoch": 0.21556461620280093,
      "grad_norm": 1.5557562112808228,
      "learning_rate": 0.0001856285160497329,
      "loss": 0.2344,
      "step": 3040
    },
    {
      "epoch": 0.21627371033504697,
      "grad_norm": 1.2119675874710083,
      "learning_rate": 0.00018558124143147544,
      "loss": 0.2986,
      "step": 3050
    },
    {
      "epoch": 0.21698280446729304,
      "grad_norm": 0.8368197679519653,
      "learning_rate": 0.00018553396681321799,
      "loss": 0.2288,
      "step": 3060
    },
    {
      "epoch": 0.21769189859953908,
      "grad_norm": 1.9182953834533691,
      "learning_rate": 0.00018548669219496054,
      "loss": 0.2508,
      "step": 3070
    },
    {
      "epoch": 0.21840099273178515,
      "grad_norm": 2.6802284717559814,
      "learning_rate": 0.00018543941757670306,
      "loss": 0.2257,
      "step": 3080
    },
    {
      "epoch": 0.2191100868640312,
      "grad_norm": 1.6028999090194702,
      "learning_rate": 0.00018539214295844564,
      "loss": 0.2486,
      "step": 3090
    },
    {
      "epoch": 0.21981918099627726,
      "grad_norm": 2.4337384700775146,
      "learning_rate": 0.00018534486834018816,
      "loss": 0.2997,
      "step": 3100
    },
    {
      "epoch": 0.2205282751285233,
      "grad_norm": 2.5377070903778076,
      "learning_rate": 0.0001852975937219307,
      "loss": 0.3074,
      "step": 3110
    },
    {
      "epoch": 0.22123736926076937,
      "grad_norm": 1.228701114654541,
      "learning_rate": 0.00018525031910367326,
      "loss": 0.2341,
      "step": 3120
    },
    {
      "epoch": 0.22194646339301544,
      "grad_norm": 0.8511971235275269,
      "learning_rate": 0.00018520304448541578,
      "loss": 0.2647,
      "step": 3130
    },
    {
      "epoch": 0.22265555752526148,
      "grad_norm": 0.8759741187095642,
      "learning_rate": 0.00018515576986715833,
      "loss": 0.254,
      "step": 3140
    },
    {
      "epoch": 0.22336465165750755,
      "grad_norm": 0.6809282302856445,
      "learning_rate": 0.00018510849524890088,
      "loss": 0.2879,
      "step": 3150
    },
    {
      "epoch": 0.2240737457897536,
      "grad_norm": 0.6446786522865295,
      "learning_rate": 0.00018506122063064343,
      "loss": 0.2247,
      "step": 3160
    },
    {
      "epoch": 0.22478283992199966,
      "grad_norm": 2.7945873737335205,
      "learning_rate": 0.00018501394601238595,
      "loss": 0.2599,
      "step": 3170
    },
    {
      "epoch": 0.2254919340542457,
      "grad_norm": 1.4430768489837646,
      "learning_rate": 0.0001849666713941285,
      "loss": 0.2607,
      "step": 3180
    },
    {
      "epoch": 0.22620102818649176,
      "grad_norm": 0.9001272916793823,
      "learning_rate": 0.00018491939677587105,
      "loss": 0.2454,
      "step": 3190
    },
    {
      "epoch": 0.2269101223187378,
      "grad_norm": 0.8195831179618835,
      "learning_rate": 0.00018487212215761358,
      "loss": 0.2567,
      "step": 3200
    },
    {
      "epoch": 0.22761921645098387,
      "grad_norm": 1.7363831996917725,
      "learning_rate": 0.00018482484753935615,
      "loss": 0.2631,
      "step": 3210
    },
    {
      "epoch": 0.22832831058322992,
      "grad_norm": 1.6596931219100952,
      "learning_rate": 0.00018477757292109867,
      "loss": 0.2624,
      "step": 3220
    },
    {
      "epoch": 0.22903740471547598,
      "grad_norm": 0.838108479976654,
      "learning_rate": 0.0001847302983028412,
      "loss": 0.2662,
      "step": 3230
    },
    {
      "epoch": 0.22974649884772202,
      "grad_norm": 0.9053291082382202,
      "learning_rate": 0.00018468302368458377,
      "loss": 0.2454,
      "step": 3240
    },
    {
      "epoch": 0.2304555929799681,
      "grad_norm": 0.8540410995483398,
      "learning_rate": 0.0001846357490663263,
      "loss": 0.2802,
      "step": 3250
    },
    {
      "epoch": 0.23116468711221413,
      "grad_norm": 1.3271421194076538,
      "learning_rate": 0.00018458847444806885,
      "loss": 0.26,
      "step": 3260
    },
    {
      "epoch": 0.2318737812444602,
      "grad_norm": 1.786763072013855,
      "learning_rate": 0.00018454119982981137,
      "loss": 0.2942,
      "step": 3270
    },
    {
      "epoch": 0.23258287537670624,
      "grad_norm": 1.0150028467178345,
      "learning_rate": 0.00018449392521155392,
      "loss": 0.2653,
      "step": 3280
    },
    {
      "epoch": 0.2332919695089523,
      "grad_norm": 1.0376200675964355,
      "learning_rate": 0.00018444665059329647,
      "loss": 0.2226,
      "step": 3290
    },
    {
      "epoch": 0.23400106364119838,
      "grad_norm": 1.6831271648406982,
      "learning_rate": 0.000184399375975039,
      "loss": 0.2642,
      "step": 3300
    },
    {
      "epoch": 0.23471015777344442,
      "grad_norm": 1.4219847917556763,
      "learning_rate": 0.00018435210135678157,
      "loss": 0.25,
      "step": 3310
    },
    {
      "epoch": 0.2354192519056905,
      "grad_norm": 1.1142116785049438,
      "learning_rate": 0.0001843048267385241,
      "loss": 0.2892,
      "step": 3320
    },
    {
      "epoch": 0.23612834603793653,
      "grad_norm": 1.8804235458374023,
      "learning_rate": 0.00018425755212026664,
      "loss": 0.268,
      "step": 3330
    },
    {
      "epoch": 0.2368374401701826,
      "grad_norm": 0.967840850353241,
      "learning_rate": 0.0001842102775020092,
      "loss": 0.2508,
      "step": 3340
    },
    {
      "epoch": 0.23754653430242864,
      "grad_norm": 1.0075278282165527,
      "learning_rate": 0.00018416300288375171,
      "loss": 0.2642,
      "step": 3350
    },
    {
      "epoch": 0.2382556284346747,
      "grad_norm": 1.603419542312622,
      "learning_rate": 0.00018411572826549426,
      "loss": 0.2884,
      "step": 3360
    },
    {
      "epoch": 0.23896472256692075,
      "grad_norm": 0.749382734298706,
      "learning_rate": 0.00018406845364723681,
      "loss": 0.2676,
      "step": 3370
    },
    {
      "epoch": 0.23967381669916682,
      "grad_norm": 1.1719074249267578,
      "learning_rate": 0.00018402117902897936,
      "loss": 0.2554,
      "step": 3380
    },
    {
      "epoch": 0.24038291083141286,
      "grad_norm": 1.0753965377807617,
      "learning_rate": 0.0001839739044107219,
      "loss": 0.2642,
      "step": 3390
    },
    {
      "epoch": 0.24109200496365893,
      "grad_norm": 1.0663886070251465,
      "learning_rate": 0.00018392662979246444,
      "loss": 0.2277,
      "step": 3400
    },
    {
      "epoch": 0.24180109909590497,
      "grad_norm": 1.8587250709533691,
      "learning_rate": 0.00018387935517420699,
      "loss": 0.2459,
      "step": 3410
    },
    {
      "epoch": 0.24251019322815104,
      "grad_norm": 1.3095539808273315,
      "learning_rate": 0.0001838320805559495,
      "loss": 0.2672,
      "step": 3420
    },
    {
      "epoch": 0.24321928736039708,
      "grad_norm": 1.689513921737671,
      "learning_rate": 0.00018378480593769209,
      "loss": 0.2786,
      "step": 3430
    },
    {
      "epoch": 0.24392838149264315,
      "grad_norm": 0.9345725178718567,
      "learning_rate": 0.0001837375313194346,
      "loss": 0.2429,
      "step": 3440
    },
    {
      "epoch": 0.24463747562488922,
      "grad_norm": 0.8459925651550293,
      "learning_rate": 0.00018369025670117713,
      "loss": 0.2458,
      "step": 3450
    },
    {
      "epoch": 0.24534656975713526,
      "grad_norm": 0.8830893635749817,
      "learning_rate": 0.00018364298208291968,
      "loss": 0.2571,
      "step": 3460
    },
    {
      "epoch": 0.24605566388938133,
      "grad_norm": 0.79368656873703,
      "learning_rate": 0.00018359570746466223,
      "loss": 0.267,
      "step": 3470
    },
    {
      "epoch": 0.24676475802162737,
      "grad_norm": 1.2297968864440918,
      "learning_rate": 0.00018354843284640478,
      "loss": 0.2947,
      "step": 3480
    },
    {
      "epoch": 0.24747385215387344,
      "grad_norm": 0.6913232803344727,
      "learning_rate": 0.0001835011582281473,
      "loss": 0.2683,
      "step": 3490
    },
    {
      "epoch": 0.24818294628611948,
      "grad_norm": 1.1726930141448975,
      "learning_rate": 0.00018345388360988985,
      "loss": 0.265,
      "step": 3500
    },
    {
      "epoch": 0.24889204041836555,
      "grad_norm": 1.0794744491577148,
      "learning_rate": 0.0001834066089916324,
      "loss": 0.2881,
      "step": 3510
    },
    {
      "epoch": 0.2496011345506116,
      "grad_norm": 2.533205509185791,
      "learning_rate": 0.00018335933437337493,
      "loss": 0.286,
      "step": 3520
    },
    {
      "epoch": 0.2503102286828576,
      "grad_norm": 1.267185926437378,
      "learning_rate": 0.0001833120597551175,
      "loss": 0.242,
      "step": 3530
    },
    {
      "epoch": 0.2510193228151037,
      "grad_norm": 1.1512022018432617,
      "learning_rate": 0.00018326478513686003,
      "loss": 0.3004,
      "step": 3540
    },
    {
      "epoch": 0.25172841694734976,
      "grad_norm": 1.8953015804290771,
      "learning_rate": 0.00018321751051860258,
      "loss": 0.269,
      "step": 3550
    },
    {
      "epoch": 0.25243751107959583,
      "grad_norm": 1.037240743637085,
      "learning_rate": 0.00018317023590034513,
      "loss": 0.2846,
      "step": 3560
    },
    {
      "epoch": 0.25314660521184185,
      "grad_norm": 1.6666643619537354,
      "learning_rate": 0.00018312296128208765,
      "loss": 0.2538,
      "step": 3570
    },
    {
      "epoch": 0.2538556993440879,
      "grad_norm": 0.8643954396247864,
      "learning_rate": 0.0001830756866638302,
      "loss": 0.2509,
      "step": 3580
    },
    {
      "epoch": 0.254564793476334,
      "grad_norm": 0.8527016043663025,
      "learning_rate": 0.00018302841204557275,
      "loss": 0.249,
      "step": 3590
    },
    {
      "epoch": 0.25527388760858005,
      "grad_norm": 1.2303332090377808,
      "learning_rate": 0.0001829811374273153,
      "loss": 0.2456,
      "step": 3600
    },
    {
      "epoch": 0.2559829817408261,
      "grad_norm": 0.9012022018432617,
      "learning_rate": 0.00018293386280905782,
      "loss": 0.2515,
      "step": 3610
    },
    {
      "epoch": 0.25669207587307213,
      "grad_norm": 2.5316081047058105,
      "learning_rate": 0.00018288658819080037,
      "loss": 0.2606,
      "step": 3620
    },
    {
      "epoch": 0.2574011700053182,
      "grad_norm": 1.1147394180297852,
      "learning_rate": 0.00018283931357254292,
      "loss": 0.2429,
      "step": 3630
    },
    {
      "epoch": 0.25811026413756427,
      "grad_norm": 2.2033302783966064,
      "learning_rate": 0.00018279203895428544,
      "loss": 0.2731,
      "step": 3640
    },
    {
      "epoch": 0.25881935826981034,
      "grad_norm": 2.3611466884613037,
      "learning_rate": 0.000182744764336028,
      "loss": 0.2829,
      "step": 3650
    },
    {
      "epoch": 0.25952845240205635,
      "grad_norm": 1.7612866163253784,
      "learning_rate": 0.00018269748971777054,
      "loss": 0.2857,
      "step": 3660
    },
    {
      "epoch": 0.2602375465343024,
      "grad_norm": 2.4925246238708496,
      "learning_rate": 0.00018265021509951307,
      "loss": 0.2678,
      "step": 3670
    },
    {
      "epoch": 0.2609466406665485,
      "grad_norm": 0.9352823495864868,
      "learning_rate": 0.00018260294048125562,
      "loss": 0.2737,
      "step": 3680
    },
    {
      "epoch": 0.26165573479879456,
      "grad_norm": 2.3637876510620117,
      "learning_rate": 0.00018255566586299817,
      "loss": 0.2638,
      "step": 3690
    },
    {
      "epoch": 0.2623648289310406,
      "grad_norm": 0.665292501449585,
      "learning_rate": 0.00018250839124474071,
      "loss": 0.2613,
      "step": 3700
    },
    {
      "epoch": 0.26307392306328664,
      "grad_norm": 1.0509034395217896,
      "learning_rate": 0.00018246111662648324,
      "loss": 0.2424,
      "step": 3710
    },
    {
      "epoch": 0.2637830171955327,
      "grad_norm": 0.7850553393363953,
      "learning_rate": 0.0001824138420082258,
      "loss": 0.2097,
      "step": 3720
    },
    {
      "epoch": 0.2644921113277788,
      "grad_norm": 1.8905192613601685,
      "learning_rate": 0.00018236656738996834,
      "loss": 0.2591,
      "step": 3730
    },
    {
      "epoch": 0.2652012054600248,
      "grad_norm": 0.7195723056793213,
      "learning_rate": 0.00018231929277171086,
      "loss": 0.2684,
      "step": 3740
    },
    {
      "epoch": 0.26591029959227086,
      "grad_norm": 0.5878646969795227,
      "learning_rate": 0.00018227201815345344,
      "loss": 0.2522,
      "step": 3750
    },
    {
      "epoch": 0.26661939372451693,
      "grad_norm": 1.4237256050109863,
      "learning_rate": 0.00018222474353519596,
      "loss": 0.2403,
      "step": 3760
    },
    {
      "epoch": 0.267328487856763,
      "grad_norm": 0.7250386476516724,
      "learning_rate": 0.0001821774689169385,
      "loss": 0.2544,
      "step": 3770
    },
    {
      "epoch": 0.26803758198900907,
      "grad_norm": 1.2663962841033936,
      "learning_rate": 0.00018213019429868106,
      "loss": 0.2743,
      "step": 3780
    },
    {
      "epoch": 0.2687466761212551,
      "grad_norm": 2.1970319747924805,
      "learning_rate": 0.00018208291968042358,
      "loss": 0.2872,
      "step": 3790
    },
    {
      "epoch": 0.26945577025350115,
      "grad_norm": 1.1918034553527832,
      "learning_rate": 0.00018203564506216613,
      "loss": 0.2626,
      "step": 3800
    },
    {
      "epoch": 0.2701648643857472,
      "grad_norm": 0.6416267156600952,
      "learning_rate": 0.00018198837044390868,
      "loss": 0.2452,
      "step": 3810
    },
    {
      "epoch": 0.2708739585179933,
      "grad_norm": 2.134974241256714,
      "learning_rate": 0.00018194109582565123,
      "loss": 0.2757,
      "step": 3820
    },
    {
      "epoch": 0.2715830526502393,
      "grad_norm": 0.7155605554580688,
      "learning_rate": 0.00018189382120739375,
      "loss": 0.2893,
      "step": 3830
    },
    {
      "epoch": 0.27229214678248537,
      "grad_norm": 1.2465873956680298,
      "learning_rate": 0.00018184654658913628,
      "loss": 0.2441,
      "step": 3840
    },
    {
      "epoch": 0.27300124091473144,
      "grad_norm": 0.8818164467811584,
      "learning_rate": 0.00018179927197087885,
      "loss": 0.246,
      "step": 3850
    },
    {
      "epoch": 0.2737103350469775,
      "grad_norm": 2.120513916015625,
      "learning_rate": 0.00018175199735262138,
      "loss": 0.2476,
      "step": 3860
    },
    {
      "epoch": 0.2744194291792235,
      "grad_norm": 1.7517086267471313,
      "learning_rate": 0.00018170472273436393,
      "loss": 0.2423,
      "step": 3870
    },
    {
      "epoch": 0.2751285233114696,
      "grad_norm": 0.8103969097137451,
      "learning_rate": 0.00018165744811610648,
      "loss": 0.2454,
      "step": 3880
    },
    {
      "epoch": 0.27583761744371565,
      "grad_norm": 1.9974713325500488,
      "learning_rate": 0.000181610173497849,
      "loss": 0.2751,
      "step": 3890
    },
    {
      "epoch": 0.2765467115759617,
      "grad_norm": 0.8360927104949951,
      "learning_rate": 0.00018156289887959155,
      "loss": 0.2284,
      "step": 3900
    },
    {
      "epoch": 0.2772558057082078,
      "grad_norm": 0.790473997592926,
      "learning_rate": 0.0001815156242613341,
      "loss": 0.2548,
      "step": 3910
    },
    {
      "epoch": 0.2779648998404538,
      "grad_norm": 1.0280684232711792,
      "learning_rate": 0.00018146834964307665,
      "loss": 0.2587,
      "step": 3920
    },
    {
      "epoch": 0.2786739939726999,
      "grad_norm": 1.1009541749954224,
      "learning_rate": 0.00018142107502481917,
      "loss": 0.2759,
      "step": 3930
    },
    {
      "epoch": 0.27938308810494594,
      "grad_norm": 1.3664227724075317,
      "learning_rate": 0.00018137380040656172,
      "loss": 0.2438,
      "step": 3940
    },
    {
      "epoch": 0.280092182237192,
      "grad_norm": 1.8239940404891968,
      "learning_rate": 0.00018132652578830427,
      "loss": 0.2545,
      "step": 3950
    },
    {
      "epoch": 0.280801276369438,
      "grad_norm": 1.753720998764038,
      "learning_rate": 0.0001812792511700468,
      "loss": 0.2552,
      "step": 3960
    },
    {
      "epoch": 0.2815103705016841,
      "grad_norm": 0.8416692614555359,
      "learning_rate": 0.00018123197655178937,
      "loss": 0.2437,
      "step": 3970
    },
    {
      "epoch": 0.28221946463393016,
      "grad_norm": 0.8046243786811829,
      "learning_rate": 0.0001811847019335319,
      "loss": 0.2542,
      "step": 3980
    },
    {
      "epoch": 0.28292855876617623,
      "grad_norm": 1.1426329612731934,
      "learning_rate": 0.00018113742731527444,
      "loss": 0.276,
      "step": 3990
    },
    {
      "epoch": 0.28363765289842224,
      "grad_norm": 1.3122843503952026,
      "learning_rate": 0.000181090152697017,
      "loss": 0.2716,
      "step": 4000
    },
    {
      "epoch": 0.2843467470306683,
      "grad_norm": 2.3408994674682617,
      "learning_rate": 0.00018104287807875952,
      "loss": 0.2865,
      "step": 4010
    },
    {
      "epoch": 0.2850558411629144,
      "grad_norm": 2.3424763679504395,
      "learning_rate": 0.00018099560346050207,
      "loss": 0.2802,
      "step": 4020
    },
    {
      "epoch": 0.28576493529516045,
      "grad_norm": 1.2727476358413696,
      "learning_rate": 0.0001809483288422446,
      "loss": 0.2395,
      "step": 4030
    },
    {
      "epoch": 0.28647402942740646,
      "grad_norm": 2.1498067378997803,
      "learning_rate": 0.00018090105422398717,
      "loss": 0.3035,
      "step": 4040
    },
    {
      "epoch": 0.28718312355965253,
      "grad_norm": 1.0284558534622192,
      "learning_rate": 0.0001808537796057297,
      "loss": 0.2582,
      "step": 4050
    },
    {
      "epoch": 0.2878922176918986,
      "grad_norm": 2.086409568786621,
      "learning_rate": 0.00018080650498747224,
      "loss": 0.2997,
      "step": 4060
    },
    {
      "epoch": 0.28860131182414467,
      "grad_norm": 2.9512903690338135,
      "learning_rate": 0.0001807592303692148,
      "loss": 0.2568,
      "step": 4070
    },
    {
      "epoch": 0.28931040595639074,
      "grad_norm": 1.8817325830459595,
      "learning_rate": 0.0001807119557509573,
      "loss": 0.2697,
      "step": 4080
    },
    {
      "epoch": 0.29001950008863675,
      "grad_norm": 1.0644490718841553,
      "learning_rate": 0.00018066468113269986,
      "loss": 0.2478,
      "step": 4090
    },
    {
      "epoch": 0.2907285942208828,
      "grad_norm": 1.830605387687683,
      "learning_rate": 0.0001806174065144424,
      "loss": 0.2743,
      "step": 4100
    },
    {
      "epoch": 0.2914376883531289,
      "grad_norm": 0.7864764928817749,
      "learning_rate": 0.00018057013189618493,
      "loss": 0.2252,
      "step": 4110
    },
    {
      "epoch": 0.29214678248537496,
      "grad_norm": 0.8891043066978455,
      "learning_rate": 0.00018052285727792748,
      "loss": 0.2584,
      "step": 4120
    },
    {
      "epoch": 0.29285587661762097,
      "grad_norm": 0.8757243752479553,
      "learning_rate": 0.00018047558265967003,
      "loss": 0.2459,
      "step": 4130
    },
    {
      "epoch": 0.29356497074986704,
      "grad_norm": 0.8982653021812439,
      "learning_rate": 0.00018042830804141258,
      "loss": 0.255,
      "step": 4140
    },
    {
      "epoch": 0.2942740648821131,
      "grad_norm": 1.6614775657653809,
      "learning_rate": 0.0001803810334231551,
      "loss": 0.2382,
      "step": 4150
    },
    {
      "epoch": 0.2949831590143592,
      "grad_norm": 0.6976360082626343,
      "learning_rate": 0.00018033375880489766,
      "loss": 0.2613,
      "step": 4160
    },
    {
      "epoch": 0.2956922531466052,
      "grad_norm": 0.6333728432655334,
      "learning_rate": 0.0001802864841866402,
      "loss": 0.2144,
      "step": 4170
    },
    {
      "epoch": 0.29640134727885126,
      "grad_norm": 2.4130356311798096,
      "learning_rate": 0.00018023920956838273,
      "loss": 0.2275,
      "step": 4180
    },
    {
      "epoch": 0.2971104414110973,
      "grad_norm": 0.5498109459877014,
      "learning_rate": 0.0001801919349501253,
      "loss": 0.2708,
      "step": 4190
    },
    {
      "epoch": 0.2978195355433434,
      "grad_norm": 0.5913639068603516,
      "learning_rate": 0.00018014466033186783,
      "loss": 0.2321,
      "step": 4200
    },
    {
      "epoch": 0.2985286296755894,
      "grad_norm": 0.6701322197914124,
      "learning_rate": 0.00018009738571361038,
      "loss": 0.285,
      "step": 4210
    },
    {
      "epoch": 0.2992377238078355,
      "grad_norm": 0.5755533576011658,
      "learning_rate": 0.00018005011109535293,
      "loss": 0.2631,
      "step": 4220
    },
    {
      "epoch": 0.29994681794008154,
      "grad_norm": 1.0305063724517822,
      "learning_rate": 0.00018000283647709545,
      "loss": 0.2742,
      "step": 4230
    },
    {
      "epoch": 0.3006559120723276,
      "grad_norm": 1.2905946969985962,
      "learning_rate": 0.000179955561858838,
      "loss": 0.3176,
      "step": 4240
    },
    {
      "epoch": 0.3013650062045737,
      "grad_norm": 1.2588958740234375,
      "learning_rate": 0.00017990828724058052,
      "loss": 0.2641,
      "step": 4250
    },
    {
      "epoch": 0.3020741003368197,
      "grad_norm": 3.2128891944885254,
      "learning_rate": 0.0001798610126223231,
      "loss": 0.2785,
      "step": 4260
    },
    {
      "epoch": 0.30278319446906576,
      "grad_norm": 1.202512264251709,
      "learning_rate": 0.00017981373800406562,
      "loss": 0.2787,
      "step": 4270
    },
    {
      "epoch": 0.30349228860131183,
      "grad_norm": 1.4388712644577026,
      "learning_rate": 0.00017976646338580817,
      "loss": 0.2265,
      "step": 4280
    },
    {
      "epoch": 0.3042013827335579,
      "grad_norm": 1.7792384624481201,
      "learning_rate": 0.00017971918876755072,
      "loss": 0.2712,
      "step": 4290
    },
    {
      "epoch": 0.3049104768658039,
      "grad_norm": 0.8364426493644714,
      "learning_rate": 0.00017967191414929324,
      "loss": 0.2801,
      "step": 4300
    },
    {
      "epoch": 0.30561957099805,
      "grad_norm": 1.973788857460022,
      "learning_rate": 0.0001796246395310358,
      "loss": 0.2586,
      "step": 4310
    },
    {
      "epoch": 0.30632866513029605,
      "grad_norm": 0.9923695921897888,
      "learning_rate": 0.00017957736491277834,
      "loss": 0.2557,
      "step": 4320
    },
    {
      "epoch": 0.3070377592625421,
      "grad_norm": 1.724121332168579,
      "learning_rate": 0.00017953009029452087,
      "loss": 0.2677,
      "step": 4330
    },
    {
      "epoch": 0.30774685339478813,
      "grad_norm": 0.8580439686775208,
      "learning_rate": 0.00017948281567626342,
      "loss": 0.2618,
      "step": 4340
    },
    {
      "epoch": 0.3084559475270342,
      "grad_norm": 0.9599119424819946,
      "learning_rate": 0.00017943554105800597,
      "loss": 0.2458,
      "step": 4350
    },
    {
      "epoch": 0.30916504165928027,
      "grad_norm": 0.8896514773368835,
      "learning_rate": 0.00017938826643974852,
      "loss": 0.2722,
      "step": 4360
    },
    {
      "epoch": 0.30987413579152634,
      "grad_norm": 0.5425814986228943,
      "learning_rate": 0.00017934099182149104,
      "loss": 0.2331,
      "step": 4370
    },
    {
      "epoch": 0.3105832299237724,
      "grad_norm": 2.334777593612671,
      "learning_rate": 0.0001792937172032336,
      "loss": 0.2406,
      "step": 4380
    },
    {
      "epoch": 0.3112923240560184,
      "grad_norm": 2.484671115875244,
      "learning_rate": 0.00017924644258497614,
      "loss": 0.2694,
      "step": 4390
    },
    {
      "epoch": 0.3120014181882645,
      "grad_norm": 4.15142297744751,
      "learning_rate": 0.00017919916796671866,
      "loss": 0.2492,
      "step": 4400
    },
    {
      "epoch": 0.31271051232051056,
      "grad_norm": 1.2525464296340942,
      "learning_rate": 0.00017915189334846124,
      "loss": 0.2525,
      "step": 4410
    },
    {
      "epoch": 0.3134196064527566,
      "grad_norm": 2.5822558403015137,
      "learning_rate": 0.00017910461873020376,
      "loss": 0.2737,
      "step": 4420
    },
    {
      "epoch": 0.31412870058500264,
      "grad_norm": 1.4669294357299805,
      "learning_rate": 0.0001790573441119463,
      "loss": 0.2646,
      "step": 4430
    },
    {
      "epoch": 0.3148377947172487,
      "grad_norm": 2.4390170574188232,
      "learning_rate": 0.00017901006949368883,
      "loss": 0.2595,
      "step": 4440
    },
    {
      "epoch": 0.3155468888494948,
      "grad_norm": 1.679305076599121,
      "learning_rate": 0.00017896279487543138,
      "loss": 0.2502,
      "step": 4450
    },
    {
      "epoch": 0.31625598298174085,
      "grad_norm": 1.3732250928878784,
      "learning_rate": 0.00017891552025717393,
      "loss": 0.24,
      "step": 4460
    },
    {
      "epoch": 0.31696507711398686,
      "grad_norm": 1.0975297689437866,
      "learning_rate": 0.00017886824563891646,
      "loss": 0.2884,
      "step": 4470
    },
    {
      "epoch": 0.31767417124623293,
      "grad_norm": 1.612221121788025,
      "learning_rate": 0.00017882097102065903,
      "loss": 0.2666,
      "step": 4480
    },
    {
      "epoch": 0.318383265378479,
      "grad_norm": 0.9923020601272583,
      "learning_rate": 0.00017877369640240156,
      "loss": 0.2671,
      "step": 4490
    },
    {
      "epoch": 0.31909235951072507,
      "grad_norm": 1.1972486972808838,
      "learning_rate": 0.0001787264217841441,
      "loss": 0.2617,
      "step": 4500
    },
    {
      "epoch": 0.3198014536429711,
      "grad_norm": 1.6039806604385376,
      "learning_rate": 0.00017867914716588666,
      "loss": 0.2661,
      "step": 4510
    },
    {
      "epoch": 0.32051054777521715,
      "grad_norm": 1.6550395488739014,
      "learning_rate": 0.00017863187254762918,
      "loss": 0.2705,
      "step": 4520
    },
    {
      "epoch": 0.3212196419074632,
      "grad_norm": 0.7678760290145874,
      "learning_rate": 0.00017858459792937173,
      "loss": 0.2585,
      "step": 4530
    },
    {
      "epoch": 0.3219287360397093,
      "grad_norm": 0.9087663888931274,
      "learning_rate": 0.00017853732331111428,
      "loss": 0.295,
      "step": 4540
    },
    {
      "epoch": 0.32263783017195535,
      "grad_norm": 0.7870078682899475,
      "learning_rate": 0.00017849004869285683,
      "loss": 0.2369,
      "step": 4550
    },
    {
      "epoch": 0.32334692430420137,
      "grad_norm": 0.8359392285346985,
      "learning_rate": 0.00017844277407459935,
      "loss": 0.2308,
      "step": 4560
    },
    {
      "epoch": 0.32405601843644743,
      "grad_norm": 0.7538592219352722,
      "learning_rate": 0.0001783954994563419,
      "loss": 0.2183,
      "step": 4570
    },
    {
      "epoch": 0.3247651125686935,
      "grad_norm": 0.6682364344596863,
      "learning_rate": 0.00017834822483808445,
      "loss": 0.2245,
      "step": 4580
    },
    {
      "epoch": 0.3254742067009396,
      "grad_norm": 0.8391419649124146,
      "learning_rate": 0.00017830095021982697,
      "loss": 0.2655,
      "step": 4590
    },
    {
      "epoch": 0.3261833008331856,
      "grad_norm": 1.2950176000595093,
      "learning_rate": 0.00017825367560156952,
      "loss": 0.2562,
      "step": 4600
    },
    {
      "epoch": 0.32689239496543165,
      "grad_norm": 0.988189697265625,
      "learning_rate": 0.00017820640098331207,
      "loss": 0.2589,
      "step": 4610
    },
    {
      "epoch": 0.3276014890976777,
      "grad_norm": 1.4140211343765259,
      "learning_rate": 0.0001781591263650546,
      "loss": 0.2773,
      "step": 4620
    },
    {
      "epoch": 0.3283105832299238,
      "grad_norm": 0.655158281326294,
      "learning_rate": 0.00017811185174679715,
      "loss": 0.2503,
      "step": 4630
    },
    {
      "epoch": 0.3290196773621698,
      "grad_norm": 0.7069393992424011,
      "learning_rate": 0.0001780645771285397,
      "loss": 0.2777,
      "step": 4640
    },
    {
      "epoch": 0.3297287714944159,
      "grad_norm": 1.163470983505249,
      "learning_rate": 0.00017801730251028225,
      "loss": 0.2542,
      "step": 4650
    },
    {
      "epoch": 0.33043786562666194,
      "grad_norm": 2.6981148719787598,
      "learning_rate": 0.00017797002789202477,
      "loss": 0.251,
      "step": 4660
    },
    {
      "epoch": 0.331146959758908,
      "grad_norm": 1.27299964427948,
      "learning_rate": 0.00017792275327376732,
      "loss": 0.2338,
      "step": 4670
    },
    {
      "epoch": 0.331856053891154,
      "grad_norm": 1.1069002151489258,
      "learning_rate": 0.00017787547865550987,
      "loss": 0.2785,
      "step": 4680
    },
    {
      "epoch": 0.3325651480234001,
      "grad_norm": 1.219716191291809,
      "learning_rate": 0.0001778282040372524,
      "loss": 0.2831,
      "step": 4690
    },
    {
      "epoch": 0.33327424215564616,
      "grad_norm": 1.9857020378112793,
      "learning_rate": 0.00017778092941899497,
      "loss": 0.2578,
      "step": 4700
    },
    {
      "epoch": 0.33398333628789223,
      "grad_norm": 0.5434474945068359,
      "learning_rate": 0.0001777336548007375,
      "loss": 0.2281,
      "step": 4710
    },
    {
      "epoch": 0.3346924304201383,
      "grad_norm": 1.1170605421066284,
      "learning_rate": 0.00017768638018248004,
      "loss": 0.2527,
      "step": 4720
    },
    {
      "epoch": 0.3354015245523843,
      "grad_norm": 1.5311907529830933,
      "learning_rate": 0.0001776391055642226,
      "loss": 0.2783,
      "step": 4730
    },
    {
      "epoch": 0.3361106186846304,
      "grad_norm": 0.7099084854125977,
      "learning_rate": 0.0001775918309459651,
      "loss": 0.2534,
      "step": 4740
    },
    {
      "epoch": 0.33681971281687645,
      "grad_norm": 0.906395673751831,
      "learning_rate": 0.00017754455632770766,
      "loss": 0.2427,
      "step": 4750
    },
    {
      "epoch": 0.3375288069491225,
      "grad_norm": 0.8628702759742737,
      "learning_rate": 0.0001774972817094502,
      "loss": 0.2718,
      "step": 4760
    },
    {
      "epoch": 0.33823790108136853,
      "grad_norm": 0.8461543321609497,
      "learning_rate": 0.00017745000709119276,
      "loss": 0.2358,
      "step": 4770
    },
    {
      "epoch": 0.3389469952136146,
      "grad_norm": 1.3099058866500854,
      "learning_rate": 0.00017740273247293529,
      "loss": 0.2683,
      "step": 4780
    },
    {
      "epoch": 0.33965608934586067,
      "grad_norm": 1.2433022260665894,
      "learning_rate": 0.00017735545785467784,
      "loss": 0.249,
      "step": 4790
    },
    {
      "epoch": 0.34036518347810674,
      "grad_norm": 0.6366826891899109,
      "learning_rate": 0.00017730818323642038,
      "loss": 0.2735,
      "step": 4800
    },
    {
      "epoch": 0.34107427761035275,
      "grad_norm": 0.97030109167099,
      "learning_rate": 0.0001772609086181629,
      "loss": 0.2418,
      "step": 4810
    },
    {
      "epoch": 0.3417833717425988,
      "grad_norm": 1.5129250288009644,
      "learning_rate": 0.00017721363399990546,
      "loss": 0.2674,
      "step": 4820
    },
    {
      "epoch": 0.3424924658748449,
      "grad_norm": 0.9896216988563538,
      "learning_rate": 0.000177166359381648,
      "loss": 0.235,
      "step": 4830
    },
    {
      "epoch": 0.34320156000709096,
      "grad_norm": 1.2240359783172607,
      "learning_rate": 0.00017711908476339053,
      "loss": 0.2467,
      "step": 4840
    },
    {
      "epoch": 0.343910654139337,
      "grad_norm": 1.6246519088745117,
      "learning_rate": 0.00017707181014513308,
      "loss": 0.2375,
      "step": 4850
    },
    {
      "epoch": 0.34461974827158304,
      "grad_norm": 0.7467844486236572,
      "learning_rate": 0.00017702453552687563,
      "loss": 0.2612,
      "step": 4860
    },
    {
      "epoch": 0.3453288424038291,
      "grad_norm": 1.1963647603988647,
      "learning_rate": 0.00017697726090861818,
      "loss": 0.268,
      "step": 4870
    },
    {
      "epoch": 0.3460379365360752,
      "grad_norm": 0.592189371585846,
      "learning_rate": 0.0001769299862903607,
      "loss": 0.2267,
      "step": 4880
    },
    {
      "epoch": 0.34674703066832124,
      "grad_norm": 1.07489013671875,
      "learning_rate": 0.00017688271167210325,
      "loss": 0.2605,
      "step": 4890
    },
    {
      "epoch": 0.34745612480056726,
      "grad_norm": 2.7448432445526123,
      "learning_rate": 0.0001768354370538458,
      "loss": 0.2605,
      "step": 4900
    },
    {
      "epoch": 0.3481652189328133,
      "grad_norm": 1.5144997835159302,
      "learning_rate": 0.00017678816243558832,
      "loss": 0.2658,
      "step": 4910
    },
    {
      "epoch": 0.3488743130650594,
      "grad_norm": 1.673048973083496,
      "learning_rate": 0.0001767408878173309,
      "loss": 0.2608,
      "step": 4920
    },
    {
      "epoch": 0.34958340719730546,
      "grad_norm": 3.5375864505767822,
      "learning_rate": 0.00017669361319907342,
      "loss": 0.2278,
      "step": 4930
    },
    {
      "epoch": 0.3502925013295515,
      "grad_norm": 3.33445405960083,
      "learning_rate": 0.00017664633858081597,
      "loss": 0.2518,
      "step": 4940
    },
    {
      "epoch": 0.35100159546179754,
      "grad_norm": 1.4637892246246338,
      "learning_rate": 0.00017659906396255852,
      "loss": 0.3273,
      "step": 4950
    },
    {
      "epoch": 0.3517106895940436,
      "grad_norm": 1.431894063949585,
      "learning_rate": 0.00017655178934430105,
      "loss": 0.2754,
      "step": 4960
    },
    {
      "epoch": 0.3524197837262897,
      "grad_norm": 1.1615678071975708,
      "learning_rate": 0.0001765045147260436,
      "loss": 0.2668,
      "step": 4970
    },
    {
      "epoch": 0.3531288778585357,
      "grad_norm": 1.2754079103469849,
      "learning_rate": 0.00017645724010778615,
      "loss": 0.2577,
      "step": 4980
    },
    {
      "epoch": 0.35383797199078176,
      "grad_norm": 1.4599144458770752,
      "learning_rate": 0.0001764099654895287,
      "loss": 0.2526,
      "step": 4990
    },
    {
      "epoch": 0.35454706612302783,
      "grad_norm": 0.9555533528327942,
      "learning_rate": 0.00017636269087127122,
      "loss": 0.2611,
      "step": 5000
    },
    {
      "epoch": 0.3552561602552739,
      "grad_norm": 1.2423195838928223,
      "learning_rate": 0.00017631541625301374,
      "loss": 0.2467,
      "step": 5010
    },
    {
      "epoch": 0.35596525438751997,
      "grad_norm": 0.8186541795730591,
      "learning_rate": 0.00017626814163475632,
      "loss": 0.251,
      "step": 5020
    },
    {
      "epoch": 0.356674348519766,
      "grad_norm": 1.0290107727050781,
      "learning_rate": 0.00017622086701649884,
      "loss": 0.227,
      "step": 5030
    },
    {
      "epoch": 0.35738344265201205,
      "grad_norm": 1.5596919059753418,
      "learning_rate": 0.0001761735923982414,
      "loss": 0.2588,
      "step": 5040
    },
    {
      "epoch": 0.3580925367842581,
      "grad_norm": 1.7232105731964111,
      "learning_rate": 0.00017612631777998394,
      "loss": 0.2677,
      "step": 5050
    },
    {
      "epoch": 0.3588016309165042,
      "grad_norm": 1.3309991359710693,
      "learning_rate": 0.00017607904316172646,
      "loss": 0.2524,
      "step": 5060
    },
    {
      "epoch": 0.3595107250487502,
      "grad_norm": 0.8367503881454468,
      "learning_rate": 0.00017603176854346901,
      "loss": 0.2345,
      "step": 5070
    },
    {
      "epoch": 0.36021981918099627,
      "grad_norm": 0.5795304179191589,
      "learning_rate": 0.00017598449392521156,
      "loss": 0.2671,
      "step": 5080
    },
    {
      "epoch": 0.36092891331324234,
      "grad_norm": 1.2193645238876343,
      "learning_rate": 0.0001759372193069541,
      "loss": 0.2394,
      "step": 5090
    },
    {
      "epoch": 0.3616380074454884,
      "grad_norm": 0.8031187653541565,
      "learning_rate": 0.00017588994468869664,
      "loss": 0.2447,
      "step": 5100
    },
    {
      "epoch": 0.3623471015777344,
      "grad_norm": 1.2308318614959717,
      "learning_rate": 0.00017584267007043919,
      "loss": 0.2225,
      "step": 5110
    },
    {
      "epoch": 0.3630561957099805,
      "grad_norm": 1.2967262268066406,
      "learning_rate": 0.00017579539545218174,
      "loss": 0.2726,
      "step": 5120
    },
    {
      "epoch": 0.36376528984222656,
      "grad_norm": 0.8396568298339844,
      "learning_rate": 0.00017574812083392426,
      "loss": 0.2634,
      "step": 5130
    },
    {
      "epoch": 0.3644743839744726,
      "grad_norm": 0.7779065370559692,
      "learning_rate": 0.00017570084621566684,
      "loss": 0.2464,
      "step": 5140
    },
    {
      "epoch": 0.36518347810671864,
      "grad_norm": 0.8754645586013794,
      "learning_rate": 0.00017565357159740936,
      "loss": 0.2492,
      "step": 5150
    },
    {
      "epoch": 0.3658925722389647,
      "grad_norm": 0.9279621839523315,
      "learning_rate": 0.0001756062969791519,
      "loss": 0.249,
      "step": 5160
    },
    {
      "epoch": 0.3666016663712108,
      "grad_norm": 1.0622718334197998,
      "learning_rate": 0.00017555902236089446,
      "loss": 0.2705,
      "step": 5170
    },
    {
      "epoch": 0.36731076050345685,
      "grad_norm": 2.246631383895874,
      "learning_rate": 0.00017551174774263698,
      "loss": 0.2435,
      "step": 5180
    },
    {
      "epoch": 0.3680198546357029,
      "grad_norm": 0.9231953620910645,
      "learning_rate": 0.00017546447312437953,
      "loss": 0.2922,
      "step": 5190
    },
    {
      "epoch": 0.3687289487679489,
      "grad_norm": 0.7061732411384583,
      "learning_rate": 0.00017541719850612208,
      "loss": 0.242,
      "step": 5200
    },
    {
      "epoch": 0.369438042900195,
      "grad_norm": 1.707993984222412,
      "learning_rate": 0.00017536992388786463,
      "loss": 0.2512,
      "step": 5210
    },
    {
      "epoch": 0.37014713703244106,
      "grad_norm": 0.9760698676109314,
      "learning_rate": 0.00017532264926960715,
      "loss": 0.2459,
      "step": 5220
    },
    {
      "epoch": 0.37085623116468713,
      "grad_norm": 0.8313971161842346,
      "learning_rate": 0.00017527537465134968,
      "loss": 0.2603,
      "step": 5230
    },
    {
      "epoch": 0.37156532529693315,
      "grad_norm": 0.785847008228302,
      "learning_rate": 0.00017522810003309225,
      "loss": 0.2464,
      "step": 5240
    },
    {
      "epoch": 0.3722744194291792,
      "grad_norm": 1.6808832883834839,
      "learning_rate": 0.00017518082541483478,
      "loss": 0.2202,
      "step": 5250
    },
    {
      "epoch": 0.3729835135614253,
      "grad_norm": 1.6272454261779785,
      "learning_rate": 0.00017513355079657733,
      "loss": 0.2652,
      "step": 5260
    },
    {
      "epoch": 0.37369260769367135,
      "grad_norm": 0.8752783536911011,
      "learning_rate": 0.00017508627617831988,
      "loss": 0.2265,
      "step": 5270
    },
    {
      "epoch": 0.37440170182591737,
      "grad_norm": 0.741672933101654,
      "learning_rate": 0.0001750390015600624,
      "loss": 0.2241,
      "step": 5280
    },
    {
      "epoch": 0.37511079595816343,
      "grad_norm": 1.0004801750183105,
      "learning_rate": 0.00017499172694180495,
      "loss": 0.2753,
      "step": 5290
    },
    {
      "epoch": 0.3758198900904095,
      "grad_norm": 0.9857953786849976,
      "learning_rate": 0.0001749444523235475,
      "loss": 0.2363,
      "step": 5300
    },
    {
      "epoch": 0.37652898422265557,
      "grad_norm": 1.2848613262176514,
      "learning_rate": 0.00017489717770529005,
      "loss": 0.28,
      "step": 5310
    },
    {
      "epoch": 0.37723807835490164,
      "grad_norm": 0.8774682879447937,
      "learning_rate": 0.00017484990308703257,
      "loss": 0.2448,
      "step": 5320
    },
    {
      "epoch": 0.37794717248714765,
      "grad_norm": 0.7127108573913574,
      "learning_rate": 0.00017480262846877512,
      "loss": 0.2387,
      "step": 5330
    },
    {
      "epoch": 0.3786562666193937,
      "grad_norm": 0.9434943199157715,
      "learning_rate": 0.00017475535385051767,
      "loss": 0.2699,
      "step": 5340
    },
    {
      "epoch": 0.3793653607516398,
      "grad_norm": 1.9771902561187744,
      "learning_rate": 0.0001747080792322602,
      "loss": 0.2759,
      "step": 5350
    },
    {
      "epoch": 0.38007445488388586,
      "grad_norm": 1.9308669567108154,
      "learning_rate": 0.00017466080461400277,
      "loss": 0.2757,
      "step": 5360
    },
    {
      "epoch": 0.3807835490161319,
      "grad_norm": 1.6934961080551147,
      "learning_rate": 0.0001746135299957453,
      "loss": 0.2589,
      "step": 5370
    },
    {
      "epoch": 0.38149264314837794,
      "grad_norm": 0.8937337398529053,
      "learning_rate": 0.00017456625537748784,
      "loss": 0.281,
      "step": 5380
    },
    {
      "epoch": 0.382201737280624,
      "grad_norm": 0.7048524618148804,
      "learning_rate": 0.0001745189807592304,
      "loss": 0.2539,
      "step": 5390
    },
    {
      "epoch": 0.3829108314128701,
      "grad_norm": 0.8076632618904114,
      "learning_rate": 0.00017447170614097291,
      "loss": 0.2347,
      "step": 5400
    },
    {
      "epoch": 0.3836199255451161,
      "grad_norm": 1.0081703662872314,
      "learning_rate": 0.00017442443152271546,
      "loss": 0.2494,
      "step": 5410
    },
    {
      "epoch": 0.38432901967736216,
      "grad_norm": 0.9286239743232727,
      "learning_rate": 0.000174377156904458,
      "loss": 0.3013,
      "step": 5420
    },
    {
      "epoch": 0.38503811380960823,
      "grad_norm": 1.341783046722412,
      "learning_rate": 0.00017432988228620056,
      "loss": 0.2708,
      "step": 5430
    },
    {
      "epoch": 0.3857472079418543,
      "grad_norm": 1.1101655960083008,
      "learning_rate": 0.0001742826076679431,
      "loss": 0.2603,
      "step": 5440
    },
    {
      "epoch": 0.3864563020741003,
      "grad_norm": 1.012560248374939,
      "learning_rate": 0.0001742353330496856,
      "loss": 0.2158,
      "step": 5450
    },
    {
      "epoch": 0.3871653962063464,
      "grad_norm": 0.9700663685798645,
      "learning_rate": 0.0001741880584314282,
      "loss": 0.2541,
      "step": 5460
    },
    {
      "epoch": 0.38787449033859245,
      "grad_norm": 1.3879494667053223,
      "learning_rate": 0.0001741407838131707,
      "loss": 0.2581,
      "step": 5470
    },
    {
      "epoch": 0.3885835844708385,
      "grad_norm": 2.3038341999053955,
      "learning_rate": 0.00017409350919491326,
      "loss": 0.2415,
      "step": 5480
    },
    {
      "epoch": 0.3892926786030846,
      "grad_norm": 0.9465687274932861,
      "learning_rate": 0.0001740462345766558,
      "loss": 0.2462,
      "step": 5490
    },
    {
      "epoch": 0.3900017727353306,
      "grad_norm": 0.7240727543830872,
      "learning_rate": 0.00017399895995839833,
      "loss": 0.2525,
      "step": 5500
    },
    {
      "epoch": 0.39071086686757667,
      "grad_norm": 2.810380220413208,
      "learning_rate": 0.00017395168534014088,
      "loss": 0.2242,
      "step": 5510
    },
    {
      "epoch": 0.39141996099982274,
      "grad_norm": 1.0548253059387207,
      "learning_rate": 0.00017390441072188343,
      "loss": 0.2744,
      "step": 5520
    },
    {
      "epoch": 0.3921290551320688,
      "grad_norm": 0.6596855521202087,
      "learning_rate": 0.00017385713610362598,
      "loss": 0.2621,
      "step": 5530
    },
    {
      "epoch": 0.3928381492643148,
      "grad_norm": 0.839509904384613,
      "learning_rate": 0.0001738098614853685,
      "loss": 0.2447,
      "step": 5540
    },
    {
      "epoch": 0.3935472433965609,
      "grad_norm": 1.6293681859970093,
      "learning_rate": 0.00017376258686711105,
      "loss": 0.2427,
      "step": 5550
    },
    {
      "epoch": 0.39425633752880695,
      "grad_norm": 0.840893030166626,
      "learning_rate": 0.0001737153122488536,
      "loss": 0.2587,
      "step": 5560
    },
    {
      "epoch": 0.394965431661053,
      "grad_norm": 2.5027997493743896,
      "learning_rate": 0.00017366803763059613,
      "loss": 0.2726,
      "step": 5570
    },
    {
      "epoch": 0.39567452579329904,
      "grad_norm": 0.5841540098190308,
      "learning_rate": 0.0001736207630123387,
      "loss": 0.2454,
      "step": 5580
    },
    {
      "epoch": 0.3963836199255451,
      "grad_norm": 1.1636074781417847,
      "learning_rate": 0.00017357348839408123,
      "loss": 0.2553,
      "step": 5590
    },
    {
      "epoch": 0.3970927140577912,
      "grad_norm": 1.2045520544052124,
      "learning_rate": 0.00017352621377582378,
      "loss": 0.2499,
      "step": 5600
    },
    {
      "epoch": 0.39780180819003724,
      "grad_norm": 0.5764225721359253,
      "learning_rate": 0.0001734789391575663,
      "loss": 0.2377,
      "step": 5610
    },
    {
      "epoch": 0.39851090232228326,
      "grad_norm": 1.1490821838378906,
      "learning_rate": 0.00017343166453930885,
      "loss": 0.2251,
      "step": 5620
    },
    {
      "epoch": 0.3992199964545293,
      "grad_norm": 1.2312207221984863,
      "learning_rate": 0.0001733843899210514,
      "loss": 0.2628,
      "step": 5630
    },
    {
      "epoch": 0.3999290905867754,
      "grad_norm": 1.0323127508163452,
      "learning_rate": 0.00017333711530279392,
      "loss": 0.2573,
      "step": 5640
    },
    {
      "epoch": 0.40063818471902146,
      "grad_norm": 0.7299883365631104,
      "learning_rate": 0.0001732898406845365,
      "loss": 0.2562,
      "step": 5650
    },
    {
      "epoch": 0.40134727885126753,
      "grad_norm": 0.8849194645881653,
      "learning_rate": 0.00017324256606627902,
      "loss": 0.2683,
      "step": 5660
    },
    {
      "epoch": 0.40205637298351354,
      "grad_norm": 1.132124662399292,
      "learning_rate": 0.00017319529144802154,
      "loss": 0.2687,
      "step": 5670
    },
    {
      "epoch": 0.4027654671157596,
      "grad_norm": 1.5984652042388916,
      "learning_rate": 0.00017314801682976412,
      "loss": 0.2747,
      "step": 5680
    },
    {
      "epoch": 0.4034745612480057,
      "grad_norm": 0.6744401454925537,
      "learning_rate": 0.00017310074221150664,
      "loss": 0.2504,
      "step": 5690
    },
    {
      "epoch": 0.40418365538025175,
      "grad_norm": 0.8301132917404175,
      "learning_rate": 0.0001730534675932492,
      "loss": 0.258,
      "step": 5700
    },
    {
      "epoch": 0.40489274951249776,
      "grad_norm": 1.7080788612365723,
      "learning_rate": 0.00017300619297499174,
      "loss": 0.2532,
      "step": 5710
    },
    {
      "epoch": 0.40560184364474383,
      "grad_norm": 0.6840587854385376,
      "learning_rate": 0.00017295891835673427,
      "loss": 0.2573,
      "step": 5720
    },
    {
      "epoch": 0.4063109377769899,
      "grad_norm": 1.1023008823394775,
      "learning_rate": 0.00017291164373847682,
      "loss": 0.2362,
      "step": 5730
    },
    {
      "epoch": 0.40702003190923597,
      "grad_norm": 1.9743859767913818,
      "learning_rate": 0.00017286436912021937,
      "loss": 0.2718,
      "step": 5740
    },
    {
      "epoch": 0.407729126041482,
      "grad_norm": 1.4118046760559082,
      "learning_rate": 0.00017281709450196192,
      "loss": 0.2573,
      "step": 5750
    },
    {
      "epoch": 0.40843822017372805,
      "grad_norm": 0.6830475926399231,
      "learning_rate": 0.00017276981988370444,
      "loss": 0.2369,
      "step": 5760
    },
    {
      "epoch": 0.4091473143059741,
      "grad_norm": 0.8913531303405762,
      "learning_rate": 0.000172722545265447,
      "loss": 0.2643,
      "step": 5770
    },
    {
      "epoch": 0.4098564084382202,
      "grad_norm": 0.9796054363250732,
      "learning_rate": 0.00017267527064718954,
      "loss": 0.2738,
      "step": 5780
    },
    {
      "epoch": 0.41056550257046626,
      "grad_norm": 0.6588484644889832,
      "learning_rate": 0.00017262799602893206,
      "loss": 0.2593,
      "step": 5790
    },
    {
      "epoch": 0.41127459670271227,
      "grad_norm": 1.1752339601516724,
      "learning_rate": 0.0001725807214106746,
      "loss": 0.2652,
      "step": 5800
    },
    {
      "epoch": 0.41198369083495834,
      "grad_norm": 1.4932814836502075,
      "learning_rate": 0.00017253344679241716,
      "loss": 0.2269,
      "step": 5810
    },
    {
      "epoch": 0.4126927849672044,
      "grad_norm": 1.022940993309021,
      "learning_rate": 0.0001724861721741597,
      "loss": 0.2622,
      "step": 5820
    },
    {
      "epoch": 0.4134018790994505,
      "grad_norm": 2.859977960586548,
      "learning_rate": 0.00017243889755590223,
      "loss": 0.2732,
      "step": 5830
    },
    {
      "epoch": 0.4141109732316965,
      "grad_norm": 0.9967555403709412,
      "learning_rate": 0.00017239162293764478,
      "loss": 0.2519,
      "step": 5840
    },
    {
      "epoch": 0.41482006736394256,
      "grad_norm": 0.8102248311042786,
      "learning_rate": 0.00017234434831938733,
      "loss": 0.2686,
      "step": 5850
    },
    {
      "epoch": 0.4155291614961886,
      "grad_norm": 2.3997676372528076,
      "learning_rate": 0.00017229707370112986,
      "loss": 0.23,
      "step": 5860
    },
    {
      "epoch": 0.4162382556284347,
      "grad_norm": 1.0198088884353638,
      "learning_rate": 0.00017224979908287243,
      "loss": 0.234,
      "step": 5870
    },
    {
      "epoch": 0.4169473497606807,
      "grad_norm": 1.3578240871429443,
      "learning_rate": 0.00017220252446461496,
      "loss": 0.2445,
      "step": 5880
    },
    {
      "epoch": 0.4176564438929268,
      "grad_norm": 0.7760536670684814,
      "learning_rate": 0.0001721552498463575,
      "loss": 0.2212,
      "step": 5890
    },
    {
      "epoch": 0.41836553802517285,
      "grad_norm": 0.4989224970340729,
      "learning_rate": 0.00017210797522810005,
      "loss": 0.233,
      "step": 5900
    },
    {
      "epoch": 0.4190746321574189,
      "grad_norm": 0.6850190758705139,
      "learning_rate": 0.00017206070060984258,
      "loss": 0.2666,
      "step": 5910
    },
    {
      "epoch": 0.4197837262896649,
      "grad_norm": 1.4388498067855835,
      "learning_rate": 0.00017201342599158513,
      "loss": 0.2509,
      "step": 5920
    },
    {
      "epoch": 0.420492820421911,
      "grad_norm": 2.474364757537842,
      "learning_rate": 0.00017196615137332768,
      "loss": 0.2404,
      "step": 5930
    },
    {
      "epoch": 0.42120191455415706,
      "grad_norm": 1.0709744691848755,
      "learning_rate": 0.0001719188767550702,
      "loss": 0.2437,
      "step": 5940
    },
    {
      "epoch": 0.42191100868640313,
      "grad_norm": 1.1002089977264404,
      "learning_rate": 0.00017187160213681275,
      "loss": 0.2588,
      "step": 5950
    },
    {
      "epoch": 0.4226201028186492,
      "grad_norm": 1.1528481245040894,
      "learning_rate": 0.0001718243275185553,
      "loss": 0.2918,
      "step": 5960
    },
    {
      "epoch": 0.4233291969508952,
      "grad_norm": 0.7407636046409607,
      "learning_rate": 0.00017177705290029785,
      "loss": 0.2244,
      "step": 5970
    },
    {
      "epoch": 0.4240382910831413,
      "grad_norm": 1.0164897441864014,
      "learning_rate": 0.00017172977828204037,
      "loss": 0.2573,
      "step": 5980
    },
    {
      "epoch": 0.42474738521538735,
      "grad_norm": 3.355989456176758,
      "learning_rate": 0.00017168250366378292,
      "loss": 0.3055,
      "step": 5990
    },
    {
      "epoch": 0.4254564793476334,
      "grad_norm": 0.7361748814582825,
      "learning_rate": 0.00017163522904552547,
      "loss": 0.2236,
      "step": 6000
    },
    {
      "epoch": 0.42616557347987943,
      "grad_norm": 1.062443494796753,
      "learning_rate": 0.000171587954427268,
      "loss": 0.2639,
      "step": 6010
    },
    {
      "epoch": 0.4268746676121255,
      "grad_norm": 1.910860538482666,
      "learning_rate": 0.00017154067980901054,
      "loss": 0.2528,
      "step": 6020
    },
    {
      "epoch": 0.42758376174437157,
      "grad_norm": 4.014237880706787,
      "learning_rate": 0.0001714934051907531,
      "loss": 0.2655,
      "step": 6030
    },
    {
      "epoch": 0.42829285587661764,
      "grad_norm": 1.277752161026001,
      "learning_rate": 0.00017144613057249564,
      "loss": 0.2899,
      "step": 6040
    },
    {
      "epoch": 0.42900195000886365,
      "grad_norm": 1.0102487802505493,
      "learning_rate": 0.00017139885595423817,
      "loss": 0.2519,
      "step": 6050
    },
    {
      "epoch": 0.4297110441411097,
      "grad_norm": 1.2665214538574219,
      "learning_rate": 0.00017135158133598072,
      "loss": 0.2658,
      "step": 6060
    },
    {
      "epoch": 0.4304201382733558,
      "grad_norm": 1.44843590259552,
      "learning_rate": 0.00017130430671772327,
      "loss": 0.292,
      "step": 6070
    },
    {
      "epoch": 0.43112923240560186,
      "grad_norm": 1.2197731733322144,
      "learning_rate": 0.0001712570320994658,
      "loss": 0.2836,
      "step": 6080
    },
    {
      "epoch": 0.43183832653784787,
      "grad_norm": 2.501739501953125,
      "learning_rate": 0.00017120975748120837,
      "loss": 0.3015,
      "step": 6090
    },
    {
      "epoch": 0.43254742067009394,
      "grad_norm": 1.3754851818084717,
      "learning_rate": 0.0001711624828629509,
      "loss": 0.304,
      "step": 6100
    },
    {
      "epoch": 0.43325651480234,
      "grad_norm": 3.78804612159729,
      "learning_rate": 0.00017111520824469344,
      "loss": 0.297,
      "step": 6110
    },
    {
      "epoch": 0.4339656089345861,
      "grad_norm": 3.7277348041534424,
      "learning_rate": 0.000171067933626436,
      "loss": 0.2552,
      "step": 6120
    },
    {
      "epoch": 0.43467470306683215,
      "grad_norm": 1.329595685005188,
      "learning_rate": 0.0001710206590081785,
      "loss": 0.2702,
      "step": 6130
    },
    {
      "epoch": 0.43538379719907816,
      "grad_norm": 1.3214659690856934,
      "learning_rate": 0.00017097338438992106,
      "loss": 0.2522,
      "step": 6140
    },
    {
      "epoch": 0.43609289133132423,
      "grad_norm": 1.0618484020233154,
      "learning_rate": 0.0001709261097716636,
      "loss": 0.2228,
      "step": 6150
    },
    {
      "epoch": 0.4368019854635703,
      "grad_norm": 1.4448744058609009,
      "learning_rate": 0.00017087883515340613,
      "loss": 0.2437,
      "step": 6160
    },
    {
      "epoch": 0.43751107959581637,
      "grad_norm": 1.6131631135940552,
      "learning_rate": 0.00017083156053514868,
      "loss": 0.2405,
      "step": 6170
    },
    {
      "epoch": 0.4382201737280624,
      "grad_norm": 0.7584605813026428,
      "learning_rate": 0.00017078428591689123,
      "loss": 0.2707,
      "step": 6180
    },
    {
      "epoch": 0.43892926786030845,
      "grad_norm": 0.9375883340835571,
      "learning_rate": 0.00017073701129863378,
      "loss": 0.3088,
      "step": 6190
    },
    {
      "epoch": 0.4396383619925545,
      "grad_norm": 0.7976655960083008,
      "learning_rate": 0.0001706897366803763,
      "loss": 0.3019,
      "step": 6200
    },
    {
      "epoch": 0.4403474561248006,
      "grad_norm": 1.1275012493133545,
      "learning_rate": 0.00017064246206211886,
      "loss": 0.2952,
      "step": 6210
    },
    {
      "epoch": 0.4410565502570466,
      "grad_norm": 1.1303091049194336,
      "learning_rate": 0.0001705951874438614,
      "loss": 0.2454,
      "step": 6220
    },
    {
      "epoch": 0.44176564438929267,
      "grad_norm": 1.4819543361663818,
      "learning_rate": 0.00017054791282560393,
      "loss": 0.2534,
      "step": 6230
    },
    {
      "epoch": 0.44247473852153874,
      "grad_norm": 0.7421197295188904,
      "learning_rate": 0.00017050063820734648,
      "loss": 0.2461,
      "step": 6240
    },
    {
      "epoch": 0.4431838326537848,
      "grad_norm": 2.7424254417419434,
      "learning_rate": 0.00017045336358908903,
      "loss": 0.2701,
      "step": 6250
    },
    {
      "epoch": 0.4438929267860309,
      "grad_norm": 1.5627772808074951,
      "learning_rate": 0.00017040608897083158,
      "loss": 0.2469,
      "step": 6260
    },
    {
      "epoch": 0.4446020209182769,
      "grad_norm": 0.7436526417732239,
      "learning_rate": 0.0001703588143525741,
      "loss": 0.2547,
      "step": 6270
    },
    {
      "epoch": 0.44531111505052295,
      "grad_norm": 2.548882484436035,
      "learning_rate": 0.00017031153973431665,
      "loss": 0.2579,
      "step": 6280
    },
    {
      "epoch": 0.446020209182769,
      "grad_norm": 0.6415879130363464,
      "learning_rate": 0.0001702642651160592,
      "loss": 0.2717,
      "step": 6290
    },
    {
      "epoch": 0.4467293033150151,
      "grad_norm": 0.7674150466918945,
      "learning_rate": 0.00017021699049780172,
      "loss": 0.26,
      "step": 6300
    },
    {
      "epoch": 0.4474383974472611,
      "grad_norm": 0.9428284168243408,
      "learning_rate": 0.0001701697158795443,
      "loss": 0.2636,
      "step": 6310
    },
    {
      "epoch": 0.4481474915795072,
      "grad_norm": 0.8236544728279114,
      "learning_rate": 0.00017012244126128682,
      "loss": 0.2553,
      "step": 6320
    },
    {
      "epoch": 0.44885658571175324,
      "grad_norm": 1.8904200792312622,
      "learning_rate": 0.00017007516664302937,
      "loss": 0.2523,
      "step": 6330
    },
    {
      "epoch": 0.4495656798439993,
      "grad_norm": 0.6876662969589233,
      "learning_rate": 0.00017002789202477192,
      "loss": 0.25,
      "step": 6340
    },
    {
      "epoch": 0.4502747739762453,
      "grad_norm": 0.9696351885795593,
      "learning_rate": 0.00016998061740651445,
      "loss": 0.2401,
      "step": 6350
    },
    {
      "epoch": 0.4509838681084914,
      "grad_norm": 2.413350820541382,
      "learning_rate": 0.000169933342788257,
      "loss": 0.2582,
      "step": 6360
    },
    {
      "epoch": 0.45169296224073746,
      "grad_norm": 0.9061289429664612,
      "learning_rate": 0.00016988606816999955,
      "loss": 0.2485,
      "step": 6370
    },
    {
      "epoch": 0.45240205637298353,
      "grad_norm": 2.8079168796539307,
      "learning_rate": 0.00016983879355174207,
      "loss": 0.2882,
      "step": 6380
    },
    {
      "epoch": 0.45311115050522954,
      "grad_norm": 1.0718858242034912,
      "learning_rate": 0.00016979151893348462,
      "loss": 0.239,
      "step": 6390
    },
    {
      "epoch": 0.4538202446374756,
      "grad_norm": 0.6156556606292725,
      "learning_rate": 0.00016974424431522714,
      "loss": 0.2561,
      "step": 6400
    },
    {
      "epoch": 0.4545293387697217,
      "grad_norm": 2.820035934448242,
      "learning_rate": 0.00016969696969696972,
      "loss": 0.2489,
      "step": 6410
    },
    {
      "epoch": 0.45523843290196775,
      "grad_norm": 2.029459238052368,
      "learning_rate": 0.00016964969507871224,
      "loss": 0.2877,
      "step": 6420
    },
    {
      "epoch": 0.4559475270342138,
      "grad_norm": 1.8757832050323486,
      "learning_rate": 0.0001696024204604548,
      "loss": 0.2414,
      "step": 6430
    },
    {
      "epoch": 0.45665662116645983,
      "grad_norm": 2.2313413619995117,
      "learning_rate": 0.00016955514584219734,
      "loss": 0.2977,
      "step": 6440
    },
    {
      "epoch": 0.4573657152987059,
      "grad_norm": 0.8519567251205444,
      "learning_rate": 0.00016950787122393986,
      "loss": 0.234,
      "step": 6450
    },
    {
      "epoch": 0.45807480943095197,
      "grad_norm": 1.3101049661636353,
      "learning_rate": 0.0001694605966056824,
      "loss": 0.2599,
      "step": 6460
    },
    {
      "epoch": 0.45878390356319804,
      "grad_norm": 1.5383423566818237,
      "learning_rate": 0.00016941332198742496,
      "loss": 0.2894,
      "step": 6470
    },
    {
      "epoch": 0.45949299769544405,
      "grad_norm": 1.3512133359909058,
      "learning_rate": 0.0001693660473691675,
      "loss": 0.2595,
      "step": 6480
    },
    {
      "epoch": 0.4602020918276901,
      "grad_norm": 1.6837900876998901,
      "learning_rate": 0.00016931877275091003,
      "loss": 0.2297,
      "step": 6490
    },
    {
      "epoch": 0.4609111859599362,
      "grad_norm": 0.47172480821609497,
      "learning_rate": 0.00016927149813265258,
      "loss": 0.272,
      "step": 6500
    },
    {
      "epoch": 0.46162028009218226,
      "grad_norm": 1.7066259384155273,
      "learning_rate": 0.00016922422351439513,
      "loss": 0.2506,
      "step": 6510
    },
    {
      "epoch": 0.46232937422442827,
      "grad_norm": 1.0492100715637207,
      "learning_rate": 0.00016917694889613766,
      "loss": 0.2307,
      "step": 6520
    },
    {
      "epoch": 0.46303846835667434,
      "grad_norm": 2.611589193344116,
      "learning_rate": 0.00016912967427788023,
      "loss": 0.2499,
      "step": 6530
    },
    {
      "epoch": 0.4637475624889204,
      "grad_norm": 1.4654256105422974,
      "learning_rate": 0.00016908239965962276,
      "loss": 0.2611,
      "step": 6540
    },
    {
      "epoch": 0.4644566566211665,
      "grad_norm": 1.3679450750350952,
      "learning_rate": 0.0001690351250413653,
      "loss": 0.2658,
      "step": 6550
    },
    {
      "epoch": 0.4651657507534125,
      "grad_norm": 0.6439323425292969,
      "learning_rate": 0.00016898785042310786,
      "loss": 0.2448,
      "step": 6560
    },
    {
      "epoch": 0.46587484488565856,
      "grad_norm": 0.8437386751174927,
      "learning_rate": 0.00016894057580485038,
      "loss": 0.2721,
      "step": 6570
    },
    {
      "epoch": 0.4665839390179046,
      "grad_norm": 1.1523281335830688,
      "learning_rate": 0.00016889330118659293,
      "loss": 0.2193,
      "step": 6580
    },
    {
      "epoch": 0.4672930331501507,
      "grad_norm": 3.09570574760437,
      "learning_rate": 0.00016884602656833545,
      "loss": 0.2922,
      "step": 6590
    },
    {
      "epoch": 0.46800212728239676,
      "grad_norm": 1.1284430027008057,
      "learning_rate": 0.00016879875195007803,
      "loss": 0.2283,
      "step": 6600
    },
    {
      "epoch": 0.4687112214146428,
      "grad_norm": 1.1468740701675415,
      "learning_rate": 0.00016875147733182055,
      "loss": 0.2695,
      "step": 6610
    },
    {
      "epoch": 0.46942031554688884,
      "grad_norm": 1.1148525476455688,
      "learning_rate": 0.00016870420271356307,
      "loss": 0.2311,
      "step": 6620
    },
    {
      "epoch": 0.4701294096791349,
      "grad_norm": 1.0618282556533813,
      "learning_rate": 0.00016865692809530565,
      "loss": 0.2406,
      "step": 6630
    },
    {
      "epoch": 0.470838503811381,
      "grad_norm": 1.4824823141098022,
      "learning_rate": 0.00016860965347704817,
      "loss": 0.2186,
      "step": 6640
    },
    {
      "epoch": 0.471547597943627,
      "grad_norm": 0.9032172560691833,
      "learning_rate": 0.00016856237885879072,
      "loss": 0.2828,
      "step": 6650
    },
    {
      "epoch": 0.47225669207587306,
      "grad_norm": 0.7320101261138916,
      "learning_rate": 0.00016851510424053327,
      "loss": 0.2634,
      "step": 6660
    },
    {
      "epoch": 0.47296578620811913,
      "grad_norm": 0.8509480953216553,
      "learning_rate": 0.0001684678296222758,
      "loss": 0.2536,
      "step": 6670
    },
    {
      "epoch": 0.4736748803403652,
      "grad_norm": 1.6297870874404907,
      "learning_rate": 0.00016842055500401835,
      "loss": 0.271,
      "step": 6680
    },
    {
      "epoch": 0.4743839744726112,
      "grad_norm": 1.3709266185760498,
      "learning_rate": 0.0001683732803857609,
      "loss": 0.2815,
      "step": 6690
    },
    {
      "epoch": 0.4750930686048573,
      "grad_norm": 1.7131437063217163,
      "learning_rate": 0.00016832600576750345,
      "loss": 0.2132,
      "step": 6700
    },
    {
      "epoch": 0.47580216273710335,
      "grad_norm": 0.850377082824707,
      "learning_rate": 0.00016827873114924597,
      "loss": 0.2603,
      "step": 6710
    },
    {
      "epoch": 0.4765112568693494,
      "grad_norm": 0.6577281951904297,
      "learning_rate": 0.00016823145653098852,
      "loss": 0.2808,
      "step": 6720
    },
    {
      "epoch": 0.4772203510015955,
      "grad_norm": 0.8522474765777588,
      "learning_rate": 0.00016818418191273107,
      "loss": 0.2316,
      "step": 6730
    },
    {
      "epoch": 0.4779294451338415,
      "grad_norm": 0.9720479249954224,
      "learning_rate": 0.0001681369072944736,
      "loss": 0.2617,
      "step": 6740
    },
    {
      "epoch": 0.47863853926608757,
      "grad_norm": 0.7141656875610352,
      "learning_rate": 0.00016808963267621617,
      "loss": 0.2476,
      "step": 6750
    },
    {
      "epoch": 0.47934763339833364,
      "grad_norm": 0.8279580473899841,
      "learning_rate": 0.0001680423580579587,
      "loss": 0.2362,
      "step": 6760
    },
    {
      "epoch": 0.4800567275305797,
      "grad_norm": 0.7764220833778381,
      "learning_rate": 0.00016799508343970124,
      "loss": 0.2617,
      "step": 6770
    },
    {
      "epoch": 0.4807658216628257,
      "grad_norm": 0.6378469467163086,
      "learning_rate": 0.00016794780882144376,
      "loss": 0.2623,
      "step": 6780
    },
    {
      "epoch": 0.4814749157950718,
      "grad_norm": 1.0445111989974976,
      "learning_rate": 0.0001679005342031863,
      "loss": 0.2507,
      "step": 6790
    },
    {
      "epoch": 0.48218400992731786,
      "grad_norm": 0.8651909828186035,
      "learning_rate": 0.00016785325958492886,
      "loss": 0.2379,
      "step": 6800
    },
    {
      "epoch": 0.4828931040595639,
      "grad_norm": 0.8180457353591919,
      "learning_rate": 0.00016780598496667139,
      "loss": 0.2361,
      "step": 6810
    },
    {
      "epoch": 0.48360219819180994,
      "grad_norm": 0.5391900539398193,
      "learning_rate": 0.00016775871034841396,
      "loss": 0.2152,
      "step": 6820
    },
    {
      "epoch": 0.484311292324056,
      "grad_norm": 1.358440637588501,
      "learning_rate": 0.00016771143573015649,
      "loss": 0.2543,
      "step": 6830
    },
    {
      "epoch": 0.4850203864563021,
      "grad_norm": 1.7858701944351196,
      "learning_rate": 0.000167664161111899,
      "loss": 0.2244,
      "step": 6840
    },
    {
      "epoch": 0.48572948058854815,
      "grad_norm": 0.6583902835845947,
      "learning_rate": 0.00016761688649364159,
      "loss": 0.2702,
      "step": 6850
    },
    {
      "epoch": 0.48643857472079416,
      "grad_norm": 1.7025516033172607,
      "learning_rate": 0.0001675696118753841,
      "loss": 0.271,
      "step": 6860
    },
    {
      "epoch": 0.4871476688530402,
      "grad_norm": 0.8297758102416992,
      "learning_rate": 0.00016752233725712666,
      "loss": 0.2911,
      "step": 6870
    },
    {
      "epoch": 0.4878567629852863,
      "grad_norm": 1.8176766633987427,
      "learning_rate": 0.0001674750626388692,
      "loss": 0.2458,
      "step": 6880
    },
    {
      "epoch": 0.48856585711753237,
      "grad_norm": 1.5186408758163452,
      "learning_rate": 0.00016742778802061173,
      "loss": 0.2579,
      "step": 6890
    },
    {
      "epoch": 0.48927495124977843,
      "grad_norm": 1.0587164163589478,
      "learning_rate": 0.00016738051340235428,
      "loss": 0.2689,
      "step": 6900
    },
    {
      "epoch": 0.48998404538202445,
      "grad_norm": 0.9929186701774597,
      "learning_rate": 0.00016733323878409683,
      "loss": 0.261,
      "step": 6910
    },
    {
      "epoch": 0.4906931395142705,
      "grad_norm": 0.9418182969093323,
      "learning_rate": 0.00016728596416583938,
      "loss": 0.2444,
      "step": 6920
    },
    {
      "epoch": 0.4914022336465166,
      "grad_norm": 1.20346200466156,
      "learning_rate": 0.0001672386895475819,
      "loss": 0.2748,
      "step": 6930
    },
    {
      "epoch": 0.49211132777876265,
      "grad_norm": 0.8081868290901184,
      "learning_rate": 0.00016719141492932445,
      "loss": 0.2751,
      "step": 6940
    },
    {
      "epoch": 0.49282042191100867,
      "grad_norm": 2.220123291015625,
      "learning_rate": 0.000167144140311067,
      "loss": 0.2745,
      "step": 6950
    },
    {
      "epoch": 0.49352951604325473,
      "grad_norm": 2.6226861476898193,
      "learning_rate": 0.00016709686569280953,
      "loss": 0.2514,
      "step": 6960
    },
    {
      "epoch": 0.4942386101755008,
      "grad_norm": 0.7320757508277893,
      "learning_rate": 0.0001670495910745521,
      "loss": 0.3038,
      "step": 6970
    },
    {
      "epoch": 0.49494770430774687,
      "grad_norm": 1.1052924394607544,
      "learning_rate": 0.00016700231645629462,
      "loss": 0.2643,
      "step": 6980
    },
    {
      "epoch": 0.4956567984399929,
      "grad_norm": 2.502704381942749,
      "learning_rate": 0.00016695504183803717,
      "loss": 0.26,
      "step": 6990
    },
    {
      "epoch": 0.49636589257223895,
      "grad_norm": 1.713691234588623,
      "learning_rate": 0.0001669077672197797,
      "loss": 0.2386,
      "step": 7000
    },
    {
      "epoch": 0.497074986704485,
      "grad_norm": 3.0573108196258545,
      "learning_rate": 0.00016686049260152225,
      "loss": 0.2379,
      "step": 7010
    },
    {
      "epoch": 0.4977840808367311,
      "grad_norm": 1.9621561765670776,
      "learning_rate": 0.0001668132179832648,
      "loss": 0.2439,
      "step": 7020
    },
    {
      "epoch": 0.4984931749689771,
      "grad_norm": 1.0582407712936401,
      "learning_rate": 0.00016676594336500732,
      "loss": 0.2605,
      "step": 7030
    },
    {
      "epoch": 0.4992022691012232,
      "grad_norm": 1.1034376621246338,
      "learning_rate": 0.0001667186687467499,
      "loss": 0.2505,
      "step": 7040
    },
    {
      "epoch": 0.49991136323346924,
      "grad_norm": 2.3984196186065674,
      "learning_rate": 0.00016667139412849242,
      "loss": 0.2524,
      "step": 7050
    },
    {
      "epoch": 0.5006204573657153,
      "grad_norm": 1.3256860971450806,
      "learning_rate": 0.00016662411951023494,
      "loss": 0.2626,
      "step": 7060
    },
    {
      "epoch": 0.5013295514979613,
      "grad_norm": 0.958122968673706,
      "learning_rate": 0.00016657684489197752,
      "loss": 0.2273,
      "step": 7070
    },
    {
      "epoch": 0.5020386456302074,
      "grad_norm": 1.2537730932235718,
      "learning_rate": 0.00016652957027372004,
      "loss": 0.2796,
      "step": 7080
    },
    {
      "epoch": 0.5027477397624535,
      "grad_norm": 2.910073757171631,
      "learning_rate": 0.0001664822956554626,
      "loss": 0.227,
      "step": 7090
    },
    {
      "epoch": 0.5034568338946995,
      "grad_norm": 3.5354621410369873,
      "learning_rate": 0.00016643502103720514,
      "loss": 0.2854,
      "step": 7100
    },
    {
      "epoch": 0.5041659280269456,
      "grad_norm": 0.6045902967453003,
      "learning_rate": 0.00016638774641894766,
      "loss": 0.2586,
      "step": 7110
    },
    {
      "epoch": 0.5048750221591917,
      "grad_norm": 1.476388692855835,
      "learning_rate": 0.00016634047180069021,
      "loss": 0.2789,
      "step": 7120
    },
    {
      "epoch": 0.5055841162914377,
      "grad_norm": 1.072054147720337,
      "learning_rate": 0.00016629319718243276,
      "loss": 0.2697,
      "step": 7130
    },
    {
      "epoch": 0.5062932104236837,
      "grad_norm": 1.2643845081329346,
      "learning_rate": 0.00016624592256417531,
      "loss": 0.2756,
      "step": 7140
    },
    {
      "epoch": 0.5070023045559298,
      "grad_norm": 1.2761386632919312,
      "learning_rate": 0.00016619864794591784,
      "loss": 0.228,
      "step": 7150
    },
    {
      "epoch": 0.5077113986881758,
      "grad_norm": 2.1664671897888184,
      "learning_rate": 0.0001661513733276604,
      "loss": 0.2622,
      "step": 7160
    },
    {
      "epoch": 0.5084204928204219,
      "grad_norm": 0.7543145418167114,
      "learning_rate": 0.00016610409870940294,
      "loss": 0.2837,
      "step": 7170
    },
    {
      "epoch": 0.509129586952668,
      "grad_norm": 1.3168179988861084,
      "learning_rate": 0.00016605682409114546,
      "loss": 0.2351,
      "step": 7180
    },
    {
      "epoch": 0.509838681084914,
      "grad_norm": 0.7117513418197632,
      "learning_rate": 0.000166009549472888,
      "loss": 0.2361,
      "step": 7190
    },
    {
      "epoch": 0.5105477752171601,
      "grad_norm": 1.7741222381591797,
      "learning_rate": 0.00016596227485463056,
      "loss": 0.2866,
      "step": 7200
    },
    {
      "epoch": 0.5112568693494062,
      "grad_norm": 0.7003178000450134,
      "learning_rate": 0.0001659150002363731,
      "loss": 0.2627,
      "step": 7210
    },
    {
      "epoch": 0.5119659634816522,
      "grad_norm": 1.5204545259475708,
      "learning_rate": 0.00016586772561811563,
      "loss": 0.2466,
      "step": 7220
    },
    {
      "epoch": 0.5126750576138982,
      "grad_norm": 2.3138251304626465,
      "learning_rate": 0.00016582045099985818,
      "loss": 0.2663,
      "step": 7230
    },
    {
      "epoch": 0.5133841517461443,
      "grad_norm": 0.8860293030738831,
      "learning_rate": 0.00016577317638160073,
      "loss": 0.2518,
      "step": 7240
    },
    {
      "epoch": 0.5140932458783903,
      "grad_norm": 1.211175799369812,
      "learning_rate": 0.00016572590176334325,
      "loss": 0.241,
      "step": 7250
    },
    {
      "epoch": 0.5148023400106364,
      "grad_norm": 0.7270066738128662,
      "learning_rate": 0.00016567862714508583,
      "loss": 0.2028,
      "step": 7260
    },
    {
      "epoch": 0.5155114341428825,
      "grad_norm": 0.6584154963493347,
      "learning_rate": 0.00016563135252682835,
      "loss": 0.2405,
      "step": 7270
    },
    {
      "epoch": 0.5162205282751285,
      "grad_norm": 0.79692143201828,
      "learning_rate": 0.00016558407790857088,
      "loss": 0.2826,
      "step": 7280
    },
    {
      "epoch": 0.5169296224073746,
      "grad_norm": 2.029597759246826,
      "learning_rate": 0.00016553680329031345,
      "loss": 0.2363,
      "step": 7290
    },
    {
      "epoch": 0.5176387165396207,
      "grad_norm": 0.6934607028961182,
      "learning_rate": 0.00016548952867205598,
      "loss": 0.2437,
      "step": 7300
    },
    {
      "epoch": 0.5183478106718666,
      "grad_norm": 1.0194120407104492,
      "learning_rate": 0.00016544225405379853,
      "loss": 0.268,
      "step": 7310
    },
    {
      "epoch": 0.5190569048041127,
      "grad_norm": 0.8056464791297913,
      "learning_rate": 0.00016539497943554108,
      "loss": 0.2905,
      "step": 7320
    },
    {
      "epoch": 0.5197659989363588,
      "grad_norm": 0.7465308308601379,
      "learning_rate": 0.0001653477048172836,
      "loss": 0.2374,
      "step": 7330
    },
    {
      "epoch": 0.5204750930686048,
      "grad_norm": 0.8538028597831726,
      "learning_rate": 0.00016530043019902615,
      "loss": 0.2737,
      "step": 7340
    },
    {
      "epoch": 0.5211841872008509,
      "grad_norm": 0.8518450260162354,
      "learning_rate": 0.0001652531555807687,
      "loss": 0.255,
      "step": 7350
    },
    {
      "epoch": 0.521893281333097,
      "grad_norm": 0.9073304533958435,
      "learning_rate": 0.00016520588096251125,
      "loss": 0.2907,
      "step": 7360
    },
    {
      "epoch": 0.522602375465343,
      "grad_norm": 0.8067882061004639,
      "learning_rate": 0.00016515860634425377,
      "loss": 0.2465,
      "step": 7370
    },
    {
      "epoch": 0.5233114695975891,
      "grad_norm": 0.7012792229652405,
      "learning_rate": 0.00016511133172599632,
      "loss": 0.2685,
      "step": 7380
    },
    {
      "epoch": 0.5240205637298352,
      "grad_norm": 0.6323564648628235,
      "learning_rate": 0.00016506405710773887,
      "loss": 0.2554,
      "step": 7390
    },
    {
      "epoch": 0.5247296578620811,
      "grad_norm": 0.8951399922370911,
      "learning_rate": 0.0001650167824894814,
      "loss": 0.276,
      "step": 7400
    },
    {
      "epoch": 0.5254387519943272,
      "grad_norm": 0.9425142407417297,
      "learning_rate": 0.00016496950787122394,
      "loss": 0.2197,
      "step": 7410
    },
    {
      "epoch": 0.5261478461265733,
      "grad_norm": 0.9545151591300964,
      "learning_rate": 0.0001649222332529665,
      "loss": 0.2509,
      "step": 7420
    },
    {
      "epoch": 0.5268569402588194,
      "grad_norm": 0.8889768719673157,
      "learning_rate": 0.00016487495863470904,
      "loss": 0.2777,
      "step": 7430
    },
    {
      "epoch": 0.5275660343910654,
      "grad_norm": 1.0053818225860596,
      "learning_rate": 0.00016482768401645157,
      "loss": 0.256,
      "step": 7440
    },
    {
      "epoch": 0.5282751285233115,
      "grad_norm": 1.1070882081985474,
      "learning_rate": 0.00016478040939819412,
      "loss": 0.2448,
      "step": 7450
    },
    {
      "epoch": 0.5289842226555576,
      "grad_norm": 1.0340721607208252,
      "learning_rate": 0.00016473313477993667,
      "loss": 0.2494,
      "step": 7460
    },
    {
      "epoch": 0.5296933167878036,
      "grad_norm": 0.9629289507865906,
      "learning_rate": 0.0001646858601616792,
      "loss": 0.2452,
      "step": 7470
    },
    {
      "epoch": 0.5304024109200496,
      "grad_norm": 0.7250003814697266,
      "learning_rate": 0.00016463858554342176,
      "loss": 0.2523,
      "step": 7480
    },
    {
      "epoch": 0.5311115050522957,
      "grad_norm": 0.9653086066246033,
      "learning_rate": 0.0001645913109251643,
      "loss": 0.2534,
      "step": 7490
    },
    {
      "epoch": 0.5318205991845417,
      "grad_norm": 0.7627031803131104,
      "learning_rate": 0.0001645440363069068,
      "loss": 0.2517,
      "step": 7500
    },
    {
      "epoch": 0.5325296933167878,
      "grad_norm": 1.001596212387085,
      "learning_rate": 0.0001644967616886494,
      "loss": 0.2447,
      "step": 7510
    },
    {
      "epoch": 0.5332387874490339,
      "grad_norm": 2.174307346343994,
      "learning_rate": 0.0001644494870703919,
      "loss": 0.2507,
      "step": 7520
    },
    {
      "epoch": 0.5339478815812799,
      "grad_norm": 1.0716511011123657,
      "learning_rate": 0.00016440221245213446,
      "loss": 0.2361,
      "step": 7530
    },
    {
      "epoch": 0.534656975713526,
      "grad_norm": 1.358177900314331,
      "learning_rate": 0.000164354937833877,
      "loss": 0.2456,
      "step": 7540
    },
    {
      "epoch": 0.5353660698457721,
      "grad_norm": 1.106594204902649,
      "learning_rate": 0.00016430766321561953,
      "loss": 0.227,
      "step": 7550
    },
    {
      "epoch": 0.5360751639780181,
      "grad_norm": 0.6548940539360046,
      "learning_rate": 0.00016426038859736208,
      "loss": 0.2168,
      "step": 7560
    },
    {
      "epoch": 0.5367842581102641,
      "grad_norm": 1.9738903045654297,
      "learning_rate": 0.0001642131139791046,
      "loss": 0.2498,
      "step": 7570
    },
    {
      "epoch": 0.5374933522425102,
      "grad_norm": 2.774901866912842,
      "learning_rate": 0.00016416583936084718,
      "loss": 0.2259,
      "step": 7580
    },
    {
      "epoch": 0.5382024463747562,
      "grad_norm": 2.158940553665161,
      "learning_rate": 0.0001641185647425897,
      "loss": 0.2712,
      "step": 7590
    },
    {
      "epoch": 0.5389115405070023,
      "grad_norm": 0.90738445520401,
      "learning_rate": 0.00016407129012433225,
      "loss": 0.2701,
      "step": 7600
    },
    {
      "epoch": 0.5396206346392484,
      "grad_norm": 1.0838743448257446,
      "learning_rate": 0.0001640240155060748,
      "loss": 0.2471,
      "step": 7610
    },
    {
      "epoch": 0.5403297287714944,
      "grad_norm": 1.262906551361084,
      "learning_rate": 0.00016397674088781733,
      "loss": 0.2472,
      "step": 7620
    },
    {
      "epoch": 0.5410388229037405,
      "grad_norm": 1.0233334302902222,
      "learning_rate": 0.00016392946626955988,
      "loss": 0.2325,
      "step": 7630
    },
    {
      "epoch": 0.5417479170359866,
      "grad_norm": 0.9332904815673828,
      "learning_rate": 0.00016388219165130243,
      "loss": 0.265,
      "step": 7640
    },
    {
      "epoch": 0.5424570111682325,
      "grad_norm": 1.9027827978134155,
      "learning_rate": 0.00016383491703304498,
      "loss": 0.2387,
      "step": 7650
    },
    {
      "epoch": 0.5431661053004786,
      "grad_norm": 1.7373439073562622,
      "learning_rate": 0.0001637876424147875,
      "loss": 0.2611,
      "step": 7660
    },
    {
      "epoch": 0.5438751994327247,
      "grad_norm": 1.3075530529022217,
      "learning_rate": 0.00016374036779653005,
      "loss": 0.2518,
      "step": 7670
    },
    {
      "epoch": 0.5445842935649707,
      "grad_norm": 1.9527868032455444,
      "learning_rate": 0.0001636930931782726,
      "loss": 0.2792,
      "step": 7680
    },
    {
      "epoch": 0.5452933876972168,
      "grad_norm": 1.9326646327972412,
      "learning_rate": 0.00016364581856001512,
      "loss": 0.2712,
      "step": 7690
    },
    {
      "epoch": 0.5460024818294629,
      "grad_norm": 1.360425353050232,
      "learning_rate": 0.0001635985439417577,
      "loss": 0.2517,
      "step": 7700
    },
    {
      "epoch": 0.5467115759617089,
      "grad_norm": 1.2529093027114868,
      "learning_rate": 0.00016355126932350022,
      "loss": 0.2611,
      "step": 7710
    },
    {
      "epoch": 0.547420670093955,
      "grad_norm": 1.600210189819336,
      "learning_rate": 0.00016350399470524274,
      "loss": 0.2431,
      "step": 7720
    },
    {
      "epoch": 0.5481297642262011,
      "grad_norm": 1.8956267833709717,
      "learning_rate": 0.00016345672008698532,
      "loss": 0.2561,
      "step": 7730
    },
    {
      "epoch": 0.548838858358447,
      "grad_norm": 1.8107763528823853,
      "learning_rate": 0.00016340944546872784,
      "loss": 0.2833,
      "step": 7740
    },
    {
      "epoch": 0.5495479524906931,
      "grad_norm": 1.4607990980148315,
      "learning_rate": 0.0001633621708504704,
      "loss": 0.2956,
      "step": 7750
    },
    {
      "epoch": 0.5502570466229392,
      "grad_norm": 1.47772216796875,
      "learning_rate": 0.00016331489623221292,
      "loss": 0.2657,
      "step": 7760
    },
    {
      "epoch": 0.5509661407551852,
      "grad_norm": 1.3276877403259277,
      "learning_rate": 0.00016326762161395547,
      "loss": 0.2557,
      "step": 7770
    },
    {
      "epoch": 0.5516752348874313,
      "grad_norm": 0.7766024470329285,
      "learning_rate": 0.00016322034699569802,
      "loss": 0.2573,
      "step": 7780
    },
    {
      "epoch": 0.5523843290196774,
      "grad_norm": 0.670318067073822,
      "learning_rate": 0.00016317307237744054,
      "loss": 0.244,
      "step": 7790
    },
    {
      "epoch": 0.5530934231519234,
      "grad_norm": 1.4431226253509521,
      "learning_rate": 0.00016312579775918312,
      "loss": 0.2544,
      "step": 7800
    },
    {
      "epoch": 0.5538025172841695,
      "grad_norm": 2.172637701034546,
      "learning_rate": 0.00016307852314092564,
      "loss": 0.2586,
      "step": 7810
    },
    {
      "epoch": 0.5545116114164156,
      "grad_norm": 0.9402366876602173,
      "learning_rate": 0.0001630312485226682,
      "loss": 0.2462,
      "step": 7820
    },
    {
      "epoch": 0.5552207055486615,
      "grad_norm": 1.0714123249053955,
      "learning_rate": 0.00016298397390441074,
      "loss": 0.2563,
      "step": 7830
    },
    {
      "epoch": 0.5559297996809076,
      "grad_norm": 2.8969969749450684,
      "learning_rate": 0.00016293669928615326,
      "loss": 0.2663,
      "step": 7840
    },
    {
      "epoch": 0.5566388938131537,
      "grad_norm": 0.614098846912384,
      "learning_rate": 0.0001628894246678958,
      "loss": 0.2467,
      "step": 7850
    },
    {
      "epoch": 0.5573479879453997,
      "grad_norm": 1.0816274881362915,
      "learning_rate": 0.00016284215004963836,
      "loss": 0.2608,
      "step": 7860
    },
    {
      "epoch": 0.5580570820776458,
      "grad_norm": 1.0591957569122314,
      "learning_rate": 0.0001627948754313809,
      "loss": 0.2358,
      "step": 7870
    },
    {
      "epoch": 0.5587661762098919,
      "grad_norm": 0.5983183979988098,
      "learning_rate": 0.00016274760081312343,
      "loss": 0.251,
      "step": 7880
    },
    {
      "epoch": 0.559475270342138,
      "grad_norm": 0.9078594446182251,
      "learning_rate": 0.00016270032619486598,
      "loss": 0.2803,
      "step": 7890
    },
    {
      "epoch": 0.560184364474384,
      "grad_norm": 0.7971687316894531,
      "learning_rate": 0.00016265305157660853,
      "loss": 0.2659,
      "step": 7900
    },
    {
      "epoch": 0.56089345860663,
      "grad_norm": 1.3825945854187012,
      "learning_rate": 0.00016260577695835106,
      "loss": 0.2462,
      "step": 7910
    },
    {
      "epoch": 0.561602552738876,
      "grad_norm": 1.28517484664917,
      "learning_rate": 0.00016255850234009363,
      "loss": 0.2586,
      "step": 7920
    },
    {
      "epoch": 0.5623116468711221,
      "grad_norm": 1.3349727392196655,
      "learning_rate": 0.00016251122772183616,
      "loss": 0.2595,
      "step": 7930
    },
    {
      "epoch": 0.5630207410033682,
      "grad_norm": 1.9308587312698364,
      "learning_rate": 0.0001624639531035787,
      "loss": 0.2431,
      "step": 7940
    },
    {
      "epoch": 0.5637298351356143,
      "grad_norm": 0.9199498295783997,
      "learning_rate": 0.00016241667848532126,
      "loss": 0.2363,
      "step": 7950
    },
    {
      "epoch": 0.5644389292678603,
      "grad_norm": 0.7073503136634827,
      "learning_rate": 0.00016236940386706378,
      "loss": 0.278,
      "step": 7960
    },
    {
      "epoch": 0.5651480234001064,
      "grad_norm": 1.2067755460739136,
      "learning_rate": 0.00016232212924880633,
      "loss": 0.2623,
      "step": 7970
    },
    {
      "epoch": 0.5658571175323525,
      "grad_norm": 1.135233998298645,
      "learning_rate": 0.00016227485463054885,
      "loss": 0.2231,
      "step": 7980
    },
    {
      "epoch": 0.5665662116645985,
      "grad_norm": 1.2495979070663452,
      "learning_rate": 0.0001622275800122914,
      "loss": 0.2476,
      "step": 7990
    },
    {
      "epoch": 0.5672753057968445,
      "grad_norm": 1.0804383754730225,
      "learning_rate": 0.00016218030539403395,
      "loss": 0.2702,
      "step": 8000
    },
    {
      "epoch": 0.5679843999290906,
      "grad_norm": 0.9781856536865234,
      "learning_rate": 0.00016213303077577647,
      "loss": 0.2535,
      "step": 8010
    },
    {
      "epoch": 0.5686934940613366,
      "grad_norm": 1.10423743724823,
      "learning_rate": 0.00016208575615751905,
      "loss": 0.2437,
      "step": 8020
    },
    {
      "epoch": 0.5694025881935827,
      "grad_norm": 0.5624316334724426,
      "learning_rate": 0.00016203848153926157,
      "loss": 0.2543,
      "step": 8030
    },
    {
      "epoch": 0.5701116823258288,
      "grad_norm": 1.3715236186981201,
      "learning_rate": 0.00016199120692100412,
      "loss": 0.2528,
      "step": 8040
    },
    {
      "epoch": 0.5708207764580748,
      "grad_norm": 1.3065829277038574,
      "learning_rate": 0.00016194393230274667,
      "loss": 0.2523,
      "step": 8050
    },
    {
      "epoch": 0.5715298705903209,
      "grad_norm": 1.0782352685928345,
      "learning_rate": 0.0001618966576844892,
      "loss": 0.2188,
      "step": 8060
    },
    {
      "epoch": 0.572238964722567,
      "grad_norm": 0.945218026638031,
      "learning_rate": 0.00016184938306623175,
      "loss": 0.2444,
      "step": 8070
    },
    {
      "epoch": 0.5729480588548129,
      "grad_norm": 1.1504586935043335,
      "learning_rate": 0.0001618021084479743,
      "loss": 0.2335,
      "step": 8080
    },
    {
      "epoch": 0.573657152987059,
      "grad_norm": 0.7114772796630859,
      "learning_rate": 0.00016175483382971684,
      "loss": 0.2457,
      "step": 8090
    },
    {
      "epoch": 0.5743662471193051,
      "grad_norm": 1.6439939737319946,
      "learning_rate": 0.00016170755921145937,
      "loss": 0.2911,
      "step": 8100
    },
    {
      "epoch": 0.5750753412515511,
      "grad_norm": 1.1493864059448242,
      "learning_rate": 0.00016166028459320192,
      "loss": 0.2741,
      "step": 8110
    },
    {
      "epoch": 0.5757844353837972,
      "grad_norm": 1.0015968084335327,
      "learning_rate": 0.00016161300997494447,
      "loss": 0.248,
      "step": 8120
    },
    {
      "epoch": 0.5764935295160433,
      "grad_norm": 1.5444822311401367,
      "learning_rate": 0.000161565735356687,
      "loss": 0.26,
      "step": 8130
    },
    {
      "epoch": 0.5772026236482893,
      "grad_norm": 1.0215188264846802,
      "learning_rate": 0.00016151846073842957,
      "loss": 0.22,
      "step": 8140
    },
    {
      "epoch": 0.5779117177805354,
      "grad_norm": 1.6009471416473389,
      "learning_rate": 0.0001614711861201721,
      "loss": 0.248,
      "step": 8150
    },
    {
      "epoch": 0.5786208119127815,
      "grad_norm": 0.8085549473762512,
      "learning_rate": 0.00016142391150191464,
      "loss": 0.2444,
      "step": 8160
    },
    {
      "epoch": 0.5793299060450274,
      "grad_norm": 0.6801620721817017,
      "learning_rate": 0.00016137663688365716,
      "loss": 0.2546,
      "step": 8170
    },
    {
      "epoch": 0.5800390001772735,
      "grad_norm": 1.0831241607666016,
      "learning_rate": 0.0001613293622653997,
      "loss": 0.2526,
      "step": 8180
    },
    {
      "epoch": 0.5807480943095196,
      "grad_norm": 1.030112624168396,
      "learning_rate": 0.00016128208764714226,
      "loss": 0.2505,
      "step": 8190
    },
    {
      "epoch": 0.5814571884417656,
      "grad_norm": 1.2001454830169678,
      "learning_rate": 0.00016123481302888478,
      "loss": 0.2752,
      "step": 8200
    },
    {
      "epoch": 0.5821662825740117,
      "grad_norm": 1.0171995162963867,
      "learning_rate": 0.00016118753841062733,
      "loss": 0.2675,
      "step": 8210
    },
    {
      "epoch": 0.5828753767062578,
      "grad_norm": 1.0803228616714478,
      "learning_rate": 0.00016114026379236988,
      "loss": 0.2658,
      "step": 8220
    },
    {
      "epoch": 0.5835844708385038,
      "grad_norm": 1.3214032649993896,
      "learning_rate": 0.0001610929891741124,
      "loss": 0.2432,
      "step": 8230
    },
    {
      "epoch": 0.5842935649707499,
      "grad_norm": 3.416032075881958,
      "learning_rate": 0.00016104571455585498,
      "loss": 0.2844,
      "step": 8240
    },
    {
      "epoch": 0.5850026591029959,
      "grad_norm": 1.0645860433578491,
      "learning_rate": 0.0001609984399375975,
      "loss": 0.2214,
      "step": 8250
    },
    {
      "epoch": 0.5857117532352419,
      "grad_norm": 2.097548007965088,
      "learning_rate": 0.00016095116531934006,
      "loss": 0.2901,
      "step": 8260
    },
    {
      "epoch": 0.586420847367488,
      "grad_norm": 2.246852159500122,
      "learning_rate": 0.0001609038907010826,
      "loss": 0.2516,
      "step": 8270
    },
    {
      "epoch": 0.5871299414997341,
      "grad_norm": 4.243350505828857,
      "learning_rate": 0.00016085661608282513,
      "loss": 0.2517,
      "step": 8280
    },
    {
      "epoch": 0.5878390356319801,
      "grad_norm": 1.8558306694030762,
      "learning_rate": 0.00016080934146456768,
      "loss": 0.2531,
      "step": 8290
    },
    {
      "epoch": 0.5885481297642262,
      "grad_norm": 1.373728632926941,
      "learning_rate": 0.00016076206684631023,
      "loss": 0.2526,
      "step": 8300
    },
    {
      "epoch": 0.5892572238964723,
      "grad_norm": 0.8410623669624329,
      "learning_rate": 0.00016071479222805278,
      "loss": 0.2625,
      "step": 8310
    },
    {
      "epoch": 0.5899663180287184,
      "grad_norm": 2.3257381916046143,
      "learning_rate": 0.0001606675176097953,
      "loss": 0.248,
      "step": 8320
    },
    {
      "epoch": 0.5906754121609644,
      "grad_norm": 0.7636901140213013,
      "learning_rate": 0.00016062024299153785,
      "loss": 0.2441,
      "step": 8330
    },
    {
      "epoch": 0.5913845062932104,
      "grad_norm": 1.1456934213638306,
      "learning_rate": 0.0001605729683732804,
      "loss": 0.2845,
      "step": 8340
    },
    {
      "epoch": 0.5920936004254564,
      "grad_norm": 1.9802790880203247,
      "learning_rate": 0.00016052569375502292,
      "loss": 0.2632,
      "step": 8350
    },
    {
      "epoch": 0.5928026945577025,
      "grad_norm": 1.6161962747573853,
      "learning_rate": 0.00016047841913676547,
      "loss": 0.2585,
      "step": 8360
    },
    {
      "epoch": 0.5935117886899486,
      "grad_norm": 1.9167304039001465,
      "learning_rate": 0.00016043114451850802,
      "loss": 0.2865,
      "step": 8370
    },
    {
      "epoch": 0.5942208828221947,
      "grad_norm": 0.7926508188247681,
      "learning_rate": 0.00016038386990025057,
      "loss": 0.2507,
      "step": 8380
    },
    {
      "epoch": 0.5949299769544407,
      "grad_norm": 3.1164114475250244,
      "learning_rate": 0.0001603365952819931,
      "loss": 0.243,
      "step": 8390
    },
    {
      "epoch": 0.5956390710866868,
      "grad_norm": 0.6500614285469055,
      "learning_rate": 0.00016028932066373565,
      "loss": 0.259,
      "step": 8400
    },
    {
      "epoch": 0.5963481652189329,
      "grad_norm": 0.864072859287262,
      "learning_rate": 0.0001602420460454782,
      "loss": 0.2355,
      "step": 8410
    },
    {
      "epoch": 0.5970572593511788,
      "grad_norm": 0.8336852788925171,
      "learning_rate": 0.00016019477142722072,
      "loss": 0.2575,
      "step": 8420
    },
    {
      "epoch": 0.5977663534834249,
      "grad_norm": 0.9835830926895142,
      "learning_rate": 0.0001601474968089633,
      "loss": 0.2205,
      "step": 8430
    },
    {
      "epoch": 0.598475447615671,
      "grad_norm": 0.7903610467910767,
      "learning_rate": 0.00016010022219070582,
      "loss": 0.2551,
      "step": 8440
    },
    {
      "epoch": 0.599184541747917,
      "grad_norm": 0.5617767572402954,
      "learning_rate": 0.00016005294757244834,
      "loss": 0.2673,
      "step": 8450
    },
    {
      "epoch": 0.5998936358801631,
      "grad_norm": 1.257690668106079,
      "learning_rate": 0.00016000567295419092,
      "loss": 0.2686,
      "step": 8460
    },
    {
      "epoch": 0.6006027300124092,
      "grad_norm": 1.502864956855774,
      "learning_rate": 0.00015995839833593344,
      "loss": 0.2321,
      "step": 8470
    },
    {
      "epoch": 0.6013118241446552,
      "grad_norm": 0.8561099171638489,
      "learning_rate": 0.000159911123717676,
      "loss": 0.2633,
      "step": 8480
    },
    {
      "epoch": 0.6020209182769013,
      "grad_norm": 0.854617714881897,
      "learning_rate": 0.00015986384909941854,
      "loss": 0.224,
      "step": 8490
    },
    {
      "epoch": 0.6027300124091474,
      "grad_norm": 1.0530304908752441,
      "learning_rate": 0.00015981657448116106,
      "loss": 0.2454,
      "step": 8500
    },
    {
      "epoch": 0.6034391065413933,
      "grad_norm": 0.8925358653068542,
      "learning_rate": 0.0001597692998629036,
      "loss": 0.2509,
      "step": 8510
    },
    {
      "epoch": 0.6041482006736394,
      "grad_norm": 2.1224615573883057,
      "learning_rate": 0.00015972202524464616,
      "loss": 0.2372,
      "step": 8520
    },
    {
      "epoch": 0.6048572948058855,
      "grad_norm": 1.2394238710403442,
      "learning_rate": 0.0001596747506263887,
      "loss": 0.2375,
      "step": 8530
    },
    {
      "epoch": 0.6055663889381315,
      "grad_norm": 1.0931665897369385,
      "learning_rate": 0.00015962747600813124,
      "loss": 0.2299,
      "step": 8540
    },
    {
      "epoch": 0.6062754830703776,
      "grad_norm": 1.0494818687438965,
      "learning_rate": 0.00015958020138987379,
      "loss": 0.2629,
      "step": 8550
    },
    {
      "epoch": 0.6069845772026237,
      "grad_norm": 1.0387516021728516,
      "learning_rate": 0.00015953292677161634,
      "loss": 0.2694,
      "step": 8560
    },
    {
      "epoch": 0.6076936713348697,
      "grad_norm": 0.8195922374725342,
      "learning_rate": 0.00015948565215335886,
      "loss": 0.2547,
      "step": 8570
    },
    {
      "epoch": 0.6084027654671158,
      "grad_norm": 0.8249227404594421,
      "learning_rate": 0.0001594383775351014,
      "loss": 0.2547,
      "step": 8580
    },
    {
      "epoch": 0.6091118595993618,
      "grad_norm": 2.0360991954803467,
      "learning_rate": 0.00015939110291684396,
      "loss": 0.2695,
      "step": 8590
    },
    {
      "epoch": 0.6098209537316078,
      "grad_norm": 1.90156888961792,
      "learning_rate": 0.0001593438282985865,
      "loss": 0.2268,
      "step": 8600
    },
    {
      "epoch": 0.6105300478638539,
      "grad_norm": 1.5687994956970215,
      "learning_rate": 0.00015929655368032903,
      "loss": 0.2577,
      "step": 8610
    },
    {
      "epoch": 0.6112391419961,
      "grad_norm": 1.9237737655639648,
      "learning_rate": 0.00015924927906207158,
      "loss": 0.2389,
      "step": 8620
    },
    {
      "epoch": 0.611948236128346,
      "grad_norm": 0.782694935798645,
      "learning_rate": 0.00015920200444381413,
      "loss": 0.2586,
      "step": 8630
    },
    {
      "epoch": 0.6126573302605921,
      "grad_norm": 0.6474769711494446,
      "learning_rate": 0.00015915472982555665,
      "loss": 0.2701,
      "step": 8640
    },
    {
      "epoch": 0.6133664243928382,
      "grad_norm": 4.43462610244751,
      "learning_rate": 0.00015910745520729923,
      "loss": 0.2532,
      "step": 8650
    },
    {
      "epoch": 0.6140755185250842,
      "grad_norm": 1.6507762670516968,
      "learning_rate": 0.00015906018058904175,
      "loss": 0.2482,
      "step": 8660
    },
    {
      "epoch": 0.6147846126573303,
      "grad_norm": 1.5209572315216064,
      "learning_rate": 0.00015901290597078427,
      "loss": 0.2548,
      "step": 8670
    },
    {
      "epoch": 0.6154937067895763,
      "grad_norm": 1.6738775968551636,
      "learning_rate": 0.00015896563135252685,
      "loss": 0.2538,
      "step": 8680
    },
    {
      "epoch": 0.6162028009218223,
      "grad_norm": 0.9315844774246216,
      "learning_rate": 0.00015891835673426937,
      "loss": 0.2531,
      "step": 8690
    },
    {
      "epoch": 0.6169118950540684,
      "grad_norm": 1.3010996580123901,
      "learning_rate": 0.00015887108211601192,
      "loss": 0.2758,
      "step": 8700
    },
    {
      "epoch": 0.6176209891863145,
      "grad_norm": 1.5064902305603027,
      "learning_rate": 0.00015882380749775447,
      "loss": 0.2538,
      "step": 8710
    },
    {
      "epoch": 0.6183300833185605,
      "grad_norm": 1.6929645538330078,
      "learning_rate": 0.000158776532879497,
      "loss": 0.2716,
      "step": 8720
    },
    {
      "epoch": 0.6190391774508066,
      "grad_norm": 1.6688607931137085,
      "learning_rate": 0.00015872925826123955,
      "loss": 0.283,
      "step": 8730
    },
    {
      "epoch": 0.6197482715830527,
      "grad_norm": 1.0590131282806396,
      "learning_rate": 0.0001586819836429821,
      "loss": 0.2294,
      "step": 8740
    },
    {
      "epoch": 0.6204573657152987,
      "grad_norm": 0.7910115122795105,
      "learning_rate": 0.00015863470902472465,
      "loss": 0.2638,
      "step": 8750
    },
    {
      "epoch": 0.6211664598475448,
      "grad_norm": 1.3415393829345703,
      "learning_rate": 0.00015858743440646717,
      "loss": 0.2468,
      "step": 8760
    },
    {
      "epoch": 0.6218755539797908,
      "grad_norm": 1.956906795501709,
      "learning_rate": 0.00015854015978820972,
      "loss": 0.275,
      "step": 8770
    },
    {
      "epoch": 0.6225846481120368,
      "grad_norm": 1.2868926525115967,
      "learning_rate": 0.00015849288516995227,
      "loss": 0.2476,
      "step": 8780
    },
    {
      "epoch": 0.6232937422442829,
      "grad_norm": 0.860101580619812,
      "learning_rate": 0.0001584456105516948,
      "loss": 0.2566,
      "step": 8790
    },
    {
      "epoch": 0.624002836376529,
      "grad_norm": 2.918126106262207,
      "learning_rate": 0.00015839833593343734,
      "loss": 0.2623,
      "step": 8800
    },
    {
      "epoch": 0.624711930508775,
      "grad_norm": 1.4315489530563354,
      "learning_rate": 0.0001583510613151799,
      "loss": 0.2347,
      "step": 8810
    },
    {
      "epoch": 0.6254210246410211,
      "grad_norm": 1.445036768913269,
      "learning_rate": 0.00015830378669692244,
      "loss": 0.2632,
      "step": 8820
    },
    {
      "epoch": 0.6261301187732672,
      "grad_norm": 0.9578485488891602,
      "learning_rate": 0.00015825651207866496,
      "loss": 0.262,
      "step": 8830
    },
    {
      "epoch": 0.6268392129055133,
      "grad_norm": 0.8515826463699341,
      "learning_rate": 0.00015820923746040751,
      "loss": 0.2586,
      "step": 8840
    },
    {
      "epoch": 0.6275483070377592,
      "grad_norm": 1.6549214124679565,
      "learning_rate": 0.00015816196284215006,
      "loss": 0.2594,
      "step": 8850
    },
    {
      "epoch": 0.6282574011700053,
      "grad_norm": 2.8173201084136963,
      "learning_rate": 0.0001581146882238926,
      "loss": 0.247,
      "step": 8860
    },
    {
      "epoch": 0.6289664953022513,
      "grad_norm": 0.9445465803146362,
      "learning_rate": 0.00015806741360563516,
      "loss": 0.2192,
      "step": 8870
    },
    {
      "epoch": 0.6296755894344974,
      "grad_norm": 2.740072250366211,
      "learning_rate": 0.00015802013898737769,
      "loss": 0.246,
      "step": 8880
    },
    {
      "epoch": 0.6303846835667435,
      "grad_norm": 1.1831954717636108,
      "learning_rate": 0.0001579728643691202,
      "loss": 0.2281,
      "step": 8890
    },
    {
      "epoch": 0.6310937776989896,
      "grad_norm": 1.2776597738265991,
      "learning_rate": 0.00015792558975086279,
      "loss": 0.2266,
      "step": 8900
    },
    {
      "epoch": 0.6318028718312356,
      "grad_norm": 1.0308616161346436,
      "learning_rate": 0.0001578783151326053,
      "loss": 0.2566,
      "step": 8910
    },
    {
      "epoch": 0.6325119659634817,
      "grad_norm": 0.9011014103889465,
      "learning_rate": 0.00015783104051434786,
      "loss": 0.2613,
      "step": 8920
    },
    {
      "epoch": 0.6332210600957278,
      "grad_norm": 0.6295629739761353,
      "learning_rate": 0.0001577837658960904,
      "loss": 0.2259,
      "step": 8930
    },
    {
      "epoch": 0.6339301542279737,
      "grad_norm": 0.8980648517608643,
      "learning_rate": 0.00015773649127783293,
      "loss": 0.2396,
      "step": 8940
    },
    {
      "epoch": 0.6346392483602198,
      "grad_norm": 1.0367026329040527,
      "learning_rate": 0.00015768921665957548,
      "loss": 0.2564,
      "step": 8950
    },
    {
      "epoch": 0.6353483424924659,
      "grad_norm": 1.150984764099121,
      "learning_rate": 0.000157641942041318,
      "loss": 0.2581,
      "step": 8960
    },
    {
      "epoch": 0.6360574366247119,
      "grad_norm": 1.163185477256775,
      "learning_rate": 0.00015759466742306058,
      "loss": 0.2405,
      "step": 8970
    },
    {
      "epoch": 0.636766530756958,
      "grad_norm": 2.4058759212493896,
      "learning_rate": 0.0001575473928048031,
      "loss": 0.2674,
      "step": 8980
    },
    {
      "epoch": 0.6374756248892041,
      "grad_norm": 0.7973578572273254,
      "learning_rate": 0.00015750011818654565,
      "loss": 0.2447,
      "step": 8990
    },
    {
      "epoch": 0.6381847190214501,
      "grad_norm": 1.7048345804214478,
      "learning_rate": 0.0001574528435682882,
      "loss": 0.2409,
      "step": 9000
    },
    {
      "epoch": 0.6388938131536962,
      "grad_norm": 1.4008638858795166,
      "learning_rate": 0.00015740556895003073,
      "loss": 0.258,
      "step": 9010
    },
    {
      "epoch": 0.6396029072859422,
      "grad_norm": 1.06534743309021,
      "learning_rate": 0.00015735829433177328,
      "loss": 0.2535,
      "step": 9020
    },
    {
      "epoch": 0.6403120014181882,
      "grad_norm": 1.0517990589141846,
      "learning_rate": 0.00015731101971351583,
      "loss": 0.2648,
      "step": 9030
    },
    {
      "epoch": 0.6410210955504343,
      "grad_norm": 1.799678921699524,
      "learning_rate": 0.00015726374509525838,
      "loss": 0.2659,
      "step": 9040
    },
    {
      "epoch": 0.6417301896826804,
      "grad_norm": 1.1199603080749512,
      "learning_rate": 0.0001572164704770009,
      "loss": 0.2559,
      "step": 9050
    },
    {
      "epoch": 0.6424392838149264,
      "grad_norm": 1.003454566001892,
      "learning_rate": 0.00015716919585874345,
      "loss": 0.2651,
      "step": 9060
    },
    {
      "epoch": 0.6431483779471725,
      "grad_norm": 1.2899513244628906,
      "learning_rate": 0.000157121921240486,
      "loss": 0.2531,
      "step": 9070
    },
    {
      "epoch": 0.6438574720794186,
      "grad_norm": 1.3059637546539307,
      "learning_rate": 0.00015707464662222852,
      "loss": 0.2726,
      "step": 9080
    },
    {
      "epoch": 0.6445665662116646,
      "grad_norm": 1.4593687057495117,
      "learning_rate": 0.0001570273720039711,
      "loss": 0.2406,
      "step": 9090
    },
    {
      "epoch": 0.6452756603439107,
      "grad_norm": 1.8469843864440918,
      "learning_rate": 0.00015698009738571362,
      "loss": 0.2243,
      "step": 9100
    },
    {
      "epoch": 0.6459847544761567,
      "grad_norm": 1.3807282447814941,
      "learning_rate": 0.00015693282276745614,
      "loss": 0.2419,
      "step": 9110
    },
    {
      "epoch": 0.6466938486084027,
      "grad_norm": 2.1002357006073,
      "learning_rate": 0.00015688554814919872,
      "loss": 0.2841,
      "step": 9120
    },
    {
      "epoch": 0.6474029427406488,
      "grad_norm": 1.7763789892196655,
      "learning_rate": 0.00015683827353094124,
      "loss": 0.2553,
      "step": 9130
    },
    {
      "epoch": 0.6481120368728949,
      "grad_norm": 1.894671082496643,
      "learning_rate": 0.0001567909989126838,
      "loss": 0.2524,
      "step": 9140
    },
    {
      "epoch": 0.6488211310051409,
      "grad_norm": 0.9809976816177368,
      "learning_rate": 0.00015674372429442632,
      "loss": 0.2792,
      "step": 9150
    },
    {
      "epoch": 0.649530225137387,
      "grad_norm": 0.786459743976593,
      "learning_rate": 0.00015669644967616887,
      "loss": 0.263,
      "step": 9160
    },
    {
      "epoch": 0.6502393192696331,
      "grad_norm": 1.0137859582901,
      "learning_rate": 0.00015664917505791141,
      "loss": 0.2293,
      "step": 9170
    },
    {
      "epoch": 0.6509484134018791,
      "grad_norm": 1.0422255992889404,
      "learning_rate": 0.00015660190043965394,
      "loss": 0.2514,
      "step": 9180
    },
    {
      "epoch": 0.6516575075341251,
      "grad_norm": 0.7628852725028992,
      "learning_rate": 0.00015655462582139651,
      "loss": 0.2668,
      "step": 9190
    },
    {
      "epoch": 0.6523666016663712,
      "grad_norm": 1.3305232524871826,
      "learning_rate": 0.00015650735120313904,
      "loss": 0.2344,
      "step": 9200
    },
    {
      "epoch": 0.6530756957986172,
      "grad_norm": 0.7819294929504395,
      "learning_rate": 0.0001564600765848816,
      "loss": 0.2691,
      "step": 9210
    },
    {
      "epoch": 0.6537847899308633,
      "grad_norm": 0.8430687189102173,
      "learning_rate": 0.00015641280196662414,
      "loss": 0.2581,
      "step": 9220
    },
    {
      "epoch": 0.6544938840631094,
      "grad_norm": 1.060454249382019,
      "learning_rate": 0.00015636552734836666,
      "loss": 0.2135,
      "step": 9230
    },
    {
      "epoch": 0.6552029781953554,
      "grad_norm": 1.0413764715194702,
      "learning_rate": 0.0001563182527301092,
      "loss": 0.2594,
      "step": 9240
    },
    {
      "epoch": 0.6559120723276015,
      "grad_norm": 0.6734132766723633,
      "learning_rate": 0.00015627097811185176,
      "loss": 0.2428,
      "step": 9250
    },
    {
      "epoch": 0.6566211664598476,
      "grad_norm": 1.287505030632019,
      "learning_rate": 0.0001562237034935943,
      "loss": 0.2377,
      "step": 9260
    },
    {
      "epoch": 0.6573302605920937,
      "grad_norm": 1.5550557374954224,
      "learning_rate": 0.00015617642887533683,
      "loss": 0.2061,
      "step": 9270
    },
    {
      "epoch": 0.6580393547243396,
      "grad_norm": 1.0349037647247314,
      "learning_rate": 0.00015612915425707938,
      "loss": 0.2663,
      "step": 9280
    },
    {
      "epoch": 0.6587484488565857,
      "grad_norm": 0.8583434224128723,
      "learning_rate": 0.00015608187963882193,
      "loss": 0.2654,
      "step": 9290
    },
    {
      "epoch": 0.6594575429888317,
      "grad_norm": 1.3749260902404785,
      "learning_rate": 0.00015603460502056445,
      "loss": 0.2199,
      "step": 9300
    },
    {
      "epoch": 0.6601666371210778,
      "grad_norm": 0.7391523718833923,
      "learning_rate": 0.00015598733040230703,
      "loss": 0.2446,
      "step": 9310
    },
    {
      "epoch": 0.6608757312533239,
      "grad_norm": 0.8264405131340027,
      "learning_rate": 0.00015594005578404955,
      "loss": 0.2022,
      "step": 9320
    },
    {
      "epoch": 0.66158482538557,
      "grad_norm": 1.1162972450256348,
      "learning_rate": 0.00015589278116579208,
      "loss": 0.2468,
      "step": 9330
    },
    {
      "epoch": 0.662293919517816,
      "grad_norm": 1.806008219718933,
      "learning_rate": 0.00015584550654753463,
      "loss": 0.2534,
      "step": 9340
    },
    {
      "epoch": 0.6630030136500621,
      "grad_norm": 2.0136806964874268,
      "learning_rate": 0.00015579823192927718,
      "loss": 0.2727,
      "step": 9350
    },
    {
      "epoch": 0.663712107782308,
      "grad_norm": 1.951879858970642,
      "learning_rate": 0.00015575095731101973,
      "loss": 0.2578,
      "step": 9360
    },
    {
      "epoch": 0.6644212019145541,
      "grad_norm": 0.9857732057571411,
      "learning_rate": 0.00015570368269276225,
      "loss": 0.2732,
      "step": 9370
    },
    {
      "epoch": 0.6651302960468002,
      "grad_norm": 0.7849188446998596,
      "learning_rate": 0.0001556564080745048,
      "loss": 0.2699,
      "step": 9380
    },
    {
      "epoch": 0.6658393901790463,
      "grad_norm": 1.4661130905151367,
      "learning_rate": 0.00015560913345624735,
      "loss": 0.2584,
      "step": 9390
    },
    {
      "epoch": 0.6665484843112923,
      "grad_norm": 0.9016278982162476,
      "learning_rate": 0.00015556185883798987,
      "loss": 0.2432,
      "step": 9400
    },
    {
      "epoch": 0.6672575784435384,
      "grad_norm": 0.9506751894950867,
      "learning_rate": 0.00015551458421973245,
      "loss": 0.2949,
      "step": 9410
    },
    {
      "epoch": 0.6679666725757845,
      "grad_norm": 1.5951179265975952,
      "learning_rate": 0.00015546730960147497,
      "loss": 0.2572,
      "step": 9420
    },
    {
      "epoch": 0.6686757667080305,
      "grad_norm": 1.2447885274887085,
      "learning_rate": 0.00015542003498321752,
      "loss": 0.221,
      "step": 9430
    },
    {
      "epoch": 0.6693848608402766,
      "grad_norm": 0.9939025640487671,
      "learning_rate": 0.00015537276036496007,
      "loss": 0.2431,
      "step": 9440
    },
    {
      "epoch": 0.6700939549725226,
      "grad_norm": 0.8959689736366272,
      "learning_rate": 0.0001553254857467026,
      "loss": 0.2298,
      "step": 9450
    },
    {
      "epoch": 0.6708030491047686,
      "grad_norm": 0.776432454586029,
      "learning_rate": 0.00015527821112844514,
      "loss": 0.2262,
      "step": 9460
    },
    {
      "epoch": 0.6715121432370147,
      "grad_norm": 0.8785687685012817,
      "learning_rate": 0.0001552309365101877,
      "loss": 0.2574,
      "step": 9470
    },
    {
      "epoch": 0.6722212373692608,
      "grad_norm": 0.6780930757522583,
      "learning_rate": 0.00015518366189193024,
      "loss": 0.2458,
      "step": 9480
    },
    {
      "epoch": 0.6729303315015068,
      "grad_norm": 1.3789808750152588,
      "learning_rate": 0.00015513638727367277,
      "loss": 0.2599,
      "step": 9490
    },
    {
      "epoch": 0.6736394256337529,
      "grad_norm": 1.9562495946884155,
      "learning_rate": 0.00015508911265541532,
      "loss": 0.2514,
      "step": 9500
    },
    {
      "epoch": 0.674348519765999,
      "grad_norm": 2.511610746383667,
      "learning_rate": 0.00015504183803715787,
      "loss": 0.2449,
      "step": 9510
    },
    {
      "epoch": 0.675057613898245,
      "grad_norm": 2.439371109008789,
      "learning_rate": 0.0001549945634189004,
      "loss": 0.2749,
      "step": 9520
    },
    {
      "epoch": 0.675766708030491,
      "grad_norm": 2.566226005554199,
      "learning_rate": 0.00015494728880064294,
      "loss": 0.2426,
      "step": 9530
    },
    {
      "epoch": 0.6764758021627371,
      "grad_norm": 1.7427291870117188,
      "learning_rate": 0.0001549000141823855,
      "loss": 0.2789,
      "step": 9540
    },
    {
      "epoch": 0.6771848962949831,
      "grad_norm": 1.9352225065231323,
      "learning_rate": 0.000154852739564128,
      "loss": 0.2715,
      "step": 9550
    },
    {
      "epoch": 0.6778939904272292,
      "grad_norm": 2.260723114013672,
      "learning_rate": 0.00015480546494587056,
      "loss": 0.2603,
      "step": 9560
    },
    {
      "epoch": 0.6786030845594753,
      "grad_norm": 1.0121482610702515,
      "learning_rate": 0.0001547581903276131,
      "loss": 0.3007,
      "step": 9570
    },
    {
      "epoch": 0.6793121786917213,
      "grad_norm": 1.3381856679916382,
      "learning_rate": 0.00015471091570935566,
      "loss": 0.2795,
      "step": 9580
    },
    {
      "epoch": 0.6800212728239674,
      "grad_norm": 1.1829758882522583,
      "learning_rate": 0.00015466364109109818,
      "loss": 0.2354,
      "step": 9590
    },
    {
      "epoch": 0.6807303669562135,
      "grad_norm": 0.619773805141449,
      "learning_rate": 0.00015461636647284073,
      "loss": 0.2351,
      "step": 9600
    },
    {
      "epoch": 0.6814394610884595,
      "grad_norm": 0.743564784526825,
      "learning_rate": 0.00015456909185458328,
      "loss": 0.2435,
      "step": 9610
    },
    {
      "epoch": 0.6821485552207055,
      "grad_norm": 1.033535361289978,
      "learning_rate": 0.0001545218172363258,
      "loss": 0.2233,
      "step": 9620
    },
    {
      "epoch": 0.6828576493529516,
      "grad_norm": 0.5482068061828613,
      "learning_rate": 0.00015447454261806838,
      "loss": 0.231,
      "step": 9630
    },
    {
      "epoch": 0.6835667434851976,
      "grad_norm": 1.2145655155181885,
      "learning_rate": 0.0001544272679998109,
      "loss": 0.2434,
      "step": 9640
    },
    {
      "epoch": 0.6842758376174437,
      "grad_norm": 0.572066605091095,
      "learning_rate": 0.00015437999338155346,
      "loss": 0.217,
      "step": 9650
    },
    {
      "epoch": 0.6849849317496898,
      "grad_norm": 1.1854528188705444,
      "learning_rate": 0.000154332718763296,
      "loss": 0.2355,
      "step": 9660
    },
    {
      "epoch": 0.6856940258819358,
      "grad_norm": 1.3079149723052979,
      "learning_rate": 0.00015428544414503853,
      "loss": 0.2774,
      "step": 9670
    },
    {
      "epoch": 0.6864031200141819,
      "grad_norm": 1.37306809425354,
      "learning_rate": 0.00015423816952678108,
      "loss": 0.2547,
      "step": 9680
    },
    {
      "epoch": 0.687112214146428,
      "grad_norm": 0.7466627359390259,
      "learning_rate": 0.00015419089490852363,
      "loss": 0.2713,
      "step": 9690
    },
    {
      "epoch": 0.687821308278674,
      "grad_norm": 1.597664713859558,
      "learning_rate": 0.00015414362029026618,
      "loss": 0.2405,
      "step": 9700
    },
    {
      "epoch": 0.68853040241092,
      "grad_norm": 0.7239384651184082,
      "learning_rate": 0.0001540963456720087,
      "loss": 0.264,
      "step": 9710
    },
    {
      "epoch": 0.6892394965431661,
      "grad_norm": 0.6416068077087402,
      "learning_rate": 0.00015404907105375125,
      "loss": 0.2613,
      "step": 9720
    },
    {
      "epoch": 0.6899485906754121,
      "grad_norm": 1.8634800910949707,
      "learning_rate": 0.0001540017964354938,
      "loss": 0.2792,
      "step": 9730
    },
    {
      "epoch": 0.6906576848076582,
      "grad_norm": 1.2161004543304443,
      "learning_rate": 0.00015395452181723632,
      "loss": 0.2498,
      "step": 9740
    },
    {
      "epoch": 0.6913667789399043,
      "grad_norm": 1.2319849729537964,
      "learning_rate": 0.00015390724719897887,
      "loss": 0.288,
      "step": 9750
    },
    {
      "epoch": 0.6920758730721503,
      "grad_norm": 2.2053089141845703,
      "learning_rate": 0.00015385997258072142,
      "loss": 0.2347,
      "step": 9760
    },
    {
      "epoch": 0.6927849672043964,
      "grad_norm": 1.3291743993759155,
      "learning_rate": 0.00015381269796246397,
      "loss": 0.2559,
      "step": 9770
    },
    {
      "epoch": 0.6934940613366425,
      "grad_norm": 3.325021743774414,
      "learning_rate": 0.0001537654233442065,
      "loss": 0.2393,
      "step": 9780
    },
    {
      "epoch": 0.6942031554688884,
      "grad_norm": 1.0472103357315063,
      "learning_rate": 0.00015371814872594904,
      "loss": 0.236,
      "step": 9790
    },
    {
      "epoch": 0.6949122496011345,
      "grad_norm": 3.687384605407715,
      "learning_rate": 0.0001536708741076916,
      "loss": 0.2424,
      "step": 9800
    },
    {
      "epoch": 0.6956213437333806,
      "grad_norm": 1.397047758102417,
      "learning_rate": 0.00015362359948943412,
      "loss": 0.2703,
      "step": 9810
    },
    {
      "epoch": 0.6963304378656267,
      "grad_norm": 0.8604673743247986,
      "learning_rate": 0.00015357632487117667,
      "loss": 0.2761,
      "step": 9820
    },
    {
      "epoch": 0.6970395319978727,
      "grad_norm": 0.9166991114616394,
      "learning_rate": 0.00015352905025291922,
      "loss": 0.2348,
      "step": 9830
    },
    {
      "epoch": 0.6977486261301188,
      "grad_norm": 1.384734034538269,
      "learning_rate": 0.00015348177563466174,
      "loss": 0.2566,
      "step": 9840
    },
    {
      "epoch": 0.6984577202623649,
      "grad_norm": 3.163792848587036,
      "learning_rate": 0.00015343450101640432,
      "loss": 0.2614,
      "step": 9850
    },
    {
      "epoch": 0.6991668143946109,
      "grad_norm": 0.9774994254112244,
      "learning_rate": 0.00015338722639814684,
      "loss": 0.2641,
      "step": 9860
    },
    {
      "epoch": 0.699875908526857,
      "grad_norm": 2.0622129440307617,
      "learning_rate": 0.0001533399517798894,
      "loss": 0.263,
      "step": 9870
    },
    {
      "epoch": 0.700585002659103,
      "grad_norm": 0.8345804810523987,
      "learning_rate": 0.00015329267716163194,
      "loss": 0.2573,
      "step": 9880
    },
    {
      "epoch": 0.701294096791349,
      "grad_norm": 1.0174052715301514,
      "learning_rate": 0.00015324540254337446,
      "loss": 0.2217,
      "step": 9890
    },
    {
      "epoch": 0.7020031909235951,
      "grad_norm": 1.1893562078475952,
      "learning_rate": 0.000153198127925117,
      "loss": 0.275,
      "step": 9900
    },
    {
      "epoch": 0.7027122850558412,
      "grad_norm": 0.9175121188163757,
      "learning_rate": 0.00015315085330685956,
      "loss": 0.2783,
      "step": 9910
    },
    {
      "epoch": 0.7034213791880872,
      "grad_norm": 2.9017763137817383,
      "learning_rate": 0.0001531035786886021,
      "loss": 0.2831,
      "step": 9920
    },
    {
      "epoch": 0.7041304733203333,
      "grad_norm": 3.800508975982666,
      "learning_rate": 0.00015305630407034463,
      "loss": 0.2737,
      "step": 9930
    },
    {
      "epoch": 0.7048395674525794,
      "grad_norm": 1.00039541721344,
      "learning_rate": 0.00015300902945208718,
      "loss": 0.253,
      "step": 9940
    },
    {
      "epoch": 0.7055486615848254,
      "grad_norm": 0.8087174296379089,
      "learning_rate": 0.00015296175483382973,
      "loss": 0.2201,
      "step": 9950
    },
    {
      "epoch": 0.7062577557170714,
      "grad_norm": 0.7183470726013184,
      "learning_rate": 0.00015291448021557226,
      "loss": 0.2695,
      "step": 9960
    },
    {
      "epoch": 0.7069668498493175,
      "grad_norm": 1.3990449905395508,
      "learning_rate": 0.0001528672055973148,
      "loss": 0.2748,
      "step": 9970
    },
    {
      "epoch": 0.7076759439815635,
      "grad_norm": 2.574075698852539,
      "learning_rate": 0.00015281993097905736,
      "loss": 0.2401,
      "step": 9980
    },
    {
      "epoch": 0.7083850381138096,
      "grad_norm": 1.1016203165054321,
      "learning_rate": 0.0001527726563607999,
      "loss": 0.2429,
      "step": 9990
    },
    {
      "epoch": 0.7090941322460557,
      "grad_norm": 1.2382872104644775,
      "learning_rate": 0.00015272538174254243,
      "loss": 0.2247,
      "step": 10000
    },
    {
      "epoch": 0.7098032263783017,
      "grad_norm": 1.5933935642242432,
      "learning_rate": 0.00015267810712428498,
      "loss": 0.2438,
      "step": 10010
    },
    {
      "epoch": 0.7105123205105478,
      "grad_norm": 1.0334182977676392,
      "learning_rate": 0.00015263083250602753,
      "loss": 0.2207,
      "step": 10020
    },
    {
      "epoch": 0.7112214146427939,
      "grad_norm": 0.786270797252655,
      "learning_rate": 0.00015258355788777005,
      "loss": 0.2525,
      "step": 10030
    },
    {
      "epoch": 0.7119305087750399,
      "grad_norm": 1.4586913585662842,
      "learning_rate": 0.0001525362832695126,
      "loss": 0.2617,
      "step": 10040
    },
    {
      "epoch": 0.7126396029072859,
      "grad_norm": 1.1071714162826538,
      "learning_rate": 0.00015248900865125515,
      "loss": 0.2292,
      "step": 10050
    },
    {
      "epoch": 0.713348697039532,
      "grad_norm": 1.0731679201126099,
      "learning_rate": 0.00015244173403299767,
      "loss": 0.2529,
      "step": 10060
    },
    {
      "epoch": 0.714057791171778,
      "grad_norm": 0.9485594034194946,
      "learning_rate": 0.00015239445941474025,
      "loss": 0.2707,
      "step": 10070
    },
    {
      "epoch": 0.7147668853040241,
      "grad_norm": 1.290605902671814,
      "learning_rate": 0.00015234718479648277,
      "loss": 0.2447,
      "step": 10080
    },
    {
      "epoch": 0.7154759794362702,
      "grad_norm": 0.7910696864128113,
      "learning_rate": 0.00015229991017822532,
      "loss": 0.2201,
      "step": 10090
    },
    {
      "epoch": 0.7161850735685162,
      "grad_norm": 0.6686616539955139,
      "learning_rate": 0.00015225263555996787,
      "loss": 0.2379,
      "step": 10100
    },
    {
      "epoch": 0.7168941677007623,
      "grad_norm": 0.6397690176963806,
      "learning_rate": 0.0001522053609417104,
      "loss": 0.2428,
      "step": 10110
    },
    {
      "epoch": 0.7176032618330084,
      "grad_norm": 0.8810450434684753,
      "learning_rate": 0.00015215808632345295,
      "loss": 0.2529,
      "step": 10120
    },
    {
      "epoch": 0.7183123559652543,
      "grad_norm": 0.8842422366142273,
      "learning_rate": 0.00015211081170519547,
      "loss": 0.2672,
      "step": 10130
    },
    {
      "epoch": 0.7190214500975004,
      "grad_norm": 0.6915726065635681,
      "learning_rate": 0.00015206353708693805,
      "loss": 0.2745,
      "step": 10140
    },
    {
      "epoch": 0.7197305442297465,
      "grad_norm": 0.7088571786880493,
      "learning_rate": 0.00015201626246868057,
      "loss": 0.2742,
      "step": 10150
    },
    {
      "epoch": 0.7204396383619925,
      "grad_norm": 1.4522819519042969,
      "learning_rate": 0.00015196898785042312,
      "loss": 0.2421,
      "step": 10160
    },
    {
      "epoch": 0.7211487324942386,
      "grad_norm": 0.9143816232681274,
      "learning_rate": 0.00015192171323216567,
      "loss": 0.2506,
      "step": 10170
    },
    {
      "epoch": 0.7218578266264847,
      "grad_norm": 0.7214717864990234,
      "learning_rate": 0.0001518744386139082,
      "loss": 0.2438,
      "step": 10180
    },
    {
      "epoch": 0.7225669207587307,
      "grad_norm": 0.5996225476264954,
      "learning_rate": 0.00015182716399565074,
      "loss": 0.2398,
      "step": 10190
    },
    {
      "epoch": 0.7232760148909768,
      "grad_norm": 0.689755916595459,
      "learning_rate": 0.0001517798893773933,
      "loss": 0.2303,
      "step": 10200
    },
    {
      "epoch": 0.7239851090232229,
      "grad_norm": 0.7658282518386841,
      "learning_rate": 0.00015173261475913584,
      "loss": 0.28,
      "step": 10210
    },
    {
      "epoch": 0.7246942031554688,
      "grad_norm": 1.2002042531967163,
      "learning_rate": 0.00015168534014087836,
      "loss": 0.2206,
      "step": 10220
    },
    {
      "epoch": 0.7254032972877149,
      "grad_norm": 1.9689862728118896,
      "learning_rate": 0.0001516380655226209,
      "loss": 0.2428,
      "step": 10230
    },
    {
      "epoch": 0.726112391419961,
      "grad_norm": 1.6520739793777466,
      "learning_rate": 0.00015159079090436346,
      "loss": 0.2172,
      "step": 10240
    },
    {
      "epoch": 0.726821485552207,
      "grad_norm": 2.267148494720459,
      "learning_rate": 0.00015154351628610599,
      "loss": 0.2528,
      "step": 10250
    },
    {
      "epoch": 0.7275305796844531,
      "grad_norm": 0.7620441913604736,
      "learning_rate": 0.00015149624166784856,
      "loss": 0.243,
      "step": 10260
    },
    {
      "epoch": 0.7282396738166992,
      "grad_norm": 0.9078564643859863,
      "learning_rate": 0.00015144896704959108,
      "loss": 0.2572,
      "step": 10270
    },
    {
      "epoch": 0.7289487679489453,
      "grad_norm": 1.3221979141235352,
      "learning_rate": 0.0001514016924313336,
      "loss": 0.2387,
      "step": 10280
    },
    {
      "epoch": 0.7296578620811913,
      "grad_norm": 0.8428783416748047,
      "learning_rate": 0.00015135441781307618,
      "loss": 0.2948,
      "step": 10290
    },
    {
      "epoch": 0.7303669562134373,
      "grad_norm": 0.9735851883888245,
      "learning_rate": 0.0001513071431948187,
      "loss": 0.2718,
      "step": 10300
    },
    {
      "epoch": 0.7310760503456833,
      "grad_norm": 1.381284236907959,
      "learning_rate": 0.00015125986857656126,
      "loss": 0.2642,
      "step": 10310
    },
    {
      "epoch": 0.7317851444779294,
      "grad_norm": 0.8421396613121033,
      "learning_rate": 0.00015121259395830378,
      "loss": 0.2168,
      "step": 10320
    },
    {
      "epoch": 0.7324942386101755,
      "grad_norm": 1.5975747108459473,
      "learning_rate": 0.00015116531934004633,
      "loss": 0.2509,
      "step": 10330
    },
    {
      "epoch": 0.7332033327424216,
      "grad_norm": 0.8854367136955261,
      "learning_rate": 0.00015111804472178888,
      "loss": 0.2656,
      "step": 10340
    },
    {
      "epoch": 0.7339124268746676,
      "grad_norm": 1.0658900737762451,
      "learning_rate": 0.0001510707701035314,
      "loss": 0.2907,
      "step": 10350
    },
    {
      "epoch": 0.7346215210069137,
      "grad_norm": 1.9517343044281006,
      "learning_rate": 0.00015102349548527398,
      "loss": 0.2726,
      "step": 10360
    },
    {
      "epoch": 0.7353306151391598,
      "grad_norm": 1.5231070518493652,
      "learning_rate": 0.0001509762208670165,
      "loss": 0.2511,
      "step": 10370
    },
    {
      "epoch": 0.7360397092714058,
      "grad_norm": 1.064036250114441,
      "learning_rate": 0.00015092894624875905,
      "loss": 0.2501,
      "step": 10380
    },
    {
      "epoch": 0.7367488034036518,
      "grad_norm": 0.9236180186271667,
      "learning_rate": 0.0001508816716305016,
      "loss": 0.2381,
      "step": 10390
    },
    {
      "epoch": 0.7374578975358979,
      "grad_norm": 1.094853162765503,
      "learning_rate": 0.00015083439701224412,
      "loss": 0.2526,
      "step": 10400
    },
    {
      "epoch": 0.7381669916681439,
      "grad_norm": 0.8304629325866699,
      "learning_rate": 0.00015078712239398667,
      "loss": 0.2562,
      "step": 10410
    },
    {
      "epoch": 0.73887608580039,
      "grad_norm": 1.7319412231445312,
      "learning_rate": 0.00015073984777572922,
      "loss": 0.2684,
      "step": 10420
    },
    {
      "epoch": 0.7395851799326361,
      "grad_norm": 0.852570116519928,
      "learning_rate": 0.00015069257315747177,
      "loss": 0.2245,
      "step": 10430
    },
    {
      "epoch": 0.7402942740648821,
      "grad_norm": 1.3807435035705566,
      "learning_rate": 0.0001506452985392143,
      "loss": 0.2175,
      "step": 10440
    },
    {
      "epoch": 0.7410033681971282,
      "grad_norm": 0.6796525716781616,
      "learning_rate": 0.00015059802392095685,
      "loss": 0.2323,
      "step": 10450
    },
    {
      "epoch": 0.7417124623293743,
      "grad_norm": 2.2723052501678467,
      "learning_rate": 0.0001505507493026994,
      "loss": 0.2529,
      "step": 10460
    },
    {
      "epoch": 0.7424215564616203,
      "grad_norm": 1.1076347827911377,
      "learning_rate": 0.00015050347468444192,
      "loss": 0.25,
      "step": 10470
    },
    {
      "epoch": 0.7431306505938663,
      "grad_norm": 0.8981179594993591,
      "learning_rate": 0.0001504562000661845,
      "loss": 0.283,
      "step": 10480
    },
    {
      "epoch": 0.7438397447261124,
      "grad_norm": 2.480957508087158,
      "learning_rate": 0.00015040892544792702,
      "loss": 0.2347,
      "step": 10490
    },
    {
      "epoch": 0.7445488388583584,
      "grad_norm": 0.8779929876327515,
      "learning_rate": 0.00015036165082966954,
      "loss": 0.2271,
      "step": 10500
    },
    {
      "epoch": 0.7452579329906045,
      "grad_norm": 1.3709934949874878,
      "learning_rate": 0.0001503143762114121,
      "loss": 0.2526,
      "step": 10510
    },
    {
      "epoch": 0.7459670271228506,
      "grad_norm": 2.7532389163970947,
      "learning_rate": 0.00015026710159315464,
      "loss": 0.2474,
      "step": 10520
    },
    {
      "epoch": 0.7466761212550966,
      "grad_norm": 1.02128005027771,
      "learning_rate": 0.0001502198269748972,
      "loss": 0.2419,
      "step": 10530
    },
    {
      "epoch": 0.7473852153873427,
      "grad_norm": 0.6229203343391418,
      "learning_rate": 0.00015017255235663971,
      "loss": 0.2381,
      "step": 10540
    },
    {
      "epoch": 0.7480943095195888,
      "grad_norm": 1.198667287826538,
      "learning_rate": 0.00015012527773838226,
      "loss": 0.246,
      "step": 10550
    },
    {
      "epoch": 0.7488034036518347,
      "grad_norm": 1.595400094985962,
      "learning_rate": 0.0001500780031201248,
      "loss": 0.2415,
      "step": 10560
    },
    {
      "epoch": 0.7495124977840808,
      "grad_norm": 0.9468442797660828,
      "learning_rate": 0.00015003072850186734,
      "loss": 0.2556,
      "step": 10570
    },
    {
      "epoch": 0.7502215919163269,
      "grad_norm": 0.6993142366409302,
      "learning_rate": 0.0001499834538836099,
      "loss": 0.2777,
      "step": 10580
    },
    {
      "epoch": 0.7509306860485729,
      "grad_norm": 0.9833967685699463,
      "learning_rate": 0.00014993617926535244,
      "loss": 0.242,
      "step": 10590
    },
    {
      "epoch": 0.751639780180819,
      "grad_norm": 1.1490286588668823,
      "learning_rate": 0.00014988890464709499,
      "loss": 0.2523,
      "step": 10600
    },
    {
      "epoch": 0.7523488743130651,
      "grad_norm": 1.0165469646453857,
      "learning_rate": 0.00014984163002883754,
      "loss": 0.2407,
      "step": 10610
    },
    {
      "epoch": 0.7530579684453111,
      "grad_norm": 0.8363916873931885,
      "learning_rate": 0.00014979435541058006,
      "loss": 0.2444,
      "step": 10620
    },
    {
      "epoch": 0.7537670625775572,
      "grad_norm": 0.7672389149665833,
      "learning_rate": 0.0001497470807923226,
      "loss": 0.2717,
      "step": 10630
    },
    {
      "epoch": 0.7544761567098033,
      "grad_norm": 0.9159209728240967,
      "learning_rate": 0.00014969980617406516,
      "loss": 0.2922,
      "step": 10640
    },
    {
      "epoch": 0.7551852508420492,
      "grad_norm": 1.2535055875778198,
      "learning_rate": 0.0001496525315558077,
      "loss": 0.2295,
      "step": 10650
    },
    {
      "epoch": 0.7558943449742953,
      "grad_norm": 1.8547085523605347,
      "learning_rate": 0.00014960525693755023,
      "loss": 0.2477,
      "step": 10660
    },
    {
      "epoch": 0.7566034391065414,
      "grad_norm": 1.1672585010528564,
      "learning_rate": 0.00014955798231929278,
      "loss": 0.2327,
      "step": 10670
    },
    {
      "epoch": 0.7573125332387874,
      "grad_norm": 1.180302381515503,
      "learning_rate": 0.00014951070770103533,
      "loss": 0.238,
      "step": 10680
    },
    {
      "epoch": 0.7580216273710335,
      "grad_norm": 1.355271577835083,
      "learning_rate": 0.00014946343308277785,
      "loss": 0.2233,
      "step": 10690
    },
    {
      "epoch": 0.7587307215032796,
      "grad_norm": 1.3400720357894897,
      "learning_rate": 0.00014941615846452043,
      "loss": 0.2606,
      "step": 10700
    },
    {
      "epoch": 0.7594398156355257,
      "grad_norm": 1.2355912923812866,
      "learning_rate": 0.00014936888384626295,
      "loss": 0.236,
      "step": 10710
    },
    {
      "epoch": 0.7601489097677717,
      "grad_norm": 1.909067988395691,
      "learning_rate": 0.00014932160922800548,
      "loss": 0.2509,
      "step": 10720
    },
    {
      "epoch": 0.7608580039000177,
      "grad_norm": 1.1723467111587524,
      "learning_rate": 0.00014927433460974803,
      "loss": 0.2834,
      "step": 10730
    },
    {
      "epoch": 0.7615670980322637,
      "grad_norm": 4.318026542663574,
      "learning_rate": 0.00014922705999149058,
      "loss": 0.2538,
      "step": 10740
    },
    {
      "epoch": 0.7622761921645098,
      "grad_norm": 3.1318509578704834,
      "learning_rate": 0.00014917978537323313,
      "loss": 0.252,
      "step": 10750
    },
    {
      "epoch": 0.7629852862967559,
      "grad_norm": 0.616861879825592,
      "learning_rate": 0.00014913251075497565,
      "loss": 0.2452,
      "step": 10760
    },
    {
      "epoch": 0.763694380429002,
      "grad_norm": 1.311800479888916,
      "learning_rate": 0.0001490852361367182,
      "loss": 0.2464,
      "step": 10770
    },
    {
      "epoch": 0.764403474561248,
      "grad_norm": 1.067911148071289,
      "learning_rate": 0.00014903796151846075,
      "loss": 0.2462,
      "step": 10780
    },
    {
      "epoch": 0.7651125686934941,
      "grad_norm": 0.7200174927711487,
      "learning_rate": 0.00014899068690020327,
      "loss": 0.2424,
      "step": 10790
    },
    {
      "epoch": 0.7658216628257402,
      "grad_norm": 1.2114900350570679,
      "learning_rate": 0.00014894341228194585,
      "loss": 0.2319,
      "step": 10800
    },
    {
      "epoch": 0.7665307569579862,
      "grad_norm": 1.06528902053833,
      "learning_rate": 0.00014889613766368837,
      "loss": 0.2255,
      "step": 10810
    },
    {
      "epoch": 0.7672398510902322,
      "grad_norm": 2.030036211013794,
      "learning_rate": 0.00014884886304543092,
      "loss": 0.2541,
      "step": 10820
    },
    {
      "epoch": 0.7679489452224783,
      "grad_norm": 0.845474898815155,
      "learning_rate": 0.00014880158842717347,
      "loss": 0.2514,
      "step": 10830
    },
    {
      "epoch": 0.7686580393547243,
      "grad_norm": 0.8716084361076355,
      "learning_rate": 0.000148754313808916,
      "loss": 0.2592,
      "step": 10840
    },
    {
      "epoch": 0.7693671334869704,
      "grad_norm": 1.412781834602356,
      "learning_rate": 0.00014870703919065854,
      "loss": 0.2762,
      "step": 10850
    },
    {
      "epoch": 0.7700762276192165,
      "grad_norm": 0.8050802946090698,
      "learning_rate": 0.0001486597645724011,
      "loss": 0.2273,
      "step": 10860
    },
    {
      "epoch": 0.7707853217514625,
      "grad_norm": 0.8517328500747681,
      "learning_rate": 0.00014861248995414364,
      "loss": 0.2454,
      "step": 10870
    },
    {
      "epoch": 0.7714944158837086,
      "grad_norm": 0.8625912666320801,
      "learning_rate": 0.00014856521533588616,
      "loss": 0.261,
      "step": 10880
    },
    {
      "epoch": 0.7722035100159547,
      "grad_norm": 1.0539371967315674,
      "learning_rate": 0.00014851794071762871,
      "loss": 0.2581,
      "step": 10890
    },
    {
      "epoch": 0.7729126041482006,
      "grad_norm": 0.8149439096450806,
      "learning_rate": 0.00014847066609937126,
      "loss": 0.2208,
      "step": 10900
    },
    {
      "epoch": 0.7736216982804467,
      "grad_norm": 0.8748297691345215,
      "learning_rate": 0.0001484233914811138,
      "loss": 0.2567,
      "step": 10910
    },
    {
      "epoch": 0.7743307924126928,
      "grad_norm": 0.9874413013458252,
      "learning_rate": 0.00014837611686285634,
      "loss": 0.2417,
      "step": 10920
    },
    {
      "epoch": 0.7750398865449388,
      "grad_norm": 0.931992769241333,
      "learning_rate": 0.0001483288422445989,
      "loss": 0.2352,
      "step": 10930
    },
    {
      "epoch": 0.7757489806771849,
      "grad_norm": 1.0888633728027344,
      "learning_rate": 0.0001482815676263414,
      "loss": 0.2251,
      "step": 10940
    },
    {
      "epoch": 0.776458074809431,
      "grad_norm": 0.72214674949646,
      "learning_rate": 0.00014823429300808396,
      "loss": 0.2617,
      "step": 10950
    },
    {
      "epoch": 0.777167168941677,
      "grad_norm": 1.6005843877792358,
      "learning_rate": 0.0001481870183898265,
      "loss": 0.2616,
      "step": 10960
    },
    {
      "epoch": 0.7778762630739231,
      "grad_norm": 1.601538896560669,
      "learning_rate": 0.00014813974377156906,
      "loss": 0.2345,
      "step": 10970
    },
    {
      "epoch": 0.7785853572061692,
      "grad_norm": 0.7077479362487793,
      "learning_rate": 0.00014809246915331158,
      "loss": 0.2555,
      "step": 10980
    },
    {
      "epoch": 0.7792944513384151,
      "grad_norm": 1.7451622486114502,
      "learning_rate": 0.00014804519453505413,
      "loss": 0.2636,
      "step": 10990
    },
    {
      "epoch": 0.7800035454706612,
      "grad_norm": 1.3312631845474243,
      "learning_rate": 0.00014799791991679668,
      "loss": 0.2441,
      "step": 11000
    },
    {
      "epoch": 0.7807126396029073,
      "grad_norm": 1.0742299556732178,
      "learning_rate": 0.0001479506452985392,
      "loss": 0.2511,
      "step": 11010
    },
    {
      "epoch": 0.7814217337351533,
      "grad_norm": 0.7335730791091919,
      "learning_rate": 0.00014790337068028178,
      "loss": 0.2989,
      "step": 11020
    },
    {
      "epoch": 0.7821308278673994,
      "grad_norm": 0.7622009515762329,
      "learning_rate": 0.0001478560960620243,
      "loss": 0.2511,
      "step": 11030
    },
    {
      "epoch": 0.7828399219996455,
      "grad_norm": 1.0593363046646118,
      "learning_rate": 0.00014780882144376685,
      "loss": 0.2322,
      "step": 11040
    },
    {
      "epoch": 0.7835490161318915,
      "grad_norm": 2.583268880844116,
      "learning_rate": 0.0001477615468255094,
      "loss": 0.3037,
      "step": 11050
    },
    {
      "epoch": 0.7842581102641376,
      "grad_norm": 1.4504109621047974,
      "learning_rate": 0.00014771427220725193,
      "loss": 0.2461,
      "step": 11060
    },
    {
      "epoch": 0.7849672043963836,
      "grad_norm": 1.1189593076705933,
      "learning_rate": 0.00014766699758899448,
      "loss": 0.2336,
      "step": 11070
    },
    {
      "epoch": 0.7856762985286296,
      "grad_norm": 0.8927247524261475,
      "learning_rate": 0.00014761972297073703,
      "loss": 0.2403,
      "step": 11080
    },
    {
      "epoch": 0.7863853926608757,
      "grad_norm": 1.0781817436218262,
      "learning_rate": 0.00014757244835247958,
      "loss": 0.2227,
      "step": 11090
    },
    {
      "epoch": 0.7870944867931218,
      "grad_norm": 0.8971752524375916,
      "learning_rate": 0.0001475251737342221,
      "loss": 0.2579,
      "step": 11100
    },
    {
      "epoch": 0.7878035809253678,
      "grad_norm": 0.6402950882911682,
      "learning_rate": 0.00014747789911596465,
      "loss": 0.2258,
      "step": 11110
    },
    {
      "epoch": 0.7885126750576139,
      "grad_norm": 1.434830904006958,
      "learning_rate": 0.0001474306244977072,
      "loss": 0.2756,
      "step": 11120
    },
    {
      "epoch": 0.78922176918986,
      "grad_norm": 2.5571341514587402,
      "learning_rate": 0.00014738334987944972,
      "loss": 0.2487,
      "step": 11130
    },
    {
      "epoch": 0.789930863322106,
      "grad_norm": 0.895288348197937,
      "learning_rate": 0.00014733607526119227,
      "loss": 0.21,
      "step": 11140
    },
    {
      "epoch": 0.7906399574543521,
      "grad_norm": 1.7444204092025757,
      "learning_rate": 0.00014728880064293482,
      "loss": 0.2545,
      "step": 11150
    },
    {
      "epoch": 0.7913490515865981,
      "grad_norm": 1.8232966661453247,
      "learning_rate": 0.00014724152602467734,
      "loss": 0.254,
      "step": 11160
    },
    {
      "epoch": 0.7920581457188441,
      "grad_norm": 1.70316743850708,
      "learning_rate": 0.0001471942514064199,
      "loss": 0.2769,
      "step": 11170
    },
    {
      "epoch": 0.7927672398510902,
      "grad_norm": 4.124693393707275,
      "learning_rate": 0.00014714697678816244,
      "loss": 0.2555,
      "step": 11180
    },
    {
      "epoch": 0.7934763339833363,
      "grad_norm": 1.798377513885498,
      "learning_rate": 0.000147099702169905,
      "loss": 0.2439,
      "step": 11190
    },
    {
      "epoch": 0.7941854281155823,
      "grad_norm": 1.0468106269836426,
      "learning_rate": 0.00014705242755164752,
      "loss": 0.2326,
      "step": 11200
    },
    {
      "epoch": 0.7948945222478284,
      "grad_norm": 1.2024486064910889,
      "learning_rate": 0.00014700515293339007,
      "loss": 0.2816,
      "step": 11210
    },
    {
      "epoch": 0.7956036163800745,
      "grad_norm": 1.5979572534561157,
      "learning_rate": 0.00014695787831513262,
      "loss": 0.2436,
      "step": 11220
    },
    {
      "epoch": 0.7963127105123206,
      "grad_norm": 2.272218942642212,
      "learning_rate": 0.00014691060369687514,
      "loss": 0.2597,
      "step": 11230
    },
    {
      "epoch": 0.7970218046445665,
      "grad_norm": 0.850566029548645,
      "learning_rate": 0.00014686332907861772,
      "loss": 0.2223,
      "step": 11240
    },
    {
      "epoch": 0.7977308987768126,
      "grad_norm": 1.1950968503952026,
      "learning_rate": 0.00014681605446036024,
      "loss": 0.2451,
      "step": 11250
    },
    {
      "epoch": 0.7984399929090586,
      "grad_norm": 1.0781562328338623,
      "learning_rate": 0.0001467687798421028,
      "loss": 0.2586,
      "step": 11260
    },
    {
      "epoch": 0.7991490870413047,
      "grad_norm": 0.6563056111335754,
      "learning_rate": 0.00014672150522384534,
      "loss": 0.2283,
      "step": 11270
    },
    {
      "epoch": 0.7998581811735508,
      "grad_norm": 1.3403669595718384,
      "learning_rate": 0.00014667423060558786,
      "loss": 0.2388,
      "step": 11280
    },
    {
      "epoch": 0.8005672753057969,
      "grad_norm": 1.0738877058029175,
      "learning_rate": 0.0001466269559873304,
      "loss": 0.234,
      "step": 11290
    },
    {
      "epoch": 0.8012763694380429,
      "grad_norm": 1.0147913694381714,
      "learning_rate": 0.00014657968136907293,
      "loss": 0.2774,
      "step": 11300
    },
    {
      "epoch": 0.801985463570289,
      "grad_norm": 0.9522483348846436,
      "learning_rate": 0.0001465324067508155,
      "loss": 0.2724,
      "step": 11310
    },
    {
      "epoch": 0.8026945577025351,
      "grad_norm": 1.4207231998443604,
      "learning_rate": 0.00014648513213255803,
      "loss": 0.2547,
      "step": 11320
    },
    {
      "epoch": 0.803403651834781,
      "grad_norm": 1.1446213722229004,
      "learning_rate": 0.00014643785751430058,
      "loss": 0.2553,
      "step": 11330
    },
    {
      "epoch": 0.8041127459670271,
      "grad_norm": 1.008910059928894,
      "learning_rate": 0.00014639058289604313,
      "loss": 0.26,
      "step": 11340
    },
    {
      "epoch": 0.8048218400992732,
      "grad_norm": 1.6057143211364746,
      "learning_rate": 0.00014634330827778566,
      "loss": 0.2388,
      "step": 11350
    },
    {
      "epoch": 0.8055309342315192,
      "grad_norm": 1.1087217330932617,
      "learning_rate": 0.0001462960336595282,
      "loss": 0.2405,
      "step": 11360
    },
    {
      "epoch": 0.8062400283637653,
      "grad_norm": 0.8590588569641113,
      "learning_rate": 0.00014624875904127075,
      "loss": 0.2571,
      "step": 11370
    },
    {
      "epoch": 0.8069491224960114,
      "grad_norm": 0.7915533185005188,
      "learning_rate": 0.00014620148442301328,
      "loss": 0.2493,
      "step": 11380
    },
    {
      "epoch": 0.8076582166282574,
      "grad_norm": 2.0050694942474365,
      "learning_rate": 0.00014615420980475583,
      "loss": 0.2725,
      "step": 11390
    },
    {
      "epoch": 0.8083673107605035,
      "grad_norm": 0.7606596350669861,
      "learning_rate": 0.00014610693518649838,
      "loss": 0.2366,
      "step": 11400
    },
    {
      "epoch": 0.8090764048927496,
      "grad_norm": 1.458664894104004,
      "learning_rate": 0.00014605966056824093,
      "loss": 0.2467,
      "step": 11410
    },
    {
      "epoch": 0.8097854990249955,
      "grad_norm": 1.36648690700531,
      "learning_rate": 0.00014601238594998345,
      "loss": 0.2417,
      "step": 11420
    },
    {
      "epoch": 0.8104945931572416,
      "grad_norm": 1.3112688064575195,
      "learning_rate": 0.000145965111331726,
      "loss": 0.2494,
      "step": 11430
    },
    {
      "epoch": 0.8112036872894877,
      "grad_norm": 0.8465138077735901,
      "learning_rate": 0.00014591783671346855,
      "loss": 0.2349,
      "step": 11440
    },
    {
      "epoch": 0.8119127814217337,
      "grad_norm": 0.6776350736618042,
      "learning_rate": 0.00014587056209521107,
      "loss": 0.2246,
      "step": 11450
    },
    {
      "epoch": 0.8126218755539798,
      "grad_norm": 1.8348116874694824,
      "learning_rate": 0.00014582328747695365,
      "loss": 0.2344,
      "step": 11460
    },
    {
      "epoch": 0.8133309696862259,
      "grad_norm": 2.222825765609741,
      "learning_rate": 0.00014577601285869617,
      "loss": 0.2298,
      "step": 11470
    },
    {
      "epoch": 0.8140400638184719,
      "grad_norm": 1.285430908203125,
      "learning_rate": 0.00014572873824043872,
      "loss": 0.2491,
      "step": 11480
    },
    {
      "epoch": 0.814749157950718,
      "grad_norm": 2.5751211643218994,
      "learning_rate": 0.00014568146362218124,
      "loss": 0.2158,
      "step": 11490
    },
    {
      "epoch": 0.815458252082964,
      "grad_norm": 0.8743703365325928,
      "learning_rate": 0.0001456341890039238,
      "loss": 0.2282,
      "step": 11500
    },
    {
      "epoch": 0.81616734621521,
      "grad_norm": 1.3931238651275635,
      "learning_rate": 0.00014558691438566634,
      "loss": 0.2371,
      "step": 11510
    },
    {
      "epoch": 0.8168764403474561,
      "grad_norm": 0.8556860685348511,
      "learning_rate": 0.00014553963976740887,
      "loss": 0.26,
      "step": 11520
    },
    {
      "epoch": 0.8175855344797022,
      "grad_norm": 1.2083085775375366,
      "learning_rate": 0.00014549236514915144,
      "loss": 0.2674,
      "step": 11530
    },
    {
      "epoch": 0.8182946286119482,
      "grad_norm": 1.6366387605667114,
      "learning_rate": 0.00014544509053089397,
      "loss": 0.2268,
      "step": 11540
    },
    {
      "epoch": 0.8190037227441943,
      "grad_norm": 0.8529921174049377,
      "learning_rate": 0.00014539781591263652,
      "loss": 0.303,
      "step": 11550
    },
    {
      "epoch": 0.8197128168764404,
      "grad_norm": 2.883979558944702,
      "learning_rate": 0.00014535054129437907,
      "loss": 0.2547,
      "step": 11560
    },
    {
      "epoch": 0.8204219110086864,
      "grad_norm": 2.0580389499664307,
      "learning_rate": 0.0001453032666761216,
      "loss": 0.2474,
      "step": 11570
    },
    {
      "epoch": 0.8211310051409325,
      "grad_norm": 1.6939857006072998,
      "learning_rate": 0.00014525599205786414,
      "loss": 0.2801,
      "step": 11580
    },
    {
      "epoch": 0.8218400992731785,
      "grad_norm": 1.179423213005066,
      "learning_rate": 0.0001452087174396067,
      "loss": 0.2438,
      "step": 11590
    },
    {
      "epoch": 0.8225491934054245,
      "grad_norm": 2.1211769580841064,
      "learning_rate": 0.00014516144282134924,
      "loss": 0.2429,
      "step": 11600
    },
    {
      "epoch": 0.8232582875376706,
      "grad_norm": 0.6911753416061401,
      "learning_rate": 0.00014511416820309176,
      "loss": 0.2364,
      "step": 11610
    },
    {
      "epoch": 0.8239673816699167,
      "grad_norm": 1.4818079471588135,
      "learning_rate": 0.0001450668935848343,
      "loss": 0.2414,
      "step": 11620
    },
    {
      "epoch": 0.8246764758021627,
      "grad_norm": 1.6909065246582031,
      "learning_rate": 0.00014501961896657686,
      "loss": 0.2631,
      "step": 11630
    },
    {
      "epoch": 0.8253855699344088,
      "grad_norm": 1.0431257486343384,
      "learning_rate": 0.00014497234434831938,
      "loss": 0.2388,
      "step": 11640
    },
    {
      "epoch": 0.8260946640666549,
      "grad_norm": 1.17275071144104,
      "learning_rate": 0.00014492506973006193,
      "loss": 0.2765,
      "step": 11650
    },
    {
      "epoch": 0.826803758198901,
      "grad_norm": 1.3189162015914917,
      "learning_rate": 0.00014487779511180448,
      "loss": 0.2512,
      "step": 11660
    },
    {
      "epoch": 0.8275128523311469,
      "grad_norm": 0.9611376523971558,
      "learning_rate": 0.000144830520493547,
      "loss": 0.2284,
      "step": 11670
    },
    {
      "epoch": 0.828221946463393,
      "grad_norm": 0.7331429719924927,
      "learning_rate": 0.00014478324587528958,
      "loss": 0.2444,
      "step": 11680
    },
    {
      "epoch": 0.828931040595639,
      "grad_norm": 1.116255521774292,
      "learning_rate": 0.0001447359712570321,
      "loss": 0.2416,
      "step": 11690
    },
    {
      "epoch": 0.8296401347278851,
      "grad_norm": 0.9351592659950256,
      "learning_rate": 0.00014468869663877466,
      "loss": 0.2363,
      "step": 11700
    },
    {
      "epoch": 0.8303492288601312,
      "grad_norm": 1.1996607780456543,
      "learning_rate": 0.00014464142202051718,
      "loss": 0.2479,
      "step": 11710
    },
    {
      "epoch": 0.8310583229923773,
      "grad_norm": 1.2417162656784058,
      "learning_rate": 0.00014459414740225973,
      "loss": 0.2689,
      "step": 11720
    },
    {
      "epoch": 0.8317674171246233,
      "grad_norm": 0.9226788878440857,
      "learning_rate": 0.00014454687278400228,
      "loss": 0.1984,
      "step": 11730
    },
    {
      "epoch": 0.8324765112568694,
      "grad_norm": 0.7895031571388245,
      "learning_rate": 0.0001444995981657448,
      "loss": 0.2322,
      "step": 11740
    },
    {
      "epoch": 0.8331856053891155,
      "grad_norm": 0.7905623316764832,
      "learning_rate": 0.00014445232354748738,
      "loss": 0.2547,
      "step": 11750
    },
    {
      "epoch": 0.8338946995213614,
      "grad_norm": 0.7152687907218933,
      "learning_rate": 0.0001444050489292299,
      "loss": 0.2331,
      "step": 11760
    },
    {
      "epoch": 0.8346037936536075,
      "grad_norm": 0.8378807306289673,
      "learning_rate": 0.00014435777431097245,
      "loss": 0.2447,
      "step": 11770
    },
    {
      "epoch": 0.8353128877858536,
      "grad_norm": 1.5656229257583618,
      "learning_rate": 0.000144310499692715,
      "loss": 0.2179,
      "step": 11780
    },
    {
      "epoch": 0.8360219819180996,
      "grad_norm": 1.1721420288085938,
      "learning_rate": 0.00014426322507445752,
      "loss": 0.2565,
      "step": 11790
    },
    {
      "epoch": 0.8367310760503457,
      "grad_norm": 1.1870880126953125,
      "learning_rate": 0.00014421595045620007,
      "loss": 0.2298,
      "step": 11800
    },
    {
      "epoch": 0.8374401701825918,
      "grad_norm": 3.440656900405884,
      "learning_rate": 0.00014416867583794262,
      "loss": 0.2641,
      "step": 11810
    },
    {
      "epoch": 0.8381492643148378,
      "grad_norm": 2.3496289253234863,
      "learning_rate": 0.00014412140121968517,
      "loss": 0.2936,
      "step": 11820
    },
    {
      "epoch": 0.8388583584470839,
      "grad_norm": 0.9849119782447815,
      "learning_rate": 0.0001440741266014277,
      "loss": 0.2514,
      "step": 11830
    },
    {
      "epoch": 0.8395674525793299,
      "grad_norm": 1.185075283050537,
      "learning_rate": 0.00014402685198317025,
      "loss": 0.2419,
      "step": 11840
    },
    {
      "epoch": 0.8402765467115759,
      "grad_norm": 1.2204499244689941,
      "learning_rate": 0.0001439795773649128,
      "loss": 0.2464,
      "step": 11850
    },
    {
      "epoch": 0.840985640843822,
      "grad_norm": 0.8556362986564636,
      "learning_rate": 0.00014393230274665532,
      "loss": 0.2423,
      "step": 11860
    },
    {
      "epoch": 0.8416947349760681,
      "grad_norm": 1.5428986549377441,
      "learning_rate": 0.00014388502812839787,
      "loss": 0.2571,
      "step": 11870
    },
    {
      "epoch": 0.8424038291083141,
      "grad_norm": 0.796448826789856,
      "learning_rate": 0.00014383775351014042,
      "loss": 0.2443,
      "step": 11880
    },
    {
      "epoch": 0.8431129232405602,
      "grad_norm": 0.6151167750358582,
      "learning_rate": 0.00014379047889188294,
      "loss": 0.2292,
      "step": 11890
    },
    {
      "epoch": 0.8438220173728063,
      "grad_norm": 0.7852668762207031,
      "learning_rate": 0.0001437432042736255,
      "loss": 0.246,
      "step": 11900
    },
    {
      "epoch": 0.8445311115050523,
      "grad_norm": 5.0811285972595215,
      "learning_rate": 0.00014369592965536804,
      "loss": 0.2683,
      "step": 11910
    },
    {
      "epoch": 0.8452402056372984,
      "grad_norm": 1.0622875690460205,
      "learning_rate": 0.0001436486550371106,
      "loss": 0.2475,
      "step": 11920
    },
    {
      "epoch": 0.8459492997695444,
      "grad_norm": 2.114457607269287,
      "learning_rate": 0.0001436013804188531,
      "loss": 0.2477,
      "step": 11930
    },
    {
      "epoch": 0.8466583939017904,
      "grad_norm": 2.2629547119140625,
      "learning_rate": 0.00014355410580059566,
      "loss": 0.2633,
      "step": 11940
    },
    {
      "epoch": 0.8473674880340365,
      "grad_norm": 1.0253490209579468,
      "learning_rate": 0.0001435068311823382,
      "loss": 0.243,
      "step": 11950
    },
    {
      "epoch": 0.8480765821662826,
      "grad_norm": 1.1680420637130737,
      "learning_rate": 0.00014345955656408073,
      "loss": 0.2785,
      "step": 11960
    },
    {
      "epoch": 0.8487856762985286,
      "grad_norm": 1.6925668716430664,
      "learning_rate": 0.0001434122819458233,
      "loss": 0.2539,
      "step": 11970
    },
    {
      "epoch": 0.8494947704307747,
      "grad_norm": 0.6367012858390808,
      "learning_rate": 0.00014336500732756583,
      "loss": 0.2464,
      "step": 11980
    },
    {
      "epoch": 0.8502038645630208,
      "grad_norm": 1.9314472675323486,
      "learning_rate": 0.00014331773270930838,
      "loss": 0.2935,
      "step": 11990
    },
    {
      "epoch": 0.8509129586952668,
      "grad_norm": 1.592747449874878,
      "learning_rate": 0.00014327045809105093,
      "loss": 0.246,
      "step": 12000
    },
    {
      "epoch": 0.8516220528275128,
      "grad_norm": 1.0764200687408447,
      "learning_rate": 0.00014322318347279346,
      "loss": 0.2379,
      "step": 12010
    },
    {
      "epoch": 0.8523311469597589,
      "grad_norm": 0.902583658695221,
      "learning_rate": 0.000143175908854536,
      "loss": 0.2446,
      "step": 12020
    },
    {
      "epoch": 0.8530402410920049,
      "grad_norm": 1.1423466205596924,
      "learning_rate": 0.00014312863423627856,
      "loss": 0.2884,
      "step": 12030
    },
    {
      "epoch": 0.853749335224251,
      "grad_norm": 1.6704974174499512,
      "learning_rate": 0.0001430813596180211,
      "loss": 0.2957,
      "step": 12040
    },
    {
      "epoch": 0.8544584293564971,
      "grad_norm": 1.516146183013916,
      "learning_rate": 0.00014303408499976363,
      "loss": 0.2964,
      "step": 12050
    },
    {
      "epoch": 0.8551675234887431,
      "grad_norm": 1.3794236183166504,
      "learning_rate": 0.00014298681038150618,
      "loss": 0.2302,
      "step": 12060
    },
    {
      "epoch": 0.8558766176209892,
      "grad_norm": 1.6465039253234863,
      "learning_rate": 0.00014293953576324873,
      "loss": 0.2393,
      "step": 12070
    },
    {
      "epoch": 0.8565857117532353,
      "grad_norm": 1.7432154417037964,
      "learning_rate": 0.00014289226114499125,
      "loss": 0.2474,
      "step": 12080
    },
    {
      "epoch": 0.8572948058854813,
      "grad_norm": 1.0397006273269653,
      "learning_rate": 0.0001428449865267338,
      "loss": 0.2649,
      "step": 12090
    },
    {
      "epoch": 0.8580039000177273,
      "grad_norm": 0.7828798890113831,
      "learning_rate": 0.00014279771190847635,
      "loss": 0.2647,
      "step": 12100
    },
    {
      "epoch": 0.8587129941499734,
      "grad_norm": 0.8005730509757996,
      "learning_rate": 0.00014275043729021887,
      "loss": 0.2521,
      "step": 12110
    },
    {
      "epoch": 0.8594220882822194,
      "grad_norm": 0.9233523607254028,
      "learning_rate": 0.00014270316267196142,
      "loss": 0.2387,
      "step": 12120
    },
    {
      "epoch": 0.8601311824144655,
      "grad_norm": 0.7485644817352295,
      "learning_rate": 0.00014265588805370397,
      "loss": 0.2785,
      "step": 12130
    },
    {
      "epoch": 0.8608402765467116,
      "grad_norm": 1.148462176322937,
      "learning_rate": 0.00014260861343544652,
      "loss": 0.2654,
      "step": 12140
    },
    {
      "epoch": 0.8615493706789576,
      "grad_norm": 0.7304311990737915,
      "learning_rate": 0.00014256133881718905,
      "loss": 0.2459,
      "step": 12150
    },
    {
      "epoch": 0.8622584648112037,
      "grad_norm": 0.9802227020263672,
      "learning_rate": 0.0001425140641989316,
      "loss": 0.2466,
      "step": 12160
    },
    {
      "epoch": 0.8629675589434498,
      "grad_norm": 0.8793745040893555,
      "learning_rate": 0.00014246678958067415,
      "loss": 0.2562,
      "step": 12170
    },
    {
      "epoch": 0.8636766530756957,
      "grad_norm": 0.767959713935852,
      "learning_rate": 0.00014241951496241667,
      "loss": 0.2757,
      "step": 12180
    },
    {
      "epoch": 0.8643857472079418,
      "grad_norm": 1.3288720846176147,
      "learning_rate": 0.00014237224034415925,
      "loss": 0.2404,
      "step": 12190
    },
    {
      "epoch": 0.8650948413401879,
      "grad_norm": 1.087565541267395,
      "learning_rate": 0.00014232496572590177,
      "loss": 0.2547,
      "step": 12200
    },
    {
      "epoch": 0.865803935472434,
      "grad_norm": 1.3559097051620483,
      "learning_rate": 0.00014227769110764432,
      "loss": 0.2544,
      "step": 12210
    },
    {
      "epoch": 0.86651302960468,
      "grad_norm": 1.4270120859146118,
      "learning_rate": 0.00014223041648938687,
      "loss": 0.278,
      "step": 12220
    },
    {
      "epoch": 0.8672221237369261,
      "grad_norm": 0.8491505980491638,
      "learning_rate": 0.0001421831418711294,
      "loss": 0.2444,
      "step": 12230
    },
    {
      "epoch": 0.8679312178691722,
      "grad_norm": 0.9864777326583862,
      "learning_rate": 0.00014213586725287194,
      "loss": 0.2351,
      "step": 12240
    },
    {
      "epoch": 0.8686403120014182,
      "grad_norm": 2.154982566833496,
      "learning_rate": 0.0001420885926346145,
      "loss": 0.2546,
      "step": 12250
    },
    {
      "epoch": 0.8693494061336643,
      "grad_norm": 0.8200600147247314,
      "learning_rate": 0.00014204131801635704,
      "loss": 0.2562,
      "step": 12260
    },
    {
      "epoch": 0.8700585002659103,
      "grad_norm": 1.0799940824508667,
      "learning_rate": 0.00014199404339809956,
      "loss": 0.2127,
      "step": 12270
    },
    {
      "epoch": 0.8707675943981563,
      "grad_norm": 0.7020624279975891,
      "learning_rate": 0.00014194676877984209,
      "loss": 0.2339,
      "step": 12280
    },
    {
      "epoch": 0.8714766885304024,
      "grad_norm": 0.9640592336654663,
      "learning_rate": 0.00014189949416158466,
      "loss": 0.2276,
      "step": 12290
    },
    {
      "epoch": 0.8721857826626485,
      "grad_norm": 0.8264193534851074,
      "learning_rate": 0.00014185221954332719,
      "loss": 0.2859,
      "step": 12300
    },
    {
      "epoch": 0.8728948767948945,
      "grad_norm": 1.613169550895691,
      "learning_rate": 0.00014180494492506974,
      "loss": 0.274,
      "step": 12310
    },
    {
      "epoch": 0.8736039709271406,
      "grad_norm": 1.2075425386428833,
      "learning_rate": 0.00014175767030681229,
      "loss": 0.2569,
      "step": 12320
    },
    {
      "epoch": 0.8743130650593867,
      "grad_norm": 1.5869454145431519,
      "learning_rate": 0.0001417103956885548,
      "loss": 0.2723,
      "step": 12330
    },
    {
      "epoch": 0.8750221591916327,
      "grad_norm": 0.8059136867523193,
      "learning_rate": 0.00014166312107029736,
      "loss": 0.2515,
      "step": 12340
    },
    {
      "epoch": 0.8757312533238788,
      "grad_norm": 1.2749935388565063,
      "learning_rate": 0.0001416158464520399,
      "loss": 0.243,
      "step": 12350
    },
    {
      "epoch": 0.8764403474561248,
      "grad_norm": 0.9876733422279358,
      "learning_rate": 0.00014156857183378246,
      "loss": 0.2669,
      "step": 12360
    },
    {
      "epoch": 0.8771494415883708,
      "grad_norm": 0.9190427660942078,
      "learning_rate": 0.00014152129721552498,
      "loss": 0.2347,
      "step": 12370
    },
    {
      "epoch": 0.8778585357206169,
      "grad_norm": 1.7296797037124634,
      "learning_rate": 0.00014147402259726753,
      "loss": 0.2552,
      "step": 12380
    },
    {
      "epoch": 0.878567629852863,
      "grad_norm": 2.3120460510253906,
      "learning_rate": 0.00014142674797901008,
      "loss": 0.2519,
      "step": 12390
    },
    {
      "epoch": 0.879276723985109,
      "grad_norm": 0.8566023111343384,
      "learning_rate": 0.0001413794733607526,
      "loss": 0.2191,
      "step": 12400
    },
    {
      "epoch": 0.8799858181173551,
      "grad_norm": 0.8521362543106079,
      "learning_rate": 0.00014133219874249518,
      "loss": 0.2254,
      "step": 12410
    },
    {
      "epoch": 0.8806949122496012,
      "grad_norm": 0.6542296409606934,
      "learning_rate": 0.0001412849241242377,
      "loss": 0.2188,
      "step": 12420
    },
    {
      "epoch": 0.8814040063818472,
      "grad_norm": 0.9260262250900269,
      "learning_rate": 0.00014123764950598025,
      "loss": 0.2859,
      "step": 12430
    },
    {
      "epoch": 0.8821131005140932,
      "grad_norm": 2.3235673904418945,
      "learning_rate": 0.0001411903748877228,
      "loss": 0.2887,
      "step": 12440
    },
    {
      "epoch": 0.8828221946463393,
      "grad_norm": 1.968714714050293,
      "learning_rate": 0.00014114310026946532,
      "loss": 0.2576,
      "step": 12450
    },
    {
      "epoch": 0.8835312887785853,
      "grad_norm": 2.755988121032715,
      "learning_rate": 0.00014109582565120787,
      "loss": 0.2594,
      "step": 12460
    },
    {
      "epoch": 0.8842403829108314,
      "grad_norm": 1.270432949066162,
      "learning_rate": 0.00014104855103295042,
      "loss": 0.2726,
      "step": 12470
    },
    {
      "epoch": 0.8849494770430775,
      "grad_norm": 1.6817805767059326,
      "learning_rate": 0.00014100127641469297,
      "loss": 0.2616,
      "step": 12480
    },
    {
      "epoch": 0.8856585711753235,
      "grad_norm": 0.8182635307312012,
      "learning_rate": 0.0001409540017964355,
      "loss": 0.2699,
      "step": 12490
    },
    {
      "epoch": 0.8863676653075696,
      "grad_norm": 1.8121670484542847,
      "learning_rate": 0.00014090672717817802,
      "loss": 0.2427,
      "step": 12500
    },
    {
      "epoch": 0.8870767594398157,
      "grad_norm": 1.1333194971084595,
      "learning_rate": 0.0001408594525599206,
      "loss": 0.2579,
      "step": 12510
    },
    {
      "epoch": 0.8877858535720617,
      "grad_norm": 2.5755722522735596,
      "learning_rate": 0.00014081217794166312,
      "loss": 0.2621,
      "step": 12520
    },
    {
      "epoch": 0.8884949477043077,
      "grad_norm": 2.736685037612915,
      "learning_rate": 0.00014076490332340567,
      "loss": 0.243,
      "step": 12530
    },
    {
      "epoch": 0.8892040418365538,
      "grad_norm": 1.9201494455337524,
      "learning_rate": 0.00014071762870514822,
      "loss": 0.2539,
      "step": 12540
    },
    {
      "epoch": 0.8899131359687998,
      "grad_norm": 1.198137640953064,
      "learning_rate": 0.00014067035408689074,
      "loss": 0.2624,
      "step": 12550
    },
    {
      "epoch": 0.8906222301010459,
      "grad_norm": 1.5129913091659546,
      "learning_rate": 0.0001406230794686333,
      "loss": 0.2419,
      "step": 12560
    },
    {
      "epoch": 0.891331324233292,
      "grad_norm": 0.9726177453994751,
      "learning_rate": 0.00014057580485037584,
      "loss": 0.2618,
      "step": 12570
    },
    {
      "epoch": 0.892040418365538,
      "grad_norm": 1.1160212755203247,
      "learning_rate": 0.0001405285302321184,
      "loss": 0.2343,
      "step": 12580
    },
    {
      "epoch": 0.8927495124977841,
      "grad_norm": 0.6238462924957275,
      "learning_rate": 0.00014048125561386091,
      "loss": 0.2349,
      "step": 12590
    },
    {
      "epoch": 0.8934586066300302,
      "grad_norm": 2.7726516723632812,
      "learning_rate": 0.00014043398099560346,
      "loss": 0.2409,
      "step": 12600
    },
    {
      "epoch": 0.8941677007622761,
      "grad_norm": 1.0612653493881226,
      "learning_rate": 0.00014038670637734601,
      "loss": 0.2437,
      "step": 12610
    },
    {
      "epoch": 0.8948767948945222,
      "grad_norm": 0.8023272752761841,
      "learning_rate": 0.00014033943175908854,
      "loss": 0.2391,
      "step": 12620
    },
    {
      "epoch": 0.8955858890267683,
      "grad_norm": 1.676497459411621,
      "learning_rate": 0.00014029215714083111,
      "loss": 0.2371,
      "step": 12630
    },
    {
      "epoch": 0.8962949831590143,
      "grad_norm": 0.5880994200706482,
      "learning_rate": 0.00014024488252257364,
      "loss": 0.233,
      "step": 12640
    },
    {
      "epoch": 0.8970040772912604,
      "grad_norm": 0.6619063019752502,
      "learning_rate": 0.00014019760790431619,
      "loss": 0.2617,
      "step": 12650
    },
    {
      "epoch": 0.8977131714235065,
      "grad_norm": 0.8718125820159912,
      "learning_rate": 0.00014015033328605874,
      "loss": 0.2468,
      "step": 12660
    },
    {
      "epoch": 0.8984222655557526,
      "grad_norm": 1.4870803356170654,
      "learning_rate": 0.00014010305866780126,
      "loss": 0.2522,
      "step": 12670
    },
    {
      "epoch": 0.8991313596879986,
      "grad_norm": 1.6129509210586548,
      "learning_rate": 0.0001400557840495438,
      "loss": 0.2295,
      "step": 12680
    },
    {
      "epoch": 0.8998404538202447,
      "grad_norm": 0.9305228590965271,
      "learning_rate": 0.00014000850943128633,
      "loss": 0.265,
      "step": 12690
    },
    {
      "epoch": 0.9005495479524906,
      "grad_norm": 0.8991774320602417,
      "learning_rate": 0.0001399612348130289,
      "loss": 0.2736,
      "step": 12700
    },
    {
      "epoch": 0.9012586420847367,
      "grad_norm": 1.2571817636489868,
      "learning_rate": 0.00013991396019477143,
      "loss": 0.2056,
      "step": 12710
    },
    {
      "epoch": 0.9019677362169828,
      "grad_norm": 0.574806809425354,
      "learning_rate": 0.00013986668557651395,
      "loss": 0.2416,
      "step": 12720
    },
    {
      "epoch": 0.9026768303492289,
      "grad_norm": 1.4930261373519897,
      "learning_rate": 0.00013981941095825653,
      "loss": 0.2313,
      "step": 12730
    },
    {
      "epoch": 0.9033859244814749,
      "grad_norm": 2.045039653778076,
      "learning_rate": 0.00013977213633999905,
      "loss": 0.249,
      "step": 12740
    },
    {
      "epoch": 0.904095018613721,
      "grad_norm": 1.626879096031189,
      "learning_rate": 0.0001397248617217416,
      "loss": 0.2279,
      "step": 12750
    },
    {
      "epoch": 0.9048041127459671,
      "grad_norm": 0.7243241667747498,
      "learning_rate": 0.00013967758710348415,
      "loss": 0.2527,
      "step": 12760
    },
    {
      "epoch": 0.9055132068782131,
      "grad_norm": 0.6754537224769592,
      "learning_rate": 0.00013963031248522668,
      "loss": 0.2457,
      "step": 12770
    },
    {
      "epoch": 0.9062223010104591,
      "grad_norm": 1.0262051820755005,
      "learning_rate": 0.00013958303786696923,
      "loss": 0.2653,
      "step": 12780
    },
    {
      "epoch": 0.9069313951427052,
      "grad_norm": 1.4703974723815918,
      "learning_rate": 0.00013953576324871178,
      "loss": 0.2323,
      "step": 12790
    },
    {
      "epoch": 0.9076404892749512,
      "grad_norm": 1.063612461090088,
      "learning_rate": 0.00013948848863045433,
      "loss": 0.2449,
      "step": 12800
    },
    {
      "epoch": 0.9083495834071973,
      "grad_norm": 1.0870423316955566,
      "learning_rate": 0.00013944121401219685,
      "loss": 0.2556,
      "step": 12810
    },
    {
      "epoch": 0.9090586775394434,
      "grad_norm": 0.8722163438796997,
      "learning_rate": 0.0001393939393939394,
      "loss": 0.2424,
      "step": 12820
    },
    {
      "epoch": 0.9097677716716894,
      "grad_norm": 0.9614408612251282,
      "learning_rate": 0.00013934666477568195,
      "loss": 0.2496,
      "step": 12830
    },
    {
      "epoch": 0.9104768658039355,
      "grad_norm": 1.3492810726165771,
      "learning_rate": 0.00013929939015742447,
      "loss": 0.2427,
      "step": 12840
    },
    {
      "epoch": 0.9111859599361816,
      "grad_norm": 1.4551602602005005,
      "learning_rate": 0.00013925211553916705,
      "loss": 0.2566,
      "step": 12850
    },
    {
      "epoch": 0.9118950540684276,
      "grad_norm": 1.3190761804580688,
      "learning_rate": 0.00013920484092090957,
      "loss": 0.2129,
      "step": 12860
    },
    {
      "epoch": 0.9126041482006736,
      "grad_norm": 2.581979274749756,
      "learning_rate": 0.00013915756630265212,
      "loss": 0.2838,
      "step": 12870
    },
    {
      "epoch": 0.9133132423329197,
      "grad_norm": 1.040236234664917,
      "learning_rate": 0.00013911029168439464,
      "loss": 0.2482,
      "step": 12880
    },
    {
      "epoch": 0.9140223364651657,
      "grad_norm": 2.442641496658325,
      "learning_rate": 0.0001390630170661372,
      "loss": 0.2662,
      "step": 12890
    },
    {
      "epoch": 0.9147314305974118,
      "grad_norm": 1.122388243675232,
      "learning_rate": 0.00013901574244787974,
      "loss": 0.258,
      "step": 12900
    },
    {
      "epoch": 0.9154405247296579,
      "grad_norm": 1.273329257965088,
      "learning_rate": 0.00013896846782962227,
      "loss": 0.2468,
      "step": 12910
    },
    {
      "epoch": 0.9161496188619039,
      "grad_norm": 2.2125983238220215,
      "learning_rate": 0.00013892119321136484,
      "loss": 0.243,
      "step": 12920
    },
    {
      "epoch": 0.91685871299415,
      "grad_norm": 1.0524964332580566,
      "learning_rate": 0.00013887391859310737,
      "loss": 0.2424,
      "step": 12930
    },
    {
      "epoch": 0.9175678071263961,
      "grad_norm": 1.0975970029830933,
      "learning_rate": 0.00013882664397484991,
      "loss": 0.25,
      "step": 12940
    },
    {
      "epoch": 0.918276901258642,
      "grad_norm": 1.868206262588501,
      "learning_rate": 0.00013877936935659246,
      "loss": 0.258,
      "step": 12950
    },
    {
      "epoch": 0.9189859953908881,
      "grad_norm": 2.286139488220215,
      "learning_rate": 0.000138732094738335,
      "loss": 0.2194,
      "step": 12960
    },
    {
      "epoch": 0.9196950895231342,
      "grad_norm": 0.9560647010803223,
      "learning_rate": 0.00013868482012007754,
      "loss": 0.2252,
      "step": 12970
    },
    {
      "epoch": 0.9204041836553802,
      "grad_norm": 1.0392557382583618,
      "learning_rate": 0.0001386375455018201,
      "loss": 0.25,
      "step": 12980
    },
    {
      "epoch": 0.9211132777876263,
      "grad_norm": 1.1231414079666138,
      "learning_rate": 0.0001385902708835626,
      "loss": 0.2676,
      "step": 12990
    },
    {
      "epoch": 0.9218223719198724,
      "grad_norm": 1.2791646718978882,
      "learning_rate": 0.00013854299626530516,
      "loss": 0.2469,
      "step": 13000
    },
    {
      "epoch": 0.9225314660521184,
      "grad_norm": 1.8380159139633179,
      "learning_rate": 0.0001384957216470477,
      "loss": 0.2578,
      "step": 13010
    },
    {
      "epoch": 0.9232405601843645,
      "grad_norm": 1.4502508640289307,
      "learning_rate": 0.00013844844702879026,
      "loss": 0.2482,
      "step": 13020
    },
    {
      "epoch": 0.9239496543166106,
      "grad_norm": 0.943967342376709,
      "learning_rate": 0.00013840117241053278,
      "loss": 0.2548,
      "step": 13030
    },
    {
      "epoch": 0.9246587484488565,
      "grad_norm": 2.2189013957977295,
      "learning_rate": 0.00013835389779227533,
      "loss": 0.2183,
      "step": 13040
    },
    {
      "epoch": 0.9253678425811026,
      "grad_norm": 1.191894292831421,
      "learning_rate": 0.00013830662317401788,
      "loss": 0.2478,
      "step": 13050
    },
    {
      "epoch": 0.9260769367133487,
      "grad_norm": 1.165431261062622,
      "learning_rate": 0.0001382593485557604,
      "loss": 0.2431,
      "step": 13060
    },
    {
      "epoch": 0.9267860308455947,
      "grad_norm": 1.0029444694519043,
      "learning_rate": 0.00013821207393750295,
      "loss": 0.2412,
      "step": 13070
    },
    {
      "epoch": 0.9274951249778408,
      "grad_norm": 3.657796859741211,
      "learning_rate": 0.0001381647993192455,
      "loss": 0.2362,
      "step": 13080
    },
    {
      "epoch": 0.9282042191100869,
      "grad_norm": 2.417670488357544,
      "learning_rate": 0.00013811752470098805,
      "loss": 0.2666,
      "step": 13090
    },
    {
      "epoch": 0.928913313242333,
      "grad_norm": 1.0938384532928467,
      "learning_rate": 0.00013807025008273058,
      "loss": 0.2513,
      "step": 13100
    },
    {
      "epoch": 0.929622407374579,
      "grad_norm": 1.7158222198486328,
      "learning_rate": 0.00013802297546447313,
      "loss": 0.2885,
      "step": 13110
    },
    {
      "epoch": 0.930331501506825,
      "grad_norm": 0.9134015440940857,
      "learning_rate": 0.00013797570084621568,
      "loss": 0.2422,
      "step": 13120
    },
    {
      "epoch": 0.931040595639071,
      "grad_norm": 0.9336910843849182,
      "learning_rate": 0.0001379284262279582,
      "loss": 0.2315,
      "step": 13130
    },
    {
      "epoch": 0.9317496897713171,
      "grad_norm": 1.6837605237960815,
      "learning_rate": 0.00013788115160970078,
      "loss": 0.2467,
      "step": 13140
    },
    {
      "epoch": 0.9324587839035632,
      "grad_norm": 1.4695336818695068,
      "learning_rate": 0.0001378338769914433,
      "loss": 0.2586,
      "step": 13150
    },
    {
      "epoch": 0.9331678780358093,
      "grad_norm": 1.8148459196090698,
      "learning_rate": 0.00013778660237318585,
      "loss": 0.2984,
      "step": 13160
    },
    {
      "epoch": 0.9338769721680553,
      "grad_norm": 0.8988956212997437,
      "learning_rate": 0.0001377393277549284,
      "loss": 0.2506,
      "step": 13170
    },
    {
      "epoch": 0.9345860663003014,
      "grad_norm": 2.4320292472839355,
      "learning_rate": 0.00013769205313667092,
      "loss": 0.2673,
      "step": 13180
    },
    {
      "epoch": 0.9352951604325475,
      "grad_norm": 0.6655282974243164,
      "learning_rate": 0.00013764477851841347,
      "loss": 0.2581,
      "step": 13190
    },
    {
      "epoch": 0.9360042545647935,
      "grad_norm": 1.4796031713485718,
      "learning_rate": 0.00013759750390015602,
      "loss": 0.2771,
      "step": 13200
    },
    {
      "epoch": 0.9367133486970395,
      "grad_norm": 0.9679133296012878,
      "learning_rate": 0.00013755022928189854,
      "loss": 0.2378,
      "step": 13210
    },
    {
      "epoch": 0.9374224428292856,
      "grad_norm": 0.6192706823348999,
      "learning_rate": 0.0001375029546636411,
      "loss": 0.2528,
      "step": 13220
    },
    {
      "epoch": 0.9381315369615316,
      "grad_norm": 0.7831592559814453,
      "learning_rate": 0.00013745568004538364,
      "loss": 0.2304,
      "step": 13230
    },
    {
      "epoch": 0.9388406310937777,
      "grad_norm": 1.0137782096862793,
      "learning_rate": 0.0001374084054271262,
      "loss": 0.2213,
      "step": 13240
    },
    {
      "epoch": 0.9395497252260238,
      "grad_norm": 0.703059732913971,
      "learning_rate": 0.00013736113080886872,
      "loss": 0.277,
      "step": 13250
    },
    {
      "epoch": 0.9402588193582698,
      "grad_norm": 1.332207441329956,
      "learning_rate": 0.00013731385619061127,
      "loss": 0.2143,
      "step": 13260
    },
    {
      "epoch": 0.9409679134905159,
      "grad_norm": 0.801213800907135,
      "learning_rate": 0.00013726658157235382,
      "loss": 0.2324,
      "step": 13270
    },
    {
      "epoch": 0.941677007622762,
      "grad_norm": 0.7220724821090698,
      "learning_rate": 0.00013721930695409634,
      "loss": 0.2271,
      "step": 13280
    },
    {
      "epoch": 0.942386101755008,
      "grad_norm": 0.736362099647522,
      "learning_rate": 0.0001371720323358389,
      "loss": 0.2044,
      "step": 13290
    },
    {
      "epoch": 0.943095195887254,
      "grad_norm": 0.991612434387207,
      "learning_rate": 0.00013712475771758144,
      "loss": 0.2358,
      "step": 13300
    },
    {
      "epoch": 0.9438042900195001,
      "grad_norm": 1.1460353136062622,
      "learning_rate": 0.000137077483099324,
      "loss": 0.2519,
      "step": 13310
    },
    {
      "epoch": 0.9445133841517461,
      "grad_norm": 1.2630130052566528,
      "learning_rate": 0.0001370302084810665,
      "loss": 0.2538,
      "step": 13320
    },
    {
      "epoch": 0.9452224782839922,
      "grad_norm": 0.9787043333053589,
      "learning_rate": 0.00013698293386280906,
      "loss": 0.2601,
      "step": 13330
    },
    {
      "epoch": 0.9459315724162383,
      "grad_norm": 0.8020899295806885,
      "learning_rate": 0.0001369356592445516,
      "loss": 0.243,
      "step": 13340
    },
    {
      "epoch": 0.9466406665484843,
      "grad_norm": 0.8426200151443481,
      "learning_rate": 0.00013688838462629413,
      "loss": 0.2475,
      "step": 13350
    },
    {
      "epoch": 0.9473497606807304,
      "grad_norm": 1.7611031532287598,
      "learning_rate": 0.0001368411100080367,
      "loss": 0.2758,
      "step": 13360
    },
    {
      "epoch": 0.9480588548129765,
      "grad_norm": 1.4189374446868896,
      "learning_rate": 0.00013679383538977923,
      "loss": 0.2697,
      "step": 13370
    },
    {
      "epoch": 0.9487679489452224,
      "grad_norm": 1.277607798576355,
      "learning_rate": 0.00013674656077152178,
      "loss": 0.2421,
      "step": 13380
    },
    {
      "epoch": 0.9494770430774685,
      "grad_norm": 1.027039647102356,
      "learning_rate": 0.00013669928615326433,
      "loss": 0.2309,
      "step": 13390
    },
    {
      "epoch": 0.9501861372097146,
      "grad_norm": 1.0047738552093506,
      "learning_rate": 0.00013665201153500686,
      "loss": 0.2425,
      "step": 13400
    },
    {
      "epoch": 0.9508952313419606,
      "grad_norm": 1.6223583221435547,
      "learning_rate": 0.0001366047369167494,
      "loss": 0.2421,
      "step": 13410
    },
    {
      "epoch": 0.9516043254742067,
      "grad_norm": 3.140676975250244,
      "learning_rate": 0.00013655746229849196,
      "loss": 0.2236,
      "step": 13420
    },
    {
      "epoch": 0.9523134196064528,
      "grad_norm": 1.5801336765289307,
      "learning_rate": 0.00013651018768023448,
      "loss": 0.2445,
      "step": 13430
    },
    {
      "epoch": 0.9530225137386988,
      "grad_norm": 1.0683902502059937,
      "learning_rate": 0.00013646291306197703,
      "loss": 0.2237,
      "step": 13440
    },
    {
      "epoch": 0.9537316078709449,
      "grad_norm": 0.8091760873794556,
      "learning_rate": 0.00013641563844371958,
      "loss": 0.2352,
      "step": 13450
    },
    {
      "epoch": 0.954440702003191,
      "grad_norm": 1.6396925449371338,
      "learning_rate": 0.00013636836382546213,
      "loss": 0.2749,
      "step": 13460
    },
    {
      "epoch": 0.9551497961354369,
      "grad_norm": 0.5697876214981079,
      "learning_rate": 0.00013632108920720465,
      "loss": 0.2325,
      "step": 13470
    },
    {
      "epoch": 0.955858890267683,
      "grad_norm": 1.035517692565918,
      "learning_rate": 0.0001362738145889472,
      "loss": 0.236,
      "step": 13480
    },
    {
      "epoch": 0.9565679843999291,
      "grad_norm": 1.2843153476715088,
      "learning_rate": 0.00013622653997068975,
      "loss": 0.2599,
      "step": 13490
    },
    {
      "epoch": 0.9572770785321751,
      "grad_norm": 0.9173449277877808,
      "learning_rate": 0.00013617926535243227,
      "loss": 0.2526,
      "step": 13500
    },
    {
      "epoch": 0.9579861726644212,
      "grad_norm": 0.6047710180282593,
      "learning_rate": 0.00013613199073417482,
      "loss": 0.2664,
      "step": 13510
    },
    {
      "epoch": 0.9586952667966673,
      "grad_norm": 1.7032908201217651,
      "learning_rate": 0.00013608471611591737,
      "loss": 0.2136,
      "step": 13520
    },
    {
      "epoch": 0.9594043609289133,
      "grad_norm": 0.9175741076469421,
      "learning_rate": 0.00013603744149765992,
      "loss": 0.2232,
      "step": 13530
    },
    {
      "epoch": 0.9601134550611594,
      "grad_norm": 2.600078821182251,
      "learning_rate": 0.00013599016687940244,
      "loss": 0.2729,
      "step": 13540
    },
    {
      "epoch": 0.9608225491934054,
      "grad_norm": 1.0071855783462524,
      "learning_rate": 0.000135942892261145,
      "loss": 0.2523,
      "step": 13550
    },
    {
      "epoch": 0.9615316433256514,
      "grad_norm": 1.1857597827911377,
      "learning_rate": 0.00013589561764288754,
      "loss": 0.2707,
      "step": 13560
    },
    {
      "epoch": 0.9622407374578975,
      "grad_norm": 1.2934373617172241,
      "learning_rate": 0.00013584834302463007,
      "loss": 0.2737,
      "step": 13570
    },
    {
      "epoch": 0.9629498315901436,
      "grad_norm": 1.3065212965011597,
      "learning_rate": 0.00013580106840637264,
      "loss": 0.2461,
      "step": 13580
    },
    {
      "epoch": 0.9636589257223896,
      "grad_norm": 0.9839276075363159,
      "learning_rate": 0.00013575379378811517,
      "loss": 0.2391,
      "step": 13590
    },
    {
      "epoch": 0.9643680198546357,
      "grad_norm": 1.4434353113174438,
      "learning_rate": 0.00013570651916985772,
      "loss": 0.2317,
      "step": 13600
    },
    {
      "epoch": 0.9650771139868818,
      "grad_norm": 0.9650722146034241,
      "learning_rate": 0.00013565924455160027,
      "loss": 0.2457,
      "step": 13610
    },
    {
      "epoch": 0.9657862081191279,
      "grad_norm": 0.8986744284629822,
      "learning_rate": 0.0001356119699333428,
      "loss": 0.2433,
      "step": 13620
    },
    {
      "epoch": 0.9664953022513739,
      "grad_norm": 0.7044779658317566,
      "learning_rate": 0.00013556469531508534,
      "loss": 0.2631,
      "step": 13630
    },
    {
      "epoch": 0.9672043963836199,
      "grad_norm": 1.6996053457260132,
      "learning_rate": 0.0001355174206968279,
      "loss": 0.2496,
      "step": 13640
    },
    {
      "epoch": 0.967913490515866,
      "grad_norm": 1.0181713104248047,
      "learning_rate": 0.00013547014607857044,
      "loss": 0.2471,
      "step": 13650
    },
    {
      "epoch": 0.968622584648112,
      "grad_norm": 1.2194240093231201,
      "learning_rate": 0.00013542287146031296,
      "loss": 0.2544,
      "step": 13660
    },
    {
      "epoch": 0.9693316787803581,
      "grad_norm": 2.057694911956787,
      "learning_rate": 0.00013537559684205548,
      "loss": 0.2298,
      "step": 13670
    },
    {
      "epoch": 0.9700407729126042,
      "grad_norm": 1.6253694295883179,
      "learning_rate": 0.00013532832222379806,
      "loss": 0.2822,
      "step": 13680
    },
    {
      "epoch": 0.9707498670448502,
      "grad_norm": 1.7740089893341064,
      "learning_rate": 0.00013528104760554058,
      "loss": 0.237,
      "step": 13690
    },
    {
      "epoch": 0.9714589611770963,
      "grad_norm": 1.1634860038757324,
      "learning_rate": 0.00013523377298728313,
      "loss": 0.252,
      "step": 13700
    },
    {
      "epoch": 0.9721680553093424,
      "grad_norm": 0.8317580223083496,
      "learning_rate": 0.00013518649836902568,
      "loss": 0.2548,
      "step": 13710
    },
    {
      "epoch": 0.9728771494415883,
      "grad_norm": 1.8889964818954468,
      "learning_rate": 0.0001351392237507682,
      "loss": 0.2315,
      "step": 13720
    },
    {
      "epoch": 0.9735862435738344,
      "grad_norm": 2.146185874938965,
      "learning_rate": 0.00013509194913251076,
      "loss": 0.2919,
      "step": 13730
    },
    {
      "epoch": 0.9742953377060805,
      "grad_norm": 0.9962903261184692,
      "learning_rate": 0.0001350446745142533,
      "loss": 0.2382,
      "step": 13740
    },
    {
      "epoch": 0.9750044318383265,
      "grad_norm": 1.1027477979660034,
      "learning_rate": 0.00013499739989599586,
      "loss": 0.249,
      "step": 13750
    },
    {
      "epoch": 0.9757135259705726,
      "grad_norm": 0.9247071146965027,
      "learning_rate": 0.00013495012527773838,
      "loss": 0.2181,
      "step": 13760
    },
    {
      "epoch": 0.9764226201028187,
      "grad_norm": 1.6477683782577515,
      "learning_rate": 0.00013490285065948093,
      "loss": 0.2527,
      "step": 13770
    },
    {
      "epoch": 0.9771317142350647,
      "grad_norm": 0.9358848929405212,
      "learning_rate": 0.00013485557604122348,
      "loss": 0.2681,
      "step": 13780
    },
    {
      "epoch": 0.9778408083673108,
      "grad_norm": 1.1803926229476929,
      "learning_rate": 0.000134808301422966,
      "loss": 0.2311,
      "step": 13790
    },
    {
      "epoch": 0.9785499024995569,
      "grad_norm": 1.778221845626831,
      "learning_rate": 0.00013476102680470858,
      "loss": 0.2599,
      "step": 13800
    },
    {
      "epoch": 0.9792589966318028,
      "grad_norm": 1.3401380777359009,
      "learning_rate": 0.0001347137521864511,
      "loss": 0.2553,
      "step": 13810
    },
    {
      "epoch": 0.9799680907640489,
      "grad_norm": 1.9252877235412598,
      "learning_rate": 0.00013466647756819365,
      "loss": 0.2401,
      "step": 13820
    },
    {
      "epoch": 0.980677184896295,
      "grad_norm": 2.515717029571533,
      "learning_rate": 0.0001346192029499362,
      "loss": 0.2757,
      "step": 13830
    },
    {
      "epoch": 0.981386279028541,
      "grad_norm": 0.9768370985984802,
      "learning_rate": 0.00013457192833167872,
      "loss": 0.2431,
      "step": 13840
    },
    {
      "epoch": 0.9820953731607871,
      "grad_norm": 0.7197876572608948,
      "learning_rate": 0.00013452465371342127,
      "loss": 0.2623,
      "step": 13850
    },
    {
      "epoch": 0.9828044672930332,
      "grad_norm": 0.8311022520065308,
      "learning_rate": 0.0001344773790951638,
      "loss": 0.2334,
      "step": 13860
    },
    {
      "epoch": 0.9835135614252792,
      "grad_norm": 1.4245439767837524,
      "learning_rate": 0.00013443010447690637,
      "loss": 0.2283,
      "step": 13870
    },
    {
      "epoch": 0.9842226555575253,
      "grad_norm": 1.4449537992477417,
      "learning_rate": 0.0001343828298586489,
      "loss": 0.2823,
      "step": 13880
    },
    {
      "epoch": 0.9849317496897713,
      "grad_norm": 1.0182462930679321,
      "learning_rate": 0.00013433555524039142,
      "loss": 0.2237,
      "step": 13890
    },
    {
      "epoch": 0.9856408438220173,
      "grad_norm": 1.981530785560608,
      "learning_rate": 0.000134288280622134,
      "loss": 0.2459,
      "step": 13900
    },
    {
      "epoch": 0.9863499379542634,
      "grad_norm": 1.481943130493164,
      "learning_rate": 0.00013424100600387652,
      "loss": 0.2648,
      "step": 13910
    },
    {
      "epoch": 0.9870590320865095,
      "grad_norm": 0.6460914015769958,
      "learning_rate": 0.00013419373138561907,
      "loss": 0.2204,
      "step": 13920
    },
    {
      "epoch": 0.9877681262187555,
      "grad_norm": 0.5969856381416321,
      "learning_rate": 0.00013414645676736162,
      "loss": 0.2426,
      "step": 13930
    },
    {
      "epoch": 0.9884772203510016,
      "grad_norm": 1.1406534910202026,
      "learning_rate": 0.00013409918214910414,
      "loss": 0.2638,
      "step": 13940
    },
    {
      "epoch": 0.9891863144832477,
      "grad_norm": 2.0042216777801514,
      "learning_rate": 0.0001340519075308467,
      "loss": 0.2732,
      "step": 13950
    },
    {
      "epoch": 0.9898954086154937,
      "grad_norm": 2.4714295864105225,
      "learning_rate": 0.00013400463291258924,
      "loss": 0.2493,
      "step": 13960
    },
    {
      "epoch": 0.9906045027477398,
      "grad_norm": 0.6850094795227051,
      "learning_rate": 0.0001339573582943318,
      "loss": 0.2605,
      "step": 13970
    },
    {
      "epoch": 0.9913135968799858,
      "grad_norm": 1.7842782735824585,
      "learning_rate": 0.0001339100836760743,
      "loss": 0.2545,
      "step": 13980
    },
    {
      "epoch": 0.9920226910122318,
      "grad_norm": 1.576432228088379,
      "learning_rate": 0.00013386280905781686,
      "loss": 0.2698,
      "step": 13990
    },
    {
      "epoch": 0.9927317851444779,
      "grad_norm": 1.1958162784576416,
      "learning_rate": 0.0001338155344395594,
      "loss": 0.2597,
      "step": 14000
    },
    {
      "epoch": 0.993440879276724,
      "grad_norm": 1.1035503149032593,
      "learning_rate": 0.00013376825982130194,
      "loss": 0.2794,
      "step": 14010
    },
    {
      "epoch": 0.99414997340897,
      "grad_norm": 0.7380664944648743,
      "learning_rate": 0.0001337209852030445,
      "loss": 0.2339,
      "step": 14020
    },
    {
      "epoch": 0.9948590675412161,
      "grad_norm": 0.9895303249359131,
      "learning_rate": 0.00013367371058478704,
      "loss": 0.2542,
      "step": 14030
    },
    {
      "epoch": 0.9955681616734622,
      "grad_norm": 0.8424298167228699,
      "learning_rate": 0.00013362643596652958,
      "loss": 0.2657,
      "step": 14040
    },
    {
      "epoch": 0.9962772558057083,
      "grad_norm": 0.7396296262741089,
      "learning_rate": 0.0001335791613482721,
      "loss": 0.2473,
      "step": 14050
    },
    {
      "epoch": 0.9969863499379542,
      "grad_norm": 0.7499009966850281,
      "learning_rate": 0.00013353188673001466,
      "loss": 0.2424,
      "step": 14060
    },
    {
      "epoch": 0.9976954440702003,
      "grad_norm": 1.685076117515564,
      "learning_rate": 0.0001334846121117572,
      "loss": 0.2644,
      "step": 14070
    },
    {
      "epoch": 0.9984045382024463,
      "grad_norm": 0.5919407606124878,
      "learning_rate": 0.00013343733749349973,
      "loss": 0.232,
      "step": 14080
    },
    {
      "epoch": 0.9991136323346924,
      "grad_norm": 1.1098668575286865,
      "learning_rate": 0.0001333900628752423,
      "loss": 0.2499,
      "step": 14090
    },
    {
      "epoch": 0.9998227264669385,
      "grad_norm": 0.980714738368988,
      "learning_rate": 0.00013334278825698483,
      "loss": 0.216,
      "step": 14100
    },
    {
      "epoch": 1.0,
      "eval_runtime": 1271.3306,
      "eval_samples_per_second": 5.547,
      "eval_steps_per_second": 5.547,
      "step": 14103
    },
    {
      "epoch": 1.0004963658925723,
      "grad_norm": 1.2304562330245972,
      "learning_rate": 0.00013329551363872735,
      "loss": 0.237,
      "step": 14110
    },
    {
      "epoch": 1.0012054600248184,
      "grad_norm": 0.8696156740188599,
      "learning_rate": 0.00013324823902046993,
      "loss": 0.2164,
      "step": 14120
    },
    {
      "epoch": 1.0019145541570644,
      "grad_norm": 1.4314303398132324,
      "learning_rate": 0.00013320096440221245,
      "loss": 0.2658,
      "step": 14130
    },
    {
      "epoch": 1.0026236482893105,
      "grad_norm": 0.5996831655502319,
      "learning_rate": 0.000133153689783955,
      "loss": 0.2586,
      "step": 14140
    },
    {
      "epoch": 1.0033327424215566,
      "grad_norm": 1.812757968902588,
      "learning_rate": 0.00013310641516569755,
      "loss": 0.2495,
      "step": 14150
    },
    {
      "epoch": 1.0040418365538024,
      "grad_norm": 0.9168384671211243,
      "learning_rate": 0.00013305914054744007,
      "loss": 0.232,
      "step": 14160
    },
    {
      "epoch": 1.0047509306860485,
      "grad_norm": 0.7477282881736755,
      "learning_rate": 0.00013301186592918262,
      "loss": 0.1985,
      "step": 14170
    },
    {
      "epoch": 1.0054600248182946,
      "grad_norm": 1.3196337223052979,
      "learning_rate": 0.00013296459131092517,
      "loss": 0.2687,
      "step": 14180
    },
    {
      "epoch": 1.0061691189505406,
      "grad_norm": 0.9240666627883911,
      "learning_rate": 0.00013291731669266772,
      "loss": 0.2389,
      "step": 14190
    },
    {
      "epoch": 1.0068782130827867,
      "grad_norm": 0.9799751043319702,
      "learning_rate": 0.00013287004207441025,
      "loss": 0.2585,
      "step": 14200
    },
    {
      "epoch": 1.0075873072150328,
      "grad_norm": 0.933211088180542,
      "learning_rate": 0.0001328227674561528,
      "loss": 0.2319,
      "step": 14210
    },
    {
      "epoch": 1.0082964013472788,
      "grad_norm": 0.7164540886878967,
      "learning_rate": 0.00013277549283789535,
      "loss": 0.2258,
      "step": 14220
    },
    {
      "epoch": 1.009005495479525,
      "grad_norm": 1.041412353515625,
      "learning_rate": 0.00013272821821963787,
      "loss": 0.2521,
      "step": 14230
    },
    {
      "epoch": 1.009714589611771,
      "grad_norm": 1.2194857597351074,
      "learning_rate": 0.00013268094360138042,
      "loss": 0.2424,
      "step": 14240
    },
    {
      "epoch": 1.010423683744017,
      "grad_norm": 1.0649352073669434,
      "learning_rate": 0.00013263366898312297,
      "loss": 0.2407,
      "step": 14250
    },
    {
      "epoch": 1.011132777876263,
      "grad_norm": 1.594618320465088,
      "learning_rate": 0.00013258639436486552,
      "loss": 0.2627,
      "step": 14260
    },
    {
      "epoch": 1.0118418720085092,
      "grad_norm": 1.5777814388275146,
      "learning_rate": 0.00013253911974660804,
      "loss": 0.2428,
      "step": 14270
    },
    {
      "epoch": 1.0125509661407552,
      "grad_norm": 1.2280546426773071,
      "learning_rate": 0.0001324918451283506,
      "loss": 0.2817,
      "step": 14280
    },
    {
      "epoch": 1.0132600602730013,
      "grad_norm": 1.1756871938705444,
      "learning_rate": 0.00013244457051009314,
      "loss": 0.2661,
      "step": 14290
    },
    {
      "epoch": 1.0139691544052474,
      "grad_norm": 3.5678722858428955,
      "learning_rate": 0.00013239729589183566,
      "loss": 0.2821,
      "step": 14300
    },
    {
      "epoch": 1.0146782485374934,
      "grad_norm": 4.0620808601379395,
      "learning_rate": 0.00013235002127357824,
      "loss": 0.2202,
      "step": 14310
    },
    {
      "epoch": 1.0153873426697395,
      "grad_norm": 2.029639482498169,
      "learning_rate": 0.00013230274665532076,
      "loss": 0.2409,
      "step": 14320
    },
    {
      "epoch": 1.0160964368019854,
      "grad_norm": 0.7957785725593567,
      "learning_rate": 0.0001322554720370633,
      "loss": 0.2536,
      "step": 14330
    },
    {
      "epoch": 1.0168055309342314,
      "grad_norm": 2.6672937870025635,
      "learning_rate": 0.00013220819741880586,
      "loss": 0.2734,
      "step": 14340
    },
    {
      "epoch": 1.0175146250664775,
      "grad_norm": 2.1241466999053955,
      "learning_rate": 0.00013216092280054839,
      "loss": 0.2519,
      "step": 14350
    },
    {
      "epoch": 1.0182237191987236,
      "grad_norm": 0.9029992818832397,
      "learning_rate": 0.00013211364818229094,
      "loss": 0.2326,
      "step": 14360
    },
    {
      "epoch": 1.0189328133309696,
      "grad_norm": 0.9147493243217468,
      "learning_rate": 0.00013206637356403349,
      "loss": 0.246,
      "step": 14370
    },
    {
      "epoch": 1.0196419074632157,
      "grad_norm": 1.3794463872909546,
      "learning_rate": 0.000132019098945776,
      "loss": 0.2767,
      "step": 14380
    },
    {
      "epoch": 1.0203510015954618,
      "grad_norm": 1.0305551290512085,
      "learning_rate": 0.00013197182432751856,
      "loss": 0.2685,
      "step": 14390
    },
    {
      "epoch": 1.0210600957277078,
      "grad_norm": 1.3397835493087769,
      "learning_rate": 0.0001319245497092611,
      "loss": 0.2545,
      "step": 14400
    },
    {
      "epoch": 1.021769189859954,
      "grad_norm": 0.8641742467880249,
      "learning_rate": 0.00013187727509100366,
      "loss": 0.2363,
      "step": 14410
    },
    {
      "epoch": 1.0224782839922,
      "grad_norm": 0.7331773638725281,
      "learning_rate": 0.00013183000047274618,
      "loss": 0.288,
      "step": 14420
    },
    {
      "epoch": 1.023187378124446,
      "grad_norm": 1.6191548109054565,
      "learning_rate": 0.00013178272585448873,
      "loss": 0.2489,
      "step": 14430
    },
    {
      "epoch": 1.0238964722566921,
      "grad_norm": 0.8766141533851624,
      "learning_rate": 0.00013173545123623128,
      "loss": 0.2698,
      "step": 14440
    },
    {
      "epoch": 1.0246055663889382,
      "grad_norm": 1.1073769330978394,
      "learning_rate": 0.0001316881766179738,
      "loss": 0.2439,
      "step": 14450
    },
    {
      "epoch": 1.0253146605211843,
      "grad_norm": 0.9068568348884583,
      "learning_rate": 0.00013164090199971635,
      "loss": 0.2461,
      "step": 14460
    },
    {
      "epoch": 1.0260237546534303,
      "grad_norm": 0.7014873623847961,
      "learning_rate": 0.0001315936273814589,
      "loss": 0.2656,
      "step": 14470
    },
    {
      "epoch": 1.0267328487856764,
      "grad_norm": 1.3084332942962646,
      "learning_rate": 0.00013154635276320145,
      "loss": 0.2352,
      "step": 14480
    },
    {
      "epoch": 1.0274419429179225,
      "grad_norm": 0.7442516088485718,
      "learning_rate": 0.00013149907814494398,
      "loss": 0.219,
      "step": 14490
    },
    {
      "epoch": 1.0281510370501683,
      "grad_norm": 0.9433134198188782,
      "learning_rate": 0.00013145180352668653,
      "loss": 0.231,
      "step": 14500
    },
    {
      "epoch": 1.0288601311824144,
      "grad_norm": 0.6342024803161621,
      "learning_rate": 0.00013140452890842908,
      "loss": 0.2629,
      "step": 14510
    },
    {
      "epoch": 1.0295692253146604,
      "grad_norm": 1.172966718673706,
      "learning_rate": 0.0001313572542901716,
      "loss": 0.2592,
      "step": 14520
    },
    {
      "epoch": 1.0302783194469065,
      "grad_norm": 0.6926741003990173,
      "learning_rate": 0.00013130997967191417,
      "loss": 0.2525,
      "step": 14530
    },
    {
      "epoch": 1.0309874135791526,
      "grad_norm": 0.7830218076705933,
      "learning_rate": 0.0001312627050536567,
      "loss": 0.2666,
      "step": 14540
    },
    {
      "epoch": 1.0316965077113986,
      "grad_norm": 2.1012728214263916,
      "learning_rate": 0.00013121543043539922,
      "loss": 0.2293,
      "step": 14550
    },
    {
      "epoch": 1.0324056018436447,
      "grad_norm": 1.7216825485229492,
      "learning_rate": 0.0001311681558171418,
      "loss": 0.279,
      "step": 14560
    },
    {
      "epoch": 1.0331146959758908,
      "grad_norm": 0.8763328194618225,
      "learning_rate": 0.00013112088119888432,
      "loss": 0.2462,
      "step": 14570
    },
    {
      "epoch": 1.0338237901081369,
      "grad_norm": 1.3993428945541382,
      "learning_rate": 0.00013107360658062687,
      "loss": 0.2532,
      "step": 14580
    },
    {
      "epoch": 1.034532884240383,
      "grad_norm": 2.2084805965423584,
      "learning_rate": 0.00013102633196236942,
      "loss": 0.226,
      "step": 14590
    },
    {
      "epoch": 1.035241978372629,
      "grad_norm": 0.7040901184082031,
      "learning_rate": 0.00013097905734411194,
      "loss": 0.2595,
      "step": 14600
    },
    {
      "epoch": 1.035951072504875,
      "grad_norm": 1.3138623237609863,
      "learning_rate": 0.0001309317827258545,
      "loss": 0.2534,
      "step": 14610
    },
    {
      "epoch": 1.0366601666371211,
      "grad_norm": 1.0502525568008423,
      "learning_rate": 0.00013088450810759704,
      "loss": 0.2446,
      "step": 14620
    },
    {
      "epoch": 1.0373692607693672,
      "grad_norm": 1.163550853729248,
      "learning_rate": 0.0001308372334893396,
      "loss": 0.2648,
      "step": 14630
    },
    {
      "epoch": 1.0380783549016133,
      "grad_norm": 1.3639445304870605,
      "learning_rate": 0.00013078995887108211,
      "loss": 0.2637,
      "step": 14640
    },
    {
      "epoch": 1.0387874490338593,
      "grad_norm": 0.6226866245269775,
      "learning_rate": 0.00013074268425282466,
      "loss": 0.2348,
      "step": 14650
    },
    {
      "epoch": 1.0394965431661054,
      "grad_norm": 1.3192495107650757,
      "learning_rate": 0.00013069540963456721,
      "loss": 0.2609,
      "step": 14660
    },
    {
      "epoch": 1.0402056372983512,
      "grad_norm": 0.9023005366325378,
      "learning_rate": 0.00013064813501630974,
      "loss": 0.2322,
      "step": 14670
    },
    {
      "epoch": 1.0409147314305973,
      "grad_norm": 0.906557559967041,
      "learning_rate": 0.0001306008603980523,
      "loss": 0.2558,
      "step": 14680
    },
    {
      "epoch": 1.0416238255628434,
      "grad_norm": 1.0220463275909424,
      "learning_rate": 0.00013055358577979484,
      "loss": 0.248,
      "step": 14690
    },
    {
      "epoch": 1.0423329196950895,
      "grad_norm": 0.8942673206329346,
      "learning_rate": 0.0001305063111615374,
      "loss": 0.2314,
      "step": 14700
    },
    {
      "epoch": 1.0430420138273355,
      "grad_norm": 0.8305621147155762,
      "learning_rate": 0.0001304590365432799,
      "loss": 0.2448,
      "step": 14710
    },
    {
      "epoch": 1.0437511079595816,
      "grad_norm": 1.0683610439300537,
      "learning_rate": 0.00013041176192502246,
      "loss": 0.238,
      "step": 14720
    },
    {
      "epoch": 1.0444602020918277,
      "grad_norm": 1.885252594947815,
      "learning_rate": 0.000130364487306765,
      "loss": 0.2335,
      "step": 14730
    },
    {
      "epoch": 1.0451692962240737,
      "grad_norm": 0.9838663339614868,
      "learning_rate": 0.00013031721268850753,
      "loss": 0.2209,
      "step": 14740
    },
    {
      "epoch": 1.0458783903563198,
      "grad_norm": 0.8577383756637573,
      "learning_rate": 0.0001302699380702501,
      "loss": 0.2462,
      "step": 14750
    },
    {
      "epoch": 1.0465874844885659,
      "grad_norm": 1.883824110031128,
      "learning_rate": 0.00013022266345199263,
      "loss": 0.227,
      "step": 14760
    },
    {
      "epoch": 1.047296578620812,
      "grad_norm": 1.2932766675949097,
      "learning_rate": 0.00013017538883373515,
      "loss": 0.2418,
      "step": 14770
    },
    {
      "epoch": 1.048005672753058,
      "grad_norm": 1.4730055332183838,
      "learning_rate": 0.00013012811421547773,
      "loss": 0.2505,
      "step": 14780
    },
    {
      "epoch": 1.048714766885304,
      "grad_norm": 0.6850978136062622,
      "learning_rate": 0.00013008083959722025,
      "loss": 0.2464,
      "step": 14790
    },
    {
      "epoch": 1.0494238610175501,
      "grad_norm": 1.5496702194213867,
      "learning_rate": 0.0001300335649789628,
      "loss": 0.2742,
      "step": 14800
    },
    {
      "epoch": 1.0501329551497962,
      "grad_norm": 1.7079881429672241,
      "learning_rate": 0.00012998629036070535,
      "loss": 0.2636,
      "step": 14810
    },
    {
      "epoch": 1.0508420492820423,
      "grad_norm": 0.6661628484725952,
      "learning_rate": 0.00012993901574244788,
      "loss": 0.2353,
      "step": 14820
    },
    {
      "epoch": 1.0515511434142883,
      "grad_norm": 0.809563398361206,
      "learning_rate": 0.00012989174112419043,
      "loss": 0.2236,
      "step": 14830
    },
    {
      "epoch": 1.0522602375465344,
      "grad_norm": 0.8369494080543518,
      "learning_rate": 0.00012984446650593295,
      "loss": 0.265,
      "step": 14840
    },
    {
      "epoch": 1.0529693316787803,
      "grad_norm": 1.266332745552063,
      "learning_rate": 0.00012979719188767553,
      "loss": 0.2582,
      "step": 14850
    },
    {
      "epoch": 1.0536784258110263,
      "grad_norm": 1.6990188360214233,
      "learning_rate": 0.00012974991726941805,
      "loss": 0.2658,
      "step": 14860
    },
    {
      "epoch": 1.0543875199432724,
      "grad_norm": 0.8672745227813721,
      "learning_rate": 0.0001297026426511606,
      "loss": 0.2236,
      "step": 14870
    },
    {
      "epoch": 1.0550966140755185,
      "grad_norm": 1.7413581609725952,
      "learning_rate": 0.00012965536803290315,
      "loss": 0.2632,
      "step": 14880
    },
    {
      "epoch": 1.0558057082077645,
      "grad_norm": 2.694524049758911,
      "learning_rate": 0.00012960809341464567,
      "loss": 0.2537,
      "step": 14890
    },
    {
      "epoch": 1.0565148023400106,
      "grad_norm": 1.1157147884368896,
      "learning_rate": 0.00012956081879638822,
      "loss": 0.256,
      "step": 14900
    },
    {
      "epoch": 1.0572238964722567,
      "grad_norm": 1.3664824962615967,
      "learning_rate": 0.00012951354417813077,
      "loss": 0.2804,
      "step": 14910
    },
    {
      "epoch": 1.0579329906045027,
      "grad_norm": 1.8330423831939697,
      "learning_rate": 0.00012946626955987332,
      "loss": 0.2451,
      "step": 14920
    },
    {
      "epoch": 1.0586420847367488,
      "grad_norm": 2.3652756214141846,
      "learning_rate": 0.00012941899494161584,
      "loss": 0.2735,
      "step": 14930
    },
    {
      "epoch": 1.0593511788689949,
      "grad_norm": 3.528486490249634,
      "learning_rate": 0.0001293717203233584,
      "loss": 0.2935,
      "step": 14940
    },
    {
      "epoch": 1.060060273001241,
      "grad_norm": 2.0811328887939453,
      "learning_rate": 0.00012932444570510094,
      "loss": 0.2371,
      "step": 14950
    },
    {
      "epoch": 1.060769367133487,
      "grad_norm": 1.6415331363677979,
      "learning_rate": 0.00012927717108684347,
      "loss": 0.2561,
      "step": 14960
    },
    {
      "epoch": 1.061478461265733,
      "grad_norm": 1.645750880241394,
      "learning_rate": 0.00012922989646858604,
      "loss": 0.2621,
      "step": 14970
    },
    {
      "epoch": 1.0621875553979792,
      "grad_norm": 2.2211103439331055,
      "learning_rate": 0.00012918262185032857,
      "loss": 0.2407,
      "step": 14980
    },
    {
      "epoch": 1.0628966495302252,
      "grad_norm": 0.6565585732460022,
      "learning_rate": 0.00012913534723207112,
      "loss": 0.2621,
      "step": 14990
    },
    {
      "epoch": 1.0636057436624713,
      "grad_norm": 1.1605701446533203,
      "learning_rate": 0.00012908807261381367,
      "loss": 0.2515,
      "step": 15000
    },
    {
      "epoch": 1.0643148377947171,
      "grad_norm": 0.9789660573005676,
      "learning_rate": 0.0001290407979955562,
      "loss": 0.2759,
      "step": 15010
    },
    {
      "epoch": 1.0650239319269632,
      "grad_norm": 1.5264685153961182,
      "learning_rate": 0.00012899352337729874,
      "loss": 0.2232,
      "step": 15020
    },
    {
      "epoch": 1.0657330260592093,
      "grad_norm": 0.9007886648178101,
      "learning_rate": 0.00012894624875904126,
      "loss": 0.2735,
      "step": 15030
    },
    {
      "epoch": 1.0664421201914553,
      "grad_norm": 0.9712132811546326,
      "learning_rate": 0.0001288989741407838,
      "loss": 0.2754,
      "step": 15040
    },
    {
      "epoch": 1.0671512143237014,
      "grad_norm": 1.268060564994812,
      "learning_rate": 0.00012885169952252636,
      "loss": 0.2518,
      "step": 15050
    },
    {
      "epoch": 1.0678603084559475,
      "grad_norm": 0.7567052245140076,
      "learning_rate": 0.00012880442490426888,
      "loss": 0.2409,
      "step": 15060
    },
    {
      "epoch": 1.0685694025881936,
      "grad_norm": 0.7359235882759094,
      "learning_rate": 0.00012875715028601146,
      "loss": 0.2504,
      "step": 15070
    },
    {
      "epoch": 1.0692784967204396,
      "grad_norm": 0.7522855997085571,
      "learning_rate": 0.00012870987566775398,
      "loss": 0.2785,
      "step": 15080
    },
    {
      "epoch": 1.0699875908526857,
      "grad_norm": 1.2674596309661865,
      "learning_rate": 0.00012866260104949653,
      "loss": 0.2398,
      "step": 15090
    },
    {
      "epoch": 1.0706966849849318,
      "grad_norm": 0.806283175945282,
      "learning_rate": 0.00012861532643123908,
      "loss": 0.2481,
      "step": 15100
    },
    {
      "epoch": 1.0714057791171778,
      "grad_norm": 1.7004379034042358,
      "learning_rate": 0.0001285680518129816,
      "loss": 0.2474,
      "step": 15110
    },
    {
      "epoch": 1.072114873249424,
      "grad_norm": 0.7670198082923889,
      "learning_rate": 0.00012852077719472416,
      "loss": 0.2381,
      "step": 15120
    },
    {
      "epoch": 1.07282396738167,
      "grad_norm": 4.800727844238281,
      "learning_rate": 0.0001284735025764667,
      "loss": 0.2134,
      "step": 15130
    },
    {
      "epoch": 1.073533061513916,
      "grad_norm": 1.4420236349105835,
      "learning_rate": 0.00012842622795820925,
      "loss": 0.2383,
      "step": 15140
    },
    {
      "epoch": 1.074242155646162,
      "grad_norm": 2.22133731842041,
      "learning_rate": 0.00012837895333995178,
      "loss": 0.245,
      "step": 15150
    },
    {
      "epoch": 1.0749512497784082,
      "grad_norm": 1.167350172996521,
      "learning_rate": 0.00012833167872169433,
      "loss": 0.252,
      "step": 15160
    },
    {
      "epoch": 1.0756603439106542,
      "grad_norm": 2.5940475463867188,
      "learning_rate": 0.00012828440410343688,
      "loss": 0.2762,
      "step": 15170
    },
    {
      "epoch": 1.0763694380429003,
      "grad_norm": 1.3347444534301758,
      "learning_rate": 0.0001282371294851794,
      "loss": 0.2719,
      "step": 15180
    },
    {
      "epoch": 1.0770785321751462,
      "grad_norm": 1.9290603399276733,
      "learning_rate": 0.00012818985486692198,
      "loss": 0.2489,
      "step": 15190
    },
    {
      "epoch": 1.0777876263073922,
      "grad_norm": 1.1651545763015747,
      "learning_rate": 0.0001281425802486645,
      "loss": 0.2201,
      "step": 15200
    },
    {
      "epoch": 1.0784967204396383,
      "grad_norm": 0.7160441279411316,
      "learning_rate": 0.00012809530563040705,
      "loss": 0.2485,
      "step": 15210
    },
    {
      "epoch": 1.0792058145718844,
      "grad_norm": 1.2046030759811401,
      "learning_rate": 0.0001280480310121496,
      "loss": 0.2761,
      "step": 15220
    },
    {
      "epoch": 1.0799149087041304,
      "grad_norm": 0.762433648109436,
      "learning_rate": 0.00012800075639389212,
      "loss": 0.2313,
      "step": 15230
    },
    {
      "epoch": 1.0806240028363765,
      "grad_norm": 3.286942958831787,
      "learning_rate": 0.00012795348177563467,
      "loss": 0.2349,
      "step": 15240
    },
    {
      "epoch": 1.0813330969686226,
      "grad_norm": 0.8638556003570557,
      "learning_rate": 0.0001279062071573772,
      "loss": 0.2527,
      "step": 15250
    },
    {
      "epoch": 1.0820421911008686,
      "grad_norm": 2.8402891159057617,
      "learning_rate": 0.00012785893253911974,
      "loss": 0.2684,
      "step": 15260
    },
    {
      "epoch": 1.0827512852331147,
      "grad_norm": 1.783704161643982,
      "learning_rate": 0.0001278116579208623,
      "loss": 0.2372,
      "step": 15270
    },
    {
      "epoch": 1.0834603793653608,
      "grad_norm": 1.5024082660675049,
      "learning_rate": 0.00012776438330260482,
      "loss": 0.2503,
      "step": 15280
    },
    {
      "epoch": 1.0841694734976068,
      "grad_norm": 1.8201326131820679,
      "learning_rate": 0.0001277171086843474,
      "loss": 0.2442,
      "step": 15290
    },
    {
      "epoch": 1.084878567629853,
      "grad_norm": 0.7991639971733093,
      "learning_rate": 0.00012766983406608992,
      "loss": 0.2226,
      "step": 15300
    },
    {
      "epoch": 1.085587661762099,
      "grad_norm": 1.5700844526290894,
      "learning_rate": 0.00012762255944783247,
      "loss": 0.2543,
      "step": 15310
    },
    {
      "epoch": 1.086296755894345,
      "grad_norm": 1.045088291168213,
      "learning_rate": 0.00012757528482957502,
      "loss": 0.2395,
      "step": 15320
    },
    {
      "epoch": 1.0870058500265911,
      "grad_norm": 3.443014144897461,
      "learning_rate": 0.00012752801021131754,
      "loss": 0.2727,
      "step": 15330
    },
    {
      "epoch": 1.0877149441588372,
      "grad_norm": 2.3341259956359863,
      "learning_rate": 0.0001274807355930601,
      "loss": 0.2421,
      "step": 15340
    },
    {
      "epoch": 1.088424038291083,
      "grad_norm": 1.420196771621704,
      "learning_rate": 0.00012743346097480264,
      "loss": 0.2623,
      "step": 15350
    },
    {
      "epoch": 1.089133132423329,
      "grad_norm": 0.9766802787780762,
      "learning_rate": 0.0001273861863565452,
      "loss": 0.2619,
      "step": 15360
    },
    {
      "epoch": 1.0898422265555752,
      "grad_norm": 1.3900059461593628,
      "learning_rate": 0.0001273389117382877,
      "loss": 0.258,
      "step": 15370
    },
    {
      "epoch": 1.0905513206878212,
      "grad_norm": 1.2256466150283813,
      "learning_rate": 0.00012729163712003026,
      "loss": 0.2673,
      "step": 15380
    },
    {
      "epoch": 1.0912604148200673,
      "grad_norm": 0.8139193058013916,
      "learning_rate": 0.0001272443625017728,
      "loss": 0.208,
      "step": 15390
    },
    {
      "epoch": 1.0919695089523134,
      "grad_norm": 0.769254207611084,
      "learning_rate": 0.00012719708788351533,
      "loss": 0.2547,
      "step": 15400
    },
    {
      "epoch": 1.0926786030845594,
      "grad_norm": 1.0286478996276855,
      "learning_rate": 0.0001271498132652579,
      "loss": 0.2219,
      "step": 15410
    },
    {
      "epoch": 1.0933876972168055,
      "grad_norm": 0.6797981858253479,
      "learning_rate": 0.00012710253864700043,
      "loss": 0.2509,
      "step": 15420
    },
    {
      "epoch": 1.0940967913490516,
      "grad_norm": 1.1922975778579712,
      "learning_rate": 0.00012705526402874298,
      "loss": 0.2223,
      "step": 15430
    },
    {
      "epoch": 1.0948058854812976,
      "grad_norm": 2.264411449432373,
      "learning_rate": 0.0001270079894104855,
      "loss": 0.2527,
      "step": 15440
    },
    {
      "epoch": 1.0955149796135437,
      "grad_norm": 0.7820180654525757,
      "learning_rate": 0.00012696071479222806,
      "loss": 0.2655,
      "step": 15450
    },
    {
      "epoch": 1.0962240737457898,
      "grad_norm": 1.1811271905899048,
      "learning_rate": 0.0001269134401739706,
      "loss": 0.2289,
      "step": 15460
    },
    {
      "epoch": 1.0969331678780359,
      "grad_norm": 1.8451827764511108,
      "learning_rate": 0.00012686616555571313,
      "loss": 0.245,
      "step": 15470
    },
    {
      "epoch": 1.097642262010282,
      "grad_norm": 1.0825998783111572,
      "learning_rate": 0.0001268188909374557,
      "loss": 0.2513,
      "step": 15480
    },
    {
      "epoch": 1.098351356142528,
      "grad_norm": 0.7798982858657837,
      "learning_rate": 0.00012677161631919823,
      "loss": 0.2816,
      "step": 15490
    },
    {
      "epoch": 1.099060450274774,
      "grad_norm": 2.4628241062164307,
      "learning_rate": 0.00012672434170094075,
      "loss": 0.229,
      "step": 15500
    },
    {
      "epoch": 1.0997695444070201,
      "grad_norm": 1.040265440940857,
      "learning_rate": 0.00012667706708268333,
      "loss": 0.2548,
      "step": 15510
    },
    {
      "epoch": 1.1004786385392662,
      "grad_norm": 0.8775821328163147,
      "learning_rate": 0.00012662979246442585,
      "loss": 0.2699,
      "step": 15520
    },
    {
      "epoch": 1.101187732671512,
      "grad_norm": 1.9647125005722046,
      "learning_rate": 0.0001265825178461684,
      "loss": 0.2427,
      "step": 15530
    },
    {
      "epoch": 1.1018968268037581,
      "grad_norm": 0.8984100222587585,
      "learning_rate": 0.00012653524322791095,
      "loss": 0.2316,
      "step": 15540
    },
    {
      "epoch": 1.1026059209360042,
      "grad_norm": 3.6603288650512695,
      "learning_rate": 0.00012648796860965347,
      "loss": 0.25,
      "step": 15550
    },
    {
      "epoch": 1.1033150150682502,
      "grad_norm": 0.6330990195274353,
      "learning_rate": 0.00012644069399139602,
      "loss": 0.2325,
      "step": 15560
    },
    {
      "epoch": 1.1040241092004963,
      "grad_norm": 0.6598163843154907,
      "learning_rate": 0.00012639341937313857,
      "loss": 0.2389,
      "step": 15570
    },
    {
      "epoch": 1.1047332033327424,
      "grad_norm": 0.9255403876304626,
      "learning_rate": 0.00012634614475488112,
      "loss": 0.2662,
      "step": 15580
    },
    {
      "epoch": 1.1054422974649885,
      "grad_norm": 1.0067358016967773,
      "learning_rate": 0.00012629887013662365,
      "loss": 0.2272,
      "step": 15590
    },
    {
      "epoch": 1.1061513915972345,
      "grad_norm": 0.9823452830314636,
      "learning_rate": 0.0001262515955183662,
      "loss": 0.2944,
      "step": 15600
    },
    {
      "epoch": 1.1068604857294806,
      "grad_norm": 1.189488410949707,
      "learning_rate": 0.00012620432090010875,
      "loss": 0.2715,
      "step": 15610
    },
    {
      "epoch": 1.1075695798617267,
      "grad_norm": 1.791190266609192,
      "learning_rate": 0.00012615704628185127,
      "loss": 0.2348,
      "step": 15620
    },
    {
      "epoch": 1.1082786739939727,
      "grad_norm": 1.669103980064392,
      "learning_rate": 0.00012610977166359382,
      "loss": 0.2426,
      "step": 15630
    },
    {
      "epoch": 1.1089877681262188,
      "grad_norm": 1.1891741752624512,
      "learning_rate": 0.00012606249704533637,
      "loss": 0.2497,
      "step": 15640
    },
    {
      "epoch": 1.1096968622584649,
      "grad_norm": 0.8810122013092041,
      "learning_rate": 0.00012601522242707892,
      "loss": 0.2334,
      "step": 15650
    },
    {
      "epoch": 1.110405956390711,
      "grad_norm": 1.4874449968338013,
      "learning_rate": 0.00012596794780882144,
      "loss": 0.2548,
      "step": 15660
    },
    {
      "epoch": 1.111115050522957,
      "grad_norm": 0.9919830560684204,
      "learning_rate": 0.000125920673190564,
      "loss": 0.2284,
      "step": 15670
    },
    {
      "epoch": 1.111824144655203,
      "grad_norm": 1.2539201974868774,
      "learning_rate": 0.00012587339857230654,
      "loss": 0.2686,
      "step": 15680
    },
    {
      "epoch": 1.112533238787449,
      "grad_norm": 0.8894598484039307,
      "learning_rate": 0.00012582612395404906,
      "loss": 0.2397,
      "step": 15690
    },
    {
      "epoch": 1.113242332919695,
      "grad_norm": 1.144235610961914,
      "learning_rate": 0.00012577884933579164,
      "loss": 0.2754,
      "step": 15700
    },
    {
      "epoch": 1.113951427051941,
      "grad_norm": 2.1590967178344727,
      "learning_rate": 0.00012573157471753416,
      "loss": 0.2307,
      "step": 15710
    },
    {
      "epoch": 1.1146605211841871,
      "grad_norm": 4.642242908477783,
      "learning_rate": 0.00012568430009927669,
      "loss": 0.3039,
      "step": 15720
    },
    {
      "epoch": 1.1153696153164332,
      "grad_norm": 0.890163779258728,
      "learning_rate": 0.00012563702548101926,
      "loss": 0.221,
      "step": 15730
    },
    {
      "epoch": 1.1160787094486793,
      "grad_norm": 0.9399550557136536,
      "learning_rate": 0.00012558975086276178,
      "loss": 0.2463,
      "step": 15740
    },
    {
      "epoch": 1.1167878035809253,
      "grad_norm": 0.9348794221878052,
      "learning_rate": 0.00012554247624450433,
      "loss": 0.2466,
      "step": 15750
    },
    {
      "epoch": 1.1174968977131714,
      "grad_norm": 0.7237389087677002,
      "learning_rate": 0.00012549520162624688,
      "loss": 0.2552,
      "step": 15760
    },
    {
      "epoch": 1.1182059918454175,
      "grad_norm": 1.1354823112487793,
      "learning_rate": 0.0001254479270079894,
      "loss": 0.2363,
      "step": 15770
    },
    {
      "epoch": 1.1189150859776635,
      "grad_norm": 0.6457058191299438,
      "learning_rate": 0.00012540065238973196,
      "loss": 0.2414,
      "step": 15780
    },
    {
      "epoch": 1.1196241801099096,
      "grad_norm": 0.8723481297492981,
      "learning_rate": 0.0001253533777714745,
      "loss": 0.2325,
      "step": 15790
    },
    {
      "epoch": 1.1203332742421557,
      "grad_norm": 1.0700321197509766,
      "learning_rate": 0.00012530610315321706,
      "loss": 0.2427,
      "step": 15800
    },
    {
      "epoch": 1.1210423683744017,
      "grad_norm": 0.8561028242111206,
      "learning_rate": 0.00012525882853495958,
      "loss": 0.2415,
      "step": 15810
    },
    {
      "epoch": 1.1217514625066478,
      "grad_norm": 0.9819020628929138,
      "learning_rate": 0.00012521155391670213,
      "loss": 0.2656,
      "step": 15820
    },
    {
      "epoch": 1.1224605566388939,
      "grad_norm": 1.2525231838226318,
      "learning_rate": 0.00012516427929844468,
      "loss": 0.2362,
      "step": 15830
    },
    {
      "epoch": 1.12316965077114,
      "grad_norm": 1.67595636844635,
      "learning_rate": 0.0001251170046801872,
      "loss": 0.2586,
      "step": 15840
    },
    {
      "epoch": 1.123878744903386,
      "grad_norm": 1.3018146753311157,
      "learning_rate": 0.00012506973006192975,
      "loss": 0.2334,
      "step": 15850
    },
    {
      "epoch": 1.124587839035632,
      "grad_norm": 1.213213324546814,
      "learning_rate": 0.0001250224554436723,
      "loss": 0.248,
      "step": 15860
    },
    {
      "epoch": 1.1252969331678782,
      "grad_norm": 0.8748878836631775,
      "learning_rate": 0.00012497518082541485,
      "loss": 0.2486,
      "step": 15870
    },
    {
      "epoch": 1.126006027300124,
      "grad_norm": 1.7499791383743286,
      "learning_rate": 0.00012492790620715737,
      "loss": 0.2651,
      "step": 15880
    },
    {
      "epoch": 1.12671512143237,
      "grad_norm": 1.1067140102386475,
      "learning_rate": 0.00012488063158889992,
      "loss": 0.2584,
      "step": 15890
    },
    {
      "epoch": 1.1274242155646161,
      "grad_norm": 0.8775479197502136,
      "learning_rate": 0.00012483335697064247,
      "loss": 0.2383,
      "step": 15900
    },
    {
      "epoch": 1.1281333096968622,
      "grad_norm": 0.7191738486289978,
      "learning_rate": 0.000124786082352385,
      "loss": 0.2394,
      "step": 15910
    },
    {
      "epoch": 1.1288424038291083,
      "grad_norm": 0.5594625473022461,
      "learning_rate": 0.00012473880773412757,
      "loss": 0.2773,
      "step": 15920
    },
    {
      "epoch": 1.1295514979613543,
      "grad_norm": 0.7545547485351562,
      "learning_rate": 0.0001246915331158701,
      "loss": 0.2738,
      "step": 15930
    },
    {
      "epoch": 1.1302605920936004,
      "grad_norm": 1.5744683742523193,
      "learning_rate": 0.00012464425849761262,
      "loss": 0.2259,
      "step": 15940
    },
    {
      "epoch": 1.1309696862258465,
      "grad_norm": 1.3277344703674316,
      "learning_rate": 0.0001245969838793552,
      "loss": 0.2234,
      "step": 15950
    },
    {
      "epoch": 1.1316787803580926,
      "grad_norm": 0.9991459846496582,
      "learning_rate": 0.00012454970926109772,
      "loss": 0.2319,
      "step": 15960
    },
    {
      "epoch": 1.1323878744903386,
      "grad_norm": 0.9745956659317017,
      "learning_rate": 0.00012450243464284027,
      "loss": 0.2222,
      "step": 15970
    },
    {
      "epoch": 1.1330969686225847,
      "grad_norm": 0.9038827419281006,
      "learning_rate": 0.00012445516002458282,
      "loss": 0.2576,
      "step": 15980
    },
    {
      "epoch": 1.1338060627548308,
      "grad_norm": 1.203466534614563,
      "learning_rate": 0.00012440788540632534,
      "loss": 0.2363,
      "step": 15990
    },
    {
      "epoch": 1.1345151568870768,
      "grad_norm": 0.7907895445823669,
      "learning_rate": 0.0001243606107880679,
      "loss": 0.2658,
      "step": 16000
    },
    {
      "epoch": 1.135224251019323,
      "grad_norm": 1.1562116146087646,
      "learning_rate": 0.00012431333616981041,
      "loss": 0.2403,
      "step": 16010
    },
    {
      "epoch": 1.135933345151569,
      "grad_norm": 0.7678064107894897,
      "learning_rate": 0.000124266061551553,
      "loss": 0.2284,
      "step": 16020
    },
    {
      "epoch": 1.1366424392838148,
      "grad_norm": 0.8288400173187256,
      "learning_rate": 0.0001242187869332955,
      "loss": 0.2617,
      "step": 16030
    },
    {
      "epoch": 1.1373515334160609,
      "grad_norm": 1.020033359527588,
      "learning_rate": 0.00012417151231503806,
      "loss": 0.2296,
      "step": 16040
    },
    {
      "epoch": 1.138060627548307,
      "grad_norm": 1.5814002752304077,
      "learning_rate": 0.0001241242376967806,
      "loss": 0.2322,
      "step": 16050
    },
    {
      "epoch": 1.138769721680553,
      "grad_norm": 1.4066911935806274,
      "learning_rate": 0.00012407696307852314,
      "loss": 0.252,
      "step": 16060
    },
    {
      "epoch": 1.139478815812799,
      "grad_norm": 4.24123477935791,
      "learning_rate": 0.00012402968846026569,
      "loss": 0.253,
      "step": 16070
    },
    {
      "epoch": 1.1401879099450452,
      "grad_norm": 0.994007408618927,
      "learning_rate": 0.00012398241384200824,
      "loss": 0.2625,
      "step": 16080
    },
    {
      "epoch": 1.1408970040772912,
      "grad_norm": 2.3079612255096436,
      "learning_rate": 0.00012393513922375079,
      "loss": 0.2266,
      "step": 16090
    },
    {
      "epoch": 1.1416060982095373,
      "grad_norm": 3.142051935195923,
      "learning_rate": 0.0001238878646054933,
      "loss": 0.2975,
      "step": 16100
    },
    {
      "epoch": 1.1423151923417834,
      "grad_norm": 3.0184943675994873,
      "learning_rate": 0.00012384058998723586,
      "loss": 0.2567,
      "step": 16110
    },
    {
      "epoch": 1.1430242864740294,
      "grad_norm": 3.6286568641662598,
      "learning_rate": 0.0001237933153689784,
      "loss": 0.2901,
      "step": 16120
    },
    {
      "epoch": 1.1437333806062755,
      "grad_norm": 1.326451063156128,
      "learning_rate": 0.00012374604075072093,
      "loss": 0.2566,
      "step": 16130
    },
    {
      "epoch": 1.1444424747385216,
      "grad_norm": 1.7596076726913452,
      "learning_rate": 0.0001236987661324635,
      "loss": 0.2664,
      "step": 16140
    },
    {
      "epoch": 1.1451515688707676,
      "grad_norm": 1.9462083578109741,
      "learning_rate": 0.00012365149151420603,
      "loss": 0.2697,
      "step": 16150
    },
    {
      "epoch": 1.1458606630030137,
      "grad_norm": 1.4283733367919922,
      "learning_rate": 0.00012360421689594855,
      "loss": 0.2385,
      "step": 16160
    },
    {
      "epoch": 1.1465697571352598,
      "grad_norm": 1.598924994468689,
      "learning_rate": 0.00012355694227769113,
      "loss": 0.2316,
      "step": 16170
    },
    {
      "epoch": 1.1472788512675058,
      "grad_norm": 1.4820126295089722,
      "learning_rate": 0.00012350966765943365,
      "loss": 0.2785,
      "step": 16180
    },
    {
      "epoch": 1.147987945399752,
      "grad_norm": 0.9918639659881592,
      "learning_rate": 0.0001234623930411762,
      "loss": 0.2578,
      "step": 16190
    },
    {
      "epoch": 1.148697039531998,
      "grad_norm": 0.9582116007804871,
      "learning_rate": 0.00012341511842291875,
      "loss": 0.2295,
      "step": 16200
    },
    {
      "epoch": 1.149406133664244,
      "grad_norm": 1.4605860710144043,
      "learning_rate": 0.00012336784380466128,
      "loss": 0.2496,
      "step": 16210
    },
    {
      "epoch": 1.15011522779649,
      "grad_norm": 1.0699305534362793,
      "learning_rate": 0.00012332056918640382,
      "loss": 0.2398,
      "step": 16220
    },
    {
      "epoch": 1.150824321928736,
      "grad_norm": 0.9037033915519714,
      "learning_rate": 0.00012327329456814635,
      "loss": 0.2353,
      "step": 16230
    },
    {
      "epoch": 1.151533416060982,
      "grad_norm": 0.854241132736206,
      "learning_rate": 0.00012322601994988892,
      "loss": 0.2336,
      "step": 16240
    },
    {
      "epoch": 1.152242510193228,
      "grad_norm": 1.6266120672225952,
      "learning_rate": 0.00012317874533163145,
      "loss": 0.2623,
      "step": 16250
    },
    {
      "epoch": 1.1529516043254742,
      "grad_norm": 1.3150204420089722,
      "learning_rate": 0.000123131470713374,
      "loss": 0.2397,
      "step": 16260
    },
    {
      "epoch": 1.1536606984577202,
      "grad_norm": 1.961772084236145,
      "learning_rate": 0.00012308419609511655,
      "loss": 0.2397,
      "step": 16270
    },
    {
      "epoch": 1.1543697925899663,
      "grad_norm": 3.2649316787719727,
      "learning_rate": 0.00012303692147685907,
      "loss": 0.2726,
      "step": 16280
    },
    {
      "epoch": 1.1550788867222124,
      "grad_norm": 2.1552927494049072,
      "learning_rate": 0.00012298964685860162,
      "loss": 0.2387,
      "step": 16290
    },
    {
      "epoch": 1.1557879808544584,
      "grad_norm": 2.0597243309020996,
      "learning_rate": 0.00012294237224034417,
      "loss": 0.2419,
      "step": 16300
    },
    {
      "epoch": 1.1564970749867045,
      "grad_norm": 0.6652858257293701,
      "learning_rate": 0.00012289509762208672,
      "loss": 0.2302,
      "step": 16310
    },
    {
      "epoch": 1.1572061691189506,
      "grad_norm": 0.901979386806488,
      "learning_rate": 0.00012284782300382924,
      "loss": 0.2199,
      "step": 16320
    },
    {
      "epoch": 1.1579152632511966,
      "grad_norm": 0.5646016001701355,
      "learning_rate": 0.0001228005483855718,
      "loss": 0.245,
      "step": 16330
    },
    {
      "epoch": 1.1586243573834427,
      "grad_norm": 0.8896788954734802,
      "learning_rate": 0.00012275327376731434,
      "loss": 0.2428,
      "step": 16340
    },
    {
      "epoch": 1.1593334515156888,
      "grad_norm": 0.9737521409988403,
      "learning_rate": 0.00012270599914905686,
      "loss": 0.2491,
      "step": 16350
    },
    {
      "epoch": 1.1600425456479349,
      "grad_norm": 0.8252059817314148,
      "learning_rate": 0.00012265872453079944,
      "loss": 0.2292,
      "step": 16360
    },
    {
      "epoch": 1.1607516397801807,
      "grad_norm": 0.9705498218536377,
      "learning_rate": 0.00012261144991254196,
      "loss": 0.1993,
      "step": 16370
    },
    {
      "epoch": 1.1614607339124268,
      "grad_norm": 2.115494966506958,
      "learning_rate": 0.0001225641752942845,
      "loss": 0.251,
      "step": 16380
    },
    {
      "epoch": 1.1621698280446728,
      "grad_norm": 1.7845549583435059,
      "learning_rate": 0.00012251690067602706,
      "loss": 0.2527,
      "step": 16390
    },
    {
      "epoch": 1.162878922176919,
      "grad_norm": 2.858286142349243,
      "learning_rate": 0.0001224696260577696,
      "loss": 0.2259,
      "step": 16400
    },
    {
      "epoch": 1.163588016309165,
      "grad_norm": 1.1271014213562012,
      "learning_rate": 0.00012242235143951214,
      "loss": 0.2615,
      "step": 16410
    },
    {
      "epoch": 1.164297110441411,
      "grad_norm": 1.8857659101486206,
      "learning_rate": 0.00012237507682125466,
      "loss": 0.2504,
      "step": 16420
    },
    {
      "epoch": 1.1650062045736571,
      "grad_norm": 1.455970287322998,
      "learning_rate": 0.0001223278022029972,
      "loss": 0.2487,
      "step": 16430
    },
    {
      "epoch": 1.1657152987059032,
      "grad_norm": 1.058323621749878,
      "learning_rate": 0.00012228052758473976,
      "loss": 0.2251,
      "step": 16440
    },
    {
      "epoch": 1.1664243928381492,
      "grad_norm": 0.7626782655715942,
      "learning_rate": 0.00012223325296648228,
      "loss": 0.2423,
      "step": 16450
    },
    {
      "epoch": 1.1671334869703953,
      "grad_norm": 0.6967688798904419,
      "learning_rate": 0.00012218597834822486,
      "loss": 0.2316,
      "step": 16460
    },
    {
      "epoch": 1.1678425811026414,
      "grad_norm": 1.6098406314849854,
      "learning_rate": 0.00012213870372996738,
      "loss": 0.2604,
      "step": 16470
    },
    {
      "epoch": 1.1685516752348875,
      "grad_norm": 0.7153118252754211,
      "learning_rate": 0.00012209142911170993,
      "loss": 0.242,
      "step": 16480
    },
    {
      "epoch": 1.1692607693671335,
      "grad_norm": 0.6410768628120422,
      "learning_rate": 0.00012204415449345248,
      "loss": 0.2554,
      "step": 16490
    },
    {
      "epoch": 1.1699698634993796,
      "grad_norm": 0.8089541792869568,
      "learning_rate": 0.00012199687987519502,
      "loss": 0.2636,
      "step": 16500
    },
    {
      "epoch": 1.1706789576316257,
      "grad_norm": 4.162511825561523,
      "learning_rate": 0.00012194960525693755,
      "loss": 0.2726,
      "step": 16510
    },
    {
      "epoch": 1.1713880517638717,
      "grad_norm": 0.838782787322998,
      "learning_rate": 0.0001219023306386801,
      "loss": 0.2675,
      "step": 16520
    },
    {
      "epoch": 1.1720971458961178,
      "grad_norm": 1.77890944480896,
      "learning_rate": 0.00012185505602042264,
      "loss": 0.2436,
      "step": 16530
    },
    {
      "epoch": 1.1728062400283639,
      "grad_norm": 0.9315917491912842,
      "learning_rate": 0.00012180778140216518,
      "loss": 0.2237,
      "step": 16540
    },
    {
      "epoch": 1.17351533416061,
      "grad_norm": 2.8212499618530273,
      "learning_rate": 0.00012176050678390774,
      "loss": 0.2506,
      "step": 16550
    },
    {
      "epoch": 1.1742244282928558,
      "grad_norm": 1.1263854503631592,
      "learning_rate": 0.00012171323216565028,
      "loss": 0.2722,
      "step": 16560
    },
    {
      "epoch": 1.1749335224251019,
      "grad_norm": 1.7797173261642456,
      "learning_rate": 0.00012166595754739281,
      "loss": 0.2416,
      "step": 16570
    },
    {
      "epoch": 1.175642616557348,
      "grad_norm": 1.076849102973938,
      "learning_rate": 0.00012161868292913536,
      "loss": 0.2642,
      "step": 16580
    },
    {
      "epoch": 1.176351710689594,
      "grad_norm": 0.9032282829284668,
      "learning_rate": 0.0001215714083108779,
      "loss": 0.2534,
      "step": 16590
    },
    {
      "epoch": 1.17706080482184,
      "grad_norm": 1.2606779336929321,
      "learning_rate": 0.00012152413369262043,
      "loss": 0.2294,
      "step": 16600
    },
    {
      "epoch": 1.1777698989540861,
      "grad_norm": 0.9015619158744812,
      "learning_rate": 0.00012147685907436297,
      "loss": 0.2288,
      "step": 16610
    },
    {
      "epoch": 1.1784789930863322,
      "grad_norm": 2.0114805698394775,
      "learning_rate": 0.00012142958445610552,
      "loss": 0.2354,
      "step": 16620
    },
    {
      "epoch": 1.1791880872185783,
      "grad_norm": 0.6953428387641907,
      "learning_rate": 0.00012138230983784806,
      "loss": 0.2475,
      "step": 16630
    },
    {
      "epoch": 1.1798971813508243,
      "grad_norm": 3.624041795730591,
      "learning_rate": 0.0001213350352195906,
      "loss": 0.2217,
      "step": 16640
    },
    {
      "epoch": 1.1806062754830704,
      "grad_norm": 0.9178798198699951,
      "learning_rate": 0.00012128776060133316,
      "loss": 0.2763,
      "step": 16650
    },
    {
      "epoch": 1.1813153696153165,
      "grad_norm": 1.318859577178955,
      "learning_rate": 0.00012124048598307569,
      "loss": 0.2033,
      "step": 16660
    },
    {
      "epoch": 1.1820244637475625,
      "grad_norm": 2.6506190299987793,
      "learning_rate": 0.00012119321136481823,
      "loss": 0.2851,
      "step": 16670
    },
    {
      "epoch": 1.1827335578798086,
      "grad_norm": 2.0075907707214355,
      "learning_rate": 0.00012114593674656078,
      "loss": 0.2141,
      "step": 16680
    },
    {
      "epoch": 1.1834426520120547,
      "grad_norm": 1.3911163806915283,
      "learning_rate": 0.00012109866212830332,
      "loss": 0.2797,
      "step": 16690
    },
    {
      "epoch": 1.1841517461443007,
      "grad_norm": 0.8287601470947266,
      "learning_rate": 0.00012105138751004585,
      "loss": 0.2259,
      "step": 16700
    },
    {
      "epoch": 1.1848608402765466,
      "grad_norm": 1.6156858205795288,
      "learning_rate": 0.00012100411289178842,
      "loss": 0.2433,
      "step": 16710
    },
    {
      "epoch": 1.1855699344087927,
      "grad_norm": 1.0829092264175415,
      "learning_rate": 0.00012095683827353095,
      "loss": 0.2323,
      "step": 16720
    },
    {
      "epoch": 1.1862790285410387,
      "grad_norm": 0.6861928105354309,
      "learning_rate": 0.00012090956365527349,
      "loss": 0.2344,
      "step": 16730
    },
    {
      "epoch": 1.1869881226732848,
      "grad_norm": 0.7823585867881775,
      "learning_rate": 0.00012086228903701604,
      "loss": 0.2249,
      "step": 16740
    },
    {
      "epoch": 1.1876972168055309,
      "grad_norm": 1.57119619846344,
      "learning_rate": 0.00012081501441875857,
      "loss": 0.2518,
      "step": 16750
    },
    {
      "epoch": 1.188406310937777,
      "grad_norm": 0.7738785743713379,
      "learning_rate": 0.00012076773980050111,
      "loss": 0.2447,
      "step": 16760
    },
    {
      "epoch": 1.189115405070023,
      "grad_norm": 2.4318976402282715,
      "learning_rate": 0.00012072046518224367,
      "loss": 0.2784,
      "step": 16770
    },
    {
      "epoch": 1.189824499202269,
      "grad_norm": 2.1899783611297607,
      "learning_rate": 0.00012067319056398621,
      "loss": 0.259,
      "step": 16780
    },
    {
      "epoch": 1.1905335933345151,
      "grad_norm": 1.6239827871322632,
      "learning_rate": 0.00012062591594572875,
      "loss": 0.2931,
      "step": 16790
    },
    {
      "epoch": 1.1912426874667612,
      "grad_norm": 1.6456782817840576,
      "learning_rate": 0.00012057864132747128,
      "loss": 0.2481,
      "step": 16800
    },
    {
      "epoch": 1.1919517815990073,
      "grad_norm": 0.7303892970085144,
      "learning_rate": 0.00012053136670921383,
      "loss": 0.2399,
      "step": 16810
    },
    {
      "epoch": 1.1926608757312533,
      "grad_norm": 1.060102105140686,
      "learning_rate": 0.00012048409209095637,
      "loss": 0.2516,
      "step": 16820
    },
    {
      "epoch": 1.1933699698634994,
      "grad_norm": 0.7790232300758362,
      "learning_rate": 0.0001204368174726989,
      "loss": 0.2283,
      "step": 16830
    },
    {
      "epoch": 1.1940790639957455,
      "grad_norm": 0.8997341394424438,
      "learning_rate": 0.00012038954285444147,
      "loss": 0.2438,
      "step": 16840
    },
    {
      "epoch": 1.1947881581279916,
      "grad_norm": 0.7346848845481873,
      "learning_rate": 0.00012034226823618399,
      "loss": 0.2139,
      "step": 16850
    },
    {
      "epoch": 1.1954972522602376,
      "grad_norm": 0.7206915616989136,
      "learning_rate": 0.00012029499361792653,
      "loss": 0.2387,
      "step": 16860
    },
    {
      "epoch": 1.1962063463924837,
      "grad_norm": 0.7292442321777344,
      "learning_rate": 0.00012024771899966909,
      "loss": 0.2362,
      "step": 16870
    },
    {
      "epoch": 1.1969154405247298,
      "grad_norm": 5.910409927368164,
      "learning_rate": 0.00012020044438141163,
      "loss": 0.2853,
      "step": 16880
    },
    {
      "epoch": 1.1976245346569758,
      "grad_norm": 2.633518695831299,
      "learning_rate": 0.00012015316976315416,
      "loss": 0.2128,
      "step": 16890
    },
    {
      "epoch": 1.1983336287892217,
      "grad_norm": 1.3432459831237793,
      "learning_rate": 0.00012010589514489671,
      "loss": 0.259,
      "step": 16900
    },
    {
      "epoch": 1.1990427229214677,
      "grad_norm": 1.1293765306472778,
      "learning_rate": 0.00012005862052663925,
      "loss": 0.2544,
      "step": 16910
    },
    {
      "epoch": 1.1997518170537138,
      "grad_norm": 0.9696042537689209,
      "learning_rate": 0.00012001134590838179,
      "loss": 0.2354,
      "step": 16920
    },
    {
      "epoch": 1.2004609111859599,
      "grad_norm": 0.9864296317100525,
      "learning_rate": 0.00011996407129012435,
      "loss": 0.2307,
      "step": 16930
    },
    {
      "epoch": 1.201170005318206,
      "grad_norm": 1.9116761684417725,
      "learning_rate": 0.00011991679667186689,
      "loss": 0.2593,
      "step": 16940
    },
    {
      "epoch": 1.201879099450452,
      "grad_norm": 0.8195117115974426,
      "learning_rate": 0.00011986952205360942,
      "loss": 0.2799,
      "step": 16950
    },
    {
      "epoch": 1.202588193582698,
      "grad_norm": 0.9836366772651672,
      "learning_rate": 0.00011982224743535197,
      "loss": 0.2508,
      "step": 16960
    },
    {
      "epoch": 1.2032972877149442,
      "grad_norm": 1.5521936416625977,
      "learning_rate": 0.00011977497281709451,
      "loss": 0.2353,
      "step": 16970
    },
    {
      "epoch": 1.2040063818471902,
      "grad_norm": 0.9194507598876953,
      "learning_rate": 0.00011972769819883704,
      "loss": 0.2423,
      "step": 16980
    },
    {
      "epoch": 1.2047154759794363,
      "grad_norm": 1.1908836364746094,
      "learning_rate": 0.00011968042358057958,
      "loss": 0.2595,
      "step": 16990
    },
    {
      "epoch": 1.2054245701116824,
      "grad_norm": 1.4054445028305054,
      "learning_rate": 0.00011963314896232214,
      "loss": 0.2445,
      "step": 17000
    },
    {
      "epoch": 1.2061336642439284,
      "grad_norm": 1.0124859809875488,
      "learning_rate": 0.00011958587434406468,
      "loss": 0.2511,
      "step": 17010
    },
    {
      "epoch": 1.2068427583761745,
      "grad_norm": 1.2734591960906982,
      "learning_rate": 0.00011953859972580722,
      "loss": 0.2572,
      "step": 17020
    },
    {
      "epoch": 1.2075518525084206,
      "grad_norm": 0.7567850351333618,
      "learning_rate": 0.00011949132510754977,
      "loss": 0.217,
      "step": 17030
    },
    {
      "epoch": 1.2082609466406666,
      "grad_norm": 1.6336649656295776,
      "learning_rate": 0.0001194440504892923,
      "loss": 0.2899,
      "step": 17040
    },
    {
      "epoch": 1.2089700407729125,
      "grad_norm": 0.7459844350814819,
      "learning_rate": 0.00011939677587103484,
      "loss": 0.2355,
      "step": 17050
    },
    {
      "epoch": 1.2096791349051585,
      "grad_norm": 1.1576039791107178,
      "learning_rate": 0.0001193495012527774,
      "loss": 0.2317,
      "step": 17060
    },
    {
      "epoch": 1.2103882290374046,
      "grad_norm": 4.614727020263672,
      "learning_rate": 0.00011930222663451992,
      "loss": 0.2752,
      "step": 17070
    },
    {
      "epoch": 1.2110973231696507,
      "grad_norm": 0.7424911856651306,
      "learning_rate": 0.00011925495201626246,
      "loss": 0.2633,
      "step": 17080
    },
    {
      "epoch": 1.2118064173018968,
      "grad_norm": 1.1038845777511597,
      "learning_rate": 0.00011920767739800502,
      "loss": 0.2457,
      "step": 17090
    },
    {
      "epoch": 1.2125155114341428,
      "grad_norm": 0.8677740097045898,
      "learning_rate": 0.00011916040277974756,
      "loss": 0.2696,
      "step": 17100
    },
    {
      "epoch": 1.213224605566389,
      "grad_norm": 0.7990552186965942,
      "learning_rate": 0.0001191131281614901,
      "loss": 0.2528,
      "step": 17110
    },
    {
      "epoch": 1.213933699698635,
      "grad_norm": 2.4069719314575195,
      "learning_rate": 0.00011906585354323265,
      "loss": 0.2426,
      "step": 17120
    },
    {
      "epoch": 1.214642793830881,
      "grad_norm": 1.1784309148788452,
      "learning_rate": 0.00011901857892497518,
      "loss": 0.2728,
      "step": 17130
    },
    {
      "epoch": 1.215351887963127,
      "grad_norm": 1.7464203834533691,
      "learning_rate": 0.00011897130430671772,
      "loss": 0.2549,
      "step": 17140
    },
    {
      "epoch": 1.2160609820953732,
      "grad_norm": 0.987793505191803,
      "learning_rate": 0.00011892402968846028,
      "loss": 0.2806,
      "step": 17150
    },
    {
      "epoch": 1.2167700762276192,
      "grad_norm": 3.8833420276641846,
      "learning_rate": 0.00011887675507020282,
      "loss": 0.2673,
      "step": 17160
    },
    {
      "epoch": 1.2174791703598653,
      "grad_norm": 3.4914963245391846,
      "learning_rate": 0.00011882948045194536,
      "loss": 0.2773,
      "step": 17170
    },
    {
      "epoch": 1.2181882644921114,
      "grad_norm": 1.6613850593566895,
      "learning_rate": 0.0001187822058336879,
      "loss": 0.2444,
      "step": 17180
    },
    {
      "epoch": 1.2188973586243574,
      "grad_norm": 0.9671303033828735,
      "learning_rate": 0.00011873493121543044,
      "loss": 0.2352,
      "step": 17190
    },
    {
      "epoch": 1.2196064527566035,
      "grad_norm": 0.8288170099258423,
      "learning_rate": 0.00011868765659717298,
      "loss": 0.2422,
      "step": 17200
    },
    {
      "epoch": 1.2203155468888496,
      "grad_norm": 0.7658805251121521,
      "learning_rate": 0.00011864038197891551,
      "loss": 0.2293,
      "step": 17210
    },
    {
      "epoch": 1.2210246410210956,
      "grad_norm": 1.0612329244613647,
      "learning_rate": 0.00011859310736065808,
      "loss": 0.2397,
      "step": 17220
    },
    {
      "epoch": 1.2217337351533417,
      "grad_norm": 1.0533448457717896,
      "learning_rate": 0.00011854583274240061,
      "loss": 0.231,
      "step": 17230
    },
    {
      "epoch": 1.2224428292855878,
      "grad_norm": 0.6766390800476074,
      "learning_rate": 0.00011849855812414315,
      "loss": 0.2351,
      "step": 17240
    },
    {
      "epoch": 1.2231519234178336,
      "grad_norm": 0.7028149366378784,
      "learning_rate": 0.0001184512835058857,
      "loss": 0.2241,
      "step": 17250
    },
    {
      "epoch": 1.2238610175500797,
      "grad_norm": 0.9476851224899292,
      "learning_rate": 0.00011840400888762824,
      "loss": 0.2308,
      "step": 17260
    },
    {
      "epoch": 1.2245701116823258,
      "grad_norm": 0.9856279492378235,
      "learning_rate": 0.00011835673426937077,
      "loss": 0.2442,
      "step": 17270
    },
    {
      "epoch": 1.2252792058145718,
      "grad_norm": 1.8581502437591553,
      "learning_rate": 0.00011830945965111334,
      "loss": 0.2485,
      "step": 17280
    },
    {
      "epoch": 1.225988299946818,
      "grad_norm": 0.6961978077888489,
      "learning_rate": 0.00011826218503285586,
      "loss": 0.2322,
      "step": 17290
    },
    {
      "epoch": 1.226697394079064,
      "grad_norm": 0.9575114250183105,
      "learning_rate": 0.0001182149104145984,
      "loss": 0.2484,
      "step": 17300
    },
    {
      "epoch": 1.22740648821131,
      "grad_norm": 0.8936164379119873,
      "learning_rate": 0.00011816763579634096,
      "loss": 0.2441,
      "step": 17310
    },
    {
      "epoch": 1.2281155823435561,
      "grad_norm": 1.2015665769577026,
      "learning_rate": 0.0001181203611780835,
      "loss": 0.2353,
      "step": 17320
    },
    {
      "epoch": 1.2288246764758022,
      "grad_norm": 1.4395055770874023,
      "learning_rate": 0.00011807308655982603,
      "loss": 0.247,
      "step": 17330
    },
    {
      "epoch": 1.2295337706080482,
      "grad_norm": 1.4923357963562012,
      "learning_rate": 0.00011802581194156858,
      "loss": 0.2787,
      "step": 17340
    },
    {
      "epoch": 1.2302428647402943,
      "grad_norm": 0.8955751061439514,
      "learning_rate": 0.00011797853732331112,
      "loss": 0.273,
      "step": 17350
    },
    {
      "epoch": 1.2309519588725404,
      "grad_norm": 0.7536344528198242,
      "learning_rate": 0.00011793126270505365,
      "loss": 0.2376,
      "step": 17360
    },
    {
      "epoch": 1.2316610530047865,
      "grad_norm": 0.815513551235199,
      "learning_rate": 0.00011788398808679622,
      "loss": 0.2471,
      "step": 17370
    },
    {
      "epoch": 1.2323701471370325,
      "grad_norm": 0.9096767902374268,
      "learning_rate": 0.00011783671346853875,
      "loss": 0.2476,
      "step": 17380
    },
    {
      "epoch": 1.2330792412692786,
      "grad_norm": 1.0111817121505737,
      "learning_rate": 0.00011778943885028129,
      "loss": 0.2242,
      "step": 17390
    },
    {
      "epoch": 1.2337883354015244,
      "grad_norm": 0.7046518325805664,
      "learning_rate": 0.00011774216423202383,
      "loss": 0.2459,
      "step": 17400
    },
    {
      "epoch": 1.2344974295337705,
      "grad_norm": 1.111031174659729,
      "learning_rate": 0.00011769488961376638,
      "loss": 0.2398,
      "step": 17410
    },
    {
      "epoch": 1.2352065236660166,
      "grad_norm": 0.8897399306297302,
      "learning_rate": 0.00011764761499550891,
      "loss": 0.2266,
      "step": 17420
    },
    {
      "epoch": 1.2359156177982626,
      "grad_norm": 0.8754530549049377,
      "learning_rate": 0.00011760034037725145,
      "loss": 0.2445,
      "step": 17430
    },
    {
      "epoch": 1.2366247119305087,
      "grad_norm": 1.0505927801132202,
      "learning_rate": 0.00011755306575899401,
      "loss": 0.2418,
      "step": 17440
    },
    {
      "epoch": 1.2373338060627548,
      "grad_norm": 1.6710296869277954,
      "learning_rate": 0.00011750579114073655,
      "loss": 0.2334,
      "step": 17450
    },
    {
      "epoch": 1.2380429001950009,
      "grad_norm": 1.120775580406189,
      "learning_rate": 0.00011745851652247908,
      "loss": 0.24,
      "step": 17460
    },
    {
      "epoch": 1.238751994327247,
      "grad_norm": 1.2727292776107788,
      "learning_rate": 0.00011741124190422163,
      "loss": 0.2292,
      "step": 17470
    },
    {
      "epoch": 1.239461088459493,
      "grad_norm": 2.439318895339966,
      "learning_rate": 0.00011736396728596417,
      "loss": 0.2464,
      "step": 17480
    },
    {
      "epoch": 1.240170182591739,
      "grad_norm": 0.9986798763275146,
      "learning_rate": 0.0001173166926677067,
      "loss": 0.2325,
      "step": 17490
    },
    {
      "epoch": 1.2408792767239851,
      "grad_norm": 0.8956020474433899,
      "learning_rate": 0.00011726941804944927,
      "loss": 0.2279,
      "step": 17500
    },
    {
      "epoch": 1.2415883708562312,
      "grad_norm": 0.755061686038971,
      "learning_rate": 0.0001172221434311918,
      "loss": 0.2409,
      "step": 17510
    },
    {
      "epoch": 1.2422974649884773,
      "grad_norm": 0.615081787109375,
      "learning_rate": 0.00011717486881293433,
      "loss": 0.2359,
      "step": 17520
    },
    {
      "epoch": 1.2430065591207233,
      "grad_norm": 2.4508655071258545,
      "learning_rate": 0.00011712759419467689,
      "loss": 0.2295,
      "step": 17530
    },
    {
      "epoch": 1.2437156532529694,
      "grad_norm": 0.7937042713165283,
      "learning_rate": 0.00011708031957641943,
      "loss": 0.2233,
      "step": 17540
    },
    {
      "epoch": 1.2444247473852155,
      "grad_norm": 0.6369185447692871,
      "learning_rate": 0.00011703304495816197,
      "loss": 0.2776,
      "step": 17550
    },
    {
      "epoch": 1.2451338415174615,
      "grad_norm": 0.8617658019065857,
      "learning_rate": 0.00011698577033990452,
      "loss": 0.2487,
      "step": 17560
    },
    {
      "epoch": 1.2458429356497076,
      "grad_norm": 2.2264537811279297,
      "learning_rate": 0.00011693849572164705,
      "loss": 0.2587,
      "step": 17570
    },
    {
      "epoch": 1.2465520297819537,
      "grad_norm": 1.0459662675857544,
      "learning_rate": 0.00011689122110338959,
      "loss": 0.2281,
      "step": 17580
    },
    {
      "epoch": 1.2472611239141995,
      "grad_norm": 1.5975639820098877,
      "learning_rate": 0.00011684394648513212,
      "loss": 0.2302,
      "step": 17590
    },
    {
      "epoch": 1.2479702180464456,
      "grad_norm": 1.0390244722366333,
      "learning_rate": 0.00011679667186687469,
      "loss": 0.2275,
      "step": 17600
    },
    {
      "epoch": 1.2486793121786917,
      "grad_norm": 0.6413639187812805,
      "learning_rate": 0.00011674939724861722,
      "loss": 0.2153,
      "step": 17610
    },
    {
      "epoch": 1.2493884063109377,
      "grad_norm": 1.0410447120666504,
      "learning_rate": 0.00011670212263035976,
      "loss": 0.2323,
      "step": 17620
    },
    {
      "epoch": 1.2500975004431838,
      "grad_norm": 0.9820160865783691,
      "learning_rate": 0.00011665484801210231,
      "loss": 0.2345,
      "step": 17630
    },
    {
      "epoch": 1.2508065945754299,
      "grad_norm": 1.590207815170288,
      "learning_rate": 0.00011660757339384485,
      "loss": 0.2655,
      "step": 17640
    },
    {
      "epoch": 1.251515688707676,
      "grad_norm": 0.7879983186721802,
      "learning_rate": 0.00011656029877558738,
      "loss": 0.2184,
      "step": 17650
    },
    {
      "epoch": 1.252224782839922,
      "grad_norm": 0.8677961230278015,
      "learning_rate": 0.00011651302415732995,
      "loss": 0.2299,
      "step": 17660
    },
    {
      "epoch": 1.252933876972168,
      "grad_norm": 1.1524912118911743,
      "learning_rate": 0.00011646574953907248,
      "loss": 0.26,
      "step": 17670
    },
    {
      "epoch": 1.2536429711044141,
      "grad_norm": 1.5507014989852905,
      "learning_rate": 0.00011641847492081502,
      "loss": 0.2355,
      "step": 17680
    },
    {
      "epoch": 1.2543520652366602,
      "grad_norm": 0.6635664701461792,
      "learning_rate": 0.00011637120030255757,
      "loss": 0.2269,
      "step": 17690
    },
    {
      "epoch": 1.2550611593689063,
      "grad_norm": 1.3268001079559326,
      "learning_rate": 0.0001163239256843001,
      "loss": 0.248,
      "step": 17700
    },
    {
      "epoch": 1.2557702535011523,
      "grad_norm": 0.6872743368148804,
      "learning_rate": 0.00011627665106604264,
      "loss": 0.2356,
      "step": 17710
    },
    {
      "epoch": 1.2564793476333984,
      "grad_norm": 3.1041810512542725,
      "learning_rate": 0.0001162293764477852,
      "loss": 0.2283,
      "step": 17720
    },
    {
      "epoch": 1.2571884417656443,
      "grad_norm": 1.0327448844909668,
      "learning_rate": 0.00011618210182952774,
      "loss": 0.2267,
      "step": 17730
    },
    {
      "epoch": 1.2578975358978903,
      "grad_norm": 1.1253105401992798,
      "learning_rate": 0.00011613482721127026,
      "loss": 0.2447,
      "step": 17740
    },
    {
      "epoch": 1.2586066300301364,
      "grad_norm": 0.8875378370285034,
      "learning_rate": 0.00011608755259301283,
      "loss": 0.2381,
      "step": 17750
    },
    {
      "epoch": 1.2593157241623825,
      "grad_norm": 1.3991734981536865,
      "learning_rate": 0.00011604027797475536,
      "loss": 0.2401,
      "step": 17760
    },
    {
      "epoch": 1.2600248182946285,
      "grad_norm": 0.8729121685028076,
      "learning_rate": 0.0001159930033564979,
      "loss": 0.2645,
      "step": 17770
    },
    {
      "epoch": 1.2607339124268746,
      "grad_norm": 0.8995339274406433,
      "learning_rate": 0.00011594572873824044,
      "loss": 0.2475,
      "step": 17780
    },
    {
      "epoch": 1.2614430065591207,
      "grad_norm": 1.4719040393829346,
      "learning_rate": 0.00011589845411998299,
      "loss": 0.2538,
      "step": 17790
    },
    {
      "epoch": 1.2621521006913667,
      "grad_norm": 1.8974339962005615,
      "learning_rate": 0.00011585117950172552,
      "loss": 0.2419,
      "step": 17800
    },
    {
      "epoch": 1.2628611948236128,
      "grad_norm": 1.867891550064087,
      "learning_rate": 0.00011580390488346806,
      "loss": 0.2315,
      "step": 17810
    },
    {
      "epoch": 1.2635702889558589,
      "grad_norm": 2.04901385307312,
      "learning_rate": 0.00011575663026521062,
      "loss": 0.2818,
      "step": 17820
    },
    {
      "epoch": 1.264279383088105,
      "grad_norm": 0.884120523929596,
      "learning_rate": 0.00011570935564695316,
      "loss": 0.2511,
      "step": 17830
    },
    {
      "epoch": 1.264988477220351,
      "grad_norm": 1.6210700273513794,
      "learning_rate": 0.0001156620810286957,
      "loss": 0.2662,
      "step": 17840
    },
    {
      "epoch": 1.265697571352597,
      "grad_norm": 2.271064043045044,
      "learning_rate": 0.00011561480641043824,
      "loss": 0.272,
      "step": 17850
    },
    {
      "epoch": 1.2664066654848432,
      "grad_norm": 0.8375915288925171,
      "learning_rate": 0.00011556753179218078,
      "loss": 0.2195,
      "step": 17860
    },
    {
      "epoch": 1.2671157596170892,
      "grad_norm": 3.5832152366638184,
      "learning_rate": 0.00011552025717392332,
      "loss": 0.2485,
      "step": 17870
    },
    {
      "epoch": 1.2678248537493353,
      "grad_norm": 2.4952597618103027,
      "learning_rate": 0.00011547298255566588,
      "loss": 0.2604,
      "step": 17880
    },
    {
      "epoch": 1.2685339478815814,
      "grad_norm": 1.1946052312850952,
      "learning_rate": 0.00011542570793740842,
      "loss": 0.2238,
      "step": 17890
    },
    {
      "epoch": 1.2692430420138274,
      "grad_norm": 1.2418338060379028,
      "learning_rate": 0.00011537843331915095,
      "loss": 0.253,
      "step": 17900
    },
    {
      "epoch": 1.2699521361460735,
      "grad_norm": 0.8911392092704773,
      "learning_rate": 0.0001153311587008935,
      "loss": 0.2476,
      "step": 17910
    },
    {
      "epoch": 1.2706612302783196,
      "grad_norm": 0.7655411958694458,
      "learning_rate": 0.00011528388408263604,
      "loss": 0.2351,
      "step": 17920
    },
    {
      "epoch": 1.2713703244105656,
      "grad_norm": 1.9090452194213867,
      "learning_rate": 0.00011523660946437857,
      "loss": 0.2655,
      "step": 17930
    },
    {
      "epoch": 1.2720794185428115,
      "grad_norm": 0.9633204340934753,
      "learning_rate": 0.00011518933484612114,
      "loss": 0.2818,
      "step": 17940
    },
    {
      "epoch": 1.2727885126750575,
      "grad_norm": 1.2051440477371216,
      "learning_rate": 0.00011514206022786367,
      "loss": 0.2461,
      "step": 17950
    },
    {
      "epoch": 1.2734976068073036,
      "grad_norm": 2.5829737186431885,
      "learning_rate": 0.0001150947856096062,
      "loss": 0.2834,
      "step": 17960
    },
    {
      "epoch": 1.2742067009395497,
      "grad_norm": 0.9433567523956299,
      "learning_rate": 0.00011504751099134876,
      "loss": 0.2553,
      "step": 17970
    },
    {
      "epoch": 1.2749157950717958,
      "grad_norm": 1.0030361413955688,
      "learning_rate": 0.0001150002363730913,
      "loss": 0.2642,
      "step": 17980
    },
    {
      "epoch": 1.2756248892040418,
      "grad_norm": 0.7536511421203613,
      "learning_rate": 0.00011495296175483383,
      "loss": 0.2418,
      "step": 17990
    },
    {
      "epoch": 1.276333983336288,
      "grad_norm": 1.6236540079116821,
      "learning_rate": 0.00011490568713657637,
      "loss": 0.2429,
      "step": 18000
    },
    {
      "epoch": 1.277043077468534,
      "grad_norm": 0.8790647387504578,
      "learning_rate": 0.00011485841251831892,
      "loss": 0.2714,
      "step": 18010
    },
    {
      "epoch": 1.27775217160078,
      "grad_norm": 1.0353190898895264,
      "learning_rate": 0.00011481113790006146,
      "loss": 0.2848,
      "step": 18020
    },
    {
      "epoch": 1.278461265733026,
      "grad_norm": 0.6105754375457764,
      "learning_rate": 0.00011476386328180399,
      "loss": 0.2539,
      "step": 18030
    },
    {
      "epoch": 1.2791703598652722,
      "grad_norm": 0.967171847820282,
      "learning_rate": 0.00011471658866354656,
      "loss": 0.2314,
      "step": 18040
    },
    {
      "epoch": 1.2798794539975182,
      "grad_norm": 1.5467900037765503,
      "learning_rate": 0.00011466931404528909,
      "loss": 0.2265,
      "step": 18050
    },
    {
      "epoch": 1.2805885481297643,
      "grad_norm": 1.3220703601837158,
      "learning_rate": 0.00011462203942703163,
      "loss": 0.2352,
      "step": 18060
    },
    {
      "epoch": 1.2812976422620102,
      "grad_norm": 0.6526463627815247,
      "learning_rate": 0.00011457476480877418,
      "loss": 0.2771,
      "step": 18070
    },
    {
      "epoch": 1.2820067363942562,
      "grad_norm": 0.7607747316360474,
      "learning_rate": 0.00011452749019051671,
      "loss": 0.2334,
      "step": 18080
    },
    {
      "epoch": 1.2827158305265023,
      "grad_norm": 1.5833302736282349,
      "learning_rate": 0.00011448021557225925,
      "loss": 0.2605,
      "step": 18090
    },
    {
      "epoch": 1.2834249246587484,
      "grad_norm": 0.7796398997306824,
      "learning_rate": 0.00011443294095400181,
      "loss": 0.2058,
      "step": 18100
    },
    {
      "epoch": 1.2841340187909944,
      "grad_norm": 0.7349197864532471,
      "learning_rate": 0.00011438566633574435,
      "loss": 0.2494,
      "step": 18110
    },
    {
      "epoch": 1.2848431129232405,
      "grad_norm": 1.2447468042373657,
      "learning_rate": 0.00011433839171748689,
      "loss": 0.2076,
      "step": 18120
    },
    {
      "epoch": 1.2855522070554866,
      "grad_norm": 0.898004949092865,
      "learning_rate": 0.00011429111709922944,
      "loss": 0.2319,
      "step": 18130
    },
    {
      "epoch": 1.2862613011877326,
      "grad_norm": 0.969101071357727,
      "learning_rate": 0.00011424384248097197,
      "loss": 0.2677,
      "step": 18140
    },
    {
      "epoch": 1.2869703953199787,
      "grad_norm": 1.062243103981018,
      "learning_rate": 0.00011419656786271451,
      "loss": 0.2605,
      "step": 18150
    },
    {
      "epoch": 1.2876794894522248,
      "grad_norm": 0.8293269872665405,
      "learning_rate": 0.00011414929324445707,
      "loss": 0.2436,
      "step": 18160
    },
    {
      "epoch": 1.2883885835844708,
      "grad_norm": 0.8066661357879639,
      "learning_rate": 0.00011410201862619961,
      "loss": 0.2311,
      "step": 18170
    },
    {
      "epoch": 1.289097677716717,
      "grad_norm": 1.0969730615615845,
      "learning_rate": 0.00011405474400794214,
      "loss": 0.2447,
      "step": 18180
    },
    {
      "epoch": 1.289806771848963,
      "grad_norm": 1.0634022951126099,
      "learning_rate": 0.00011400746938968467,
      "loss": 0.2212,
      "step": 18190
    },
    {
      "epoch": 1.290515865981209,
      "grad_norm": 1.1511130332946777,
      "learning_rate": 0.00011396019477142723,
      "loss": 0.2405,
      "step": 18200
    },
    {
      "epoch": 1.2912249601134551,
      "grad_norm": 1.3135513067245483,
      "learning_rate": 0.00011391292015316977,
      "loss": 0.2451,
      "step": 18210
    },
    {
      "epoch": 1.2919340542457012,
      "grad_norm": 2.2913331985473633,
      "learning_rate": 0.0001138656455349123,
      "loss": 0.2315,
      "step": 18220
    },
    {
      "epoch": 1.2926431483779472,
      "grad_norm": 1.486382246017456,
      "learning_rate": 0.00011381837091665485,
      "loss": 0.2728,
      "step": 18230
    },
    {
      "epoch": 1.2933522425101933,
      "grad_norm": 1.1741995811462402,
      "learning_rate": 0.00011377109629839739,
      "loss": 0.2636,
      "step": 18240
    },
    {
      "epoch": 1.2940613366424394,
      "grad_norm": 2.422450304031372,
      "learning_rate": 0.00011372382168013993,
      "loss": 0.2574,
      "step": 18250
    },
    {
      "epoch": 1.2947704307746855,
      "grad_norm": 0.8835809826850891,
      "learning_rate": 0.00011367654706188249,
      "loss": 0.235,
      "step": 18260
    },
    {
      "epoch": 1.2954795249069315,
      "grad_norm": 0.9499159455299377,
      "learning_rate": 0.00011362927244362503,
      "loss": 0.251,
      "step": 18270
    },
    {
      "epoch": 1.2961886190391774,
      "grad_norm": 0.9288727641105652,
      "learning_rate": 0.00011358199782536756,
      "loss": 0.2403,
      "step": 18280
    },
    {
      "epoch": 1.2968977131714234,
      "grad_norm": 0.9419351816177368,
      "learning_rate": 0.00011353472320711011,
      "loss": 0.2412,
      "step": 18290
    },
    {
      "epoch": 1.2976068073036695,
      "grad_norm": 1.1746153831481934,
      "learning_rate": 0.00011348744858885265,
      "loss": 0.2232,
      "step": 18300
    },
    {
      "epoch": 1.2983159014359156,
      "grad_norm": 1.0238659381866455,
      "learning_rate": 0.00011344017397059518,
      "loss": 0.2469,
      "step": 18310
    },
    {
      "epoch": 1.2990249955681616,
      "grad_norm": 0.699425220489502,
      "learning_rate": 0.00011339289935233775,
      "loss": 0.2302,
      "step": 18320
    },
    {
      "epoch": 1.2997340897004077,
      "grad_norm": 1.222166657447815,
      "learning_rate": 0.00011334562473408028,
      "loss": 0.2395,
      "step": 18330
    },
    {
      "epoch": 1.3004431838326538,
      "grad_norm": 0.9918177723884583,
      "learning_rate": 0.00011329835011582282,
      "loss": 0.2738,
      "step": 18340
    },
    {
      "epoch": 1.3011522779648999,
      "grad_norm": 0.9113993048667908,
      "learning_rate": 0.00011325107549756537,
      "loss": 0.271,
      "step": 18350
    },
    {
      "epoch": 1.301861372097146,
      "grad_norm": 0.8379170298576355,
      "learning_rate": 0.0001132038008793079,
      "loss": 0.2646,
      "step": 18360
    },
    {
      "epoch": 1.302570466229392,
      "grad_norm": 0.8795086741447449,
      "learning_rate": 0.00011315652626105044,
      "loss": 0.2134,
      "step": 18370
    },
    {
      "epoch": 1.303279560361638,
      "grad_norm": 1.6242411136627197,
      "learning_rate": 0.00011310925164279298,
      "loss": 0.2327,
      "step": 18380
    },
    {
      "epoch": 1.3039886544938841,
      "grad_norm": 1.1885267496109009,
      "learning_rate": 0.00011306197702453554,
      "loss": 0.2504,
      "step": 18390
    },
    {
      "epoch": 1.3046977486261302,
      "grad_norm": 1.182963490486145,
      "learning_rate": 0.00011301470240627808,
      "loss": 0.27,
      "step": 18400
    },
    {
      "epoch": 1.305406842758376,
      "grad_norm": 2.6723392009735107,
      "learning_rate": 0.0001129674277880206,
      "loss": 0.2621,
      "step": 18410
    },
    {
      "epoch": 1.306115936890622,
      "grad_norm": 0.8839777708053589,
      "learning_rate": 0.00011292015316976316,
      "loss": 0.2333,
      "step": 18420
    },
    {
      "epoch": 1.3068250310228682,
      "grad_norm": 1.8187000751495361,
      "learning_rate": 0.0001128728785515057,
      "loss": 0.2813,
      "step": 18430
    },
    {
      "epoch": 1.3075341251551142,
      "grad_norm": 0.8582186102867126,
      "learning_rate": 0.00011282560393324824,
      "loss": 0.2582,
      "step": 18440
    },
    {
      "epoch": 1.3082432192873603,
      "grad_norm": 1.7431775331497192,
      "learning_rate": 0.00011277832931499079,
      "loss": 0.2238,
      "step": 18450
    },
    {
      "epoch": 1.3089523134196064,
      "grad_norm": 2.4057817459106445,
      "learning_rate": 0.00011273105469673332,
      "loss": 0.2141,
      "step": 18460
    },
    {
      "epoch": 1.3096614075518525,
      "grad_norm": 0.8575524687767029,
      "learning_rate": 0.00011268378007847586,
      "loss": 0.247,
      "step": 18470
    },
    {
      "epoch": 1.3103705016840985,
      "grad_norm": 1.2096729278564453,
      "learning_rate": 0.00011263650546021842,
      "loss": 0.2429,
      "step": 18480
    },
    {
      "epoch": 1.3110795958163446,
      "grad_norm": 2.174044370651245,
      "learning_rate": 0.00011258923084196096,
      "loss": 0.2782,
      "step": 18490
    },
    {
      "epoch": 1.3117886899485907,
      "grad_norm": 0.7923636436462402,
      "learning_rate": 0.0001125419562237035,
      "loss": 0.2416,
      "step": 18500
    },
    {
      "epoch": 1.3124977840808367,
      "grad_norm": 1.0947959423065186,
      "learning_rate": 0.00011249468160544605,
      "loss": 0.231,
      "step": 18510
    },
    {
      "epoch": 1.3132068782130828,
      "grad_norm": 2.465153455734253,
      "learning_rate": 0.00011244740698718858,
      "loss": 0.2239,
      "step": 18520
    },
    {
      "epoch": 1.3139159723453289,
      "grad_norm": 1.043725609779358,
      "learning_rate": 0.00011240013236893112,
      "loss": 0.2219,
      "step": 18530
    },
    {
      "epoch": 1.314625066477575,
      "grad_norm": 0.9721900224685669,
      "learning_rate": 0.00011235285775067368,
      "loss": 0.2475,
      "step": 18540
    },
    {
      "epoch": 1.315334160609821,
      "grad_norm": 1.0074301958084106,
      "learning_rate": 0.00011230558313241622,
      "loss": 0.26,
      "step": 18550
    },
    {
      "epoch": 1.316043254742067,
      "grad_norm": 1.5976465940475464,
      "learning_rate": 0.00011225830851415875,
      "loss": 0.2525,
      "step": 18560
    },
    {
      "epoch": 1.3167523488743131,
      "grad_norm": 1.27410089969635,
      "learning_rate": 0.00011221103389590129,
      "loss": 0.239,
      "step": 18570
    },
    {
      "epoch": 1.3174614430065592,
      "grad_norm": 1.4822183847427368,
      "learning_rate": 0.00011216375927764384,
      "loss": 0.2377,
      "step": 18580
    },
    {
      "epoch": 1.3181705371388053,
      "grad_norm": 1.519150972366333,
      "learning_rate": 0.00011211648465938638,
      "loss": 0.2653,
      "step": 18590
    },
    {
      "epoch": 1.3188796312710513,
      "grad_norm": 1.364169716835022,
      "learning_rate": 0.00011206921004112891,
      "loss": 0.2679,
      "step": 18600
    },
    {
      "epoch": 1.3195887254032974,
      "grad_norm": 0.7622084617614746,
      "learning_rate": 0.00011202193542287148,
      "loss": 0.251,
      "step": 18610
    },
    {
      "epoch": 1.3202978195355435,
      "grad_norm": 0.9047206044197083,
      "learning_rate": 0.00011197466080461401,
      "loss": 0.2466,
      "step": 18620
    },
    {
      "epoch": 1.3210069136677893,
      "grad_norm": 0.78009033203125,
      "learning_rate": 0.00011192738618635654,
      "loss": 0.2294,
      "step": 18630
    },
    {
      "epoch": 1.3217160078000354,
      "grad_norm": 2.968132734298706,
      "learning_rate": 0.0001118801115680991,
      "loss": 0.2587,
      "step": 18640
    },
    {
      "epoch": 1.3224251019322815,
      "grad_norm": 0.9904612898826599,
      "learning_rate": 0.00011183283694984164,
      "loss": 0.2302,
      "step": 18650
    },
    {
      "epoch": 1.3231341960645275,
      "grad_norm": 1.0957860946655273,
      "learning_rate": 0.00011178556233158417,
      "loss": 0.2318,
      "step": 18660
    },
    {
      "epoch": 1.3238432901967736,
      "grad_norm": 1.39284348487854,
      "learning_rate": 0.00011173828771332673,
      "loss": 0.2309,
      "step": 18670
    },
    {
      "epoch": 1.3245523843290197,
      "grad_norm": 0.5102840662002563,
      "learning_rate": 0.00011169101309506926,
      "loss": 0.2478,
      "step": 18680
    },
    {
      "epoch": 1.3252614784612657,
      "grad_norm": 0.9815047979354858,
      "learning_rate": 0.0001116437384768118,
      "loss": 0.2298,
      "step": 18690
    },
    {
      "epoch": 1.3259705725935118,
      "grad_norm": 0.8432208299636841,
      "learning_rate": 0.00011159646385855436,
      "loss": 0.2611,
      "step": 18700
    },
    {
      "epoch": 1.3266796667257579,
      "grad_norm": 0.9556344151496887,
      "learning_rate": 0.0001115491892402969,
      "loss": 0.3075,
      "step": 18710
    },
    {
      "epoch": 1.327388760858004,
      "grad_norm": 1.6194493770599365,
      "learning_rate": 0.00011150191462203943,
      "loss": 0.2447,
      "step": 18720
    },
    {
      "epoch": 1.32809785499025,
      "grad_norm": 3.1694393157958984,
      "learning_rate": 0.00011145464000378198,
      "loss": 0.2414,
      "step": 18730
    },
    {
      "epoch": 1.328806949122496,
      "grad_norm": 1.2510807514190674,
      "learning_rate": 0.00011140736538552452,
      "loss": 0.2472,
      "step": 18740
    },
    {
      "epoch": 1.329516043254742,
      "grad_norm": 1.2051173448562622,
      "learning_rate": 0.00011136009076726705,
      "loss": 0.268,
      "step": 18750
    },
    {
      "epoch": 1.330225137386988,
      "grad_norm": 0.8731889128684998,
      "learning_rate": 0.00011131281614900959,
      "loss": 0.2167,
      "step": 18760
    },
    {
      "epoch": 1.330934231519234,
      "grad_norm": 0.7071873545646667,
      "learning_rate": 0.00011126554153075215,
      "loss": 0.2411,
      "step": 18770
    },
    {
      "epoch": 1.3316433256514801,
      "grad_norm": 1.2324382066726685,
      "learning_rate": 0.00011121826691249469,
      "loss": 0.259,
      "step": 18780
    },
    {
      "epoch": 1.3323524197837262,
      "grad_norm": 0.8807821273803711,
      "learning_rate": 0.00011117099229423722,
      "loss": 0.2235,
      "step": 18790
    },
    {
      "epoch": 1.3330615139159723,
      "grad_norm": 0.9216170907020569,
      "learning_rate": 0.00011112371767597977,
      "loss": 0.2289,
      "step": 18800
    },
    {
      "epoch": 1.3337706080482183,
      "grad_norm": 0.6566286087036133,
      "learning_rate": 0.00011107644305772231,
      "loss": 0.2234,
      "step": 18810
    },
    {
      "epoch": 1.3344797021804644,
      "grad_norm": 1.1968164443969727,
      "learning_rate": 0.00011102916843946485,
      "loss": 0.262,
      "step": 18820
    },
    {
      "epoch": 1.3351887963127105,
      "grad_norm": 0.9669411778450012,
      "learning_rate": 0.00011098189382120741,
      "loss": 0.2198,
      "step": 18830
    },
    {
      "epoch": 1.3358978904449565,
      "grad_norm": 0.7674598693847656,
      "learning_rate": 0.00011093461920294995,
      "loss": 0.2395,
      "step": 18840
    },
    {
      "epoch": 1.3366069845772026,
      "grad_norm": 1.3065649271011353,
      "learning_rate": 0.00011088734458469248,
      "loss": 0.2609,
      "step": 18850
    },
    {
      "epoch": 1.3373160787094487,
      "grad_norm": 2.0407052040100098,
      "learning_rate": 0.00011084006996643503,
      "loss": 0.2353,
      "step": 18860
    },
    {
      "epoch": 1.3380251728416948,
      "grad_norm": 1.437408208847046,
      "learning_rate": 0.00011079279534817757,
      "loss": 0.2597,
      "step": 18870
    },
    {
      "epoch": 1.3387342669739408,
      "grad_norm": 1.0265159606933594,
      "learning_rate": 0.0001107455207299201,
      "loss": 0.2767,
      "step": 18880
    },
    {
      "epoch": 1.339443361106187,
      "grad_norm": 0.9087232351303101,
      "learning_rate": 0.00011069824611166267,
      "loss": 0.2507,
      "step": 18890
    },
    {
      "epoch": 1.340152455238433,
      "grad_norm": 0.8803263902664185,
      "learning_rate": 0.00011065097149340519,
      "loss": 0.2476,
      "step": 18900
    },
    {
      "epoch": 1.340861549370679,
      "grad_norm": 0.930928111076355,
      "learning_rate": 0.00011060369687514773,
      "loss": 0.2094,
      "step": 18910
    },
    {
      "epoch": 1.341570643502925,
      "grad_norm": 1.4544235467910767,
      "learning_rate": 0.00011055642225689029,
      "loss": 0.262,
      "step": 18920
    },
    {
      "epoch": 1.3422797376351712,
      "grad_norm": 1.2848646640777588,
      "learning_rate": 0.00011050914763863283,
      "loss": 0.2991,
      "step": 18930
    },
    {
      "epoch": 1.3429888317674172,
      "grad_norm": 1.4785724878311157,
      "learning_rate": 0.00011046187302037536,
      "loss": 0.268,
      "step": 18940
    },
    {
      "epoch": 1.3436979258996633,
      "grad_norm": 1.154082179069519,
      "learning_rate": 0.00011041459840211791,
      "loss": 0.2275,
      "step": 18950
    },
    {
      "epoch": 1.3444070200319094,
      "grad_norm": 1.0740357637405396,
      "learning_rate": 0.00011036732378386045,
      "loss": 0.258,
      "step": 18960
    },
    {
      "epoch": 1.3451161141641552,
      "grad_norm": 0.9353986978530884,
      "learning_rate": 0.00011032004916560299,
      "loss": 0.2374,
      "step": 18970
    },
    {
      "epoch": 1.3458252082964013,
      "grad_norm": 1.1063591241836548,
      "learning_rate": 0.00011027277454734552,
      "loss": 0.209,
      "step": 18980
    },
    {
      "epoch": 1.3465343024286474,
      "grad_norm": 1.4331609010696411,
      "learning_rate": 0.00011022549992908809,
      "loss": 0.2358,
      "step": 18990
    },
    {
      "epoch": 1.3472433965608934,
      "grad_norm": 0.7946377396583557,
      "learning_rate": 0.00011017822531083062,
      "loss": 0.2042,
      "step": 19000
    },
    {
      "epoch": 1.3479524906931395,
      "grad_norm": 0.9985206127166748,
      "learning_rate": 0.00011013095069257316,
      "loss": 0.284,
      "step": 19010
    },
    {
      "epoch": 1.3486615848253856,
      "grad_norm": 0.8322112560272217,
      "learning_rate": 0.00011008367607431571,
      "loss": 0.2589,
      "step": 19020
    },
    {
      "epoch": 1.3493706789576316,
      "grad_norm": 0.8840250372886658,
      "learning_rate": 0.00011003640145605824,
      "loss": 0.2811,
      "step": 19030
    },
    {
      "epoch": 1.3500797730898777,
      "grad_norm": 1.1159965991973877,
      "learning_rate": 0.00010998912683780078,
      "loss": 0.2494,
      "step": 19040
    },
    {
      "epoch": 1.3507888672221238,
      "grad_norm": 1.321933388710022,
      "learning_rate": 0.00010994185221954334,
      "loss": 0.2528,
      "step": 19050
    },
    {
      "epoch": 1.3514979613543698,
      "grad_norm": 1.0272111892700195,
      "learning_rate": 0.00010989457760128588,
      "loss": 0.2212,
      "step": 19060
    },
    {
      "epoch": 1.352207055486616,
      "grad_norm": 0.8610079288482666,
      "learning_rate": 0.00010984730298302842,
      "loss": 0.2242,
      "step": 19070
    },
    {
      "epoch": 1.352916149618862,
      "grad_norm": 2.826963424682617,
      "learning_rate": 0.00010980002836477097,
      "loss": 0.2644,
      "step": 19080
    },
    {
      "epoch": 1.353625243751108,
      "grad_norm": 2.606743574142456,
      "learning_rate": 0.0001097527537465135,
      "loss": 0.2676,
      "step": 19090
    },
    {
      "epoch": 1.354334337883354,
      "grad_norm": 2.379100799560547,
      "learning_rate": 0.00010970547912825604,
      "loss": 0.2293,
      "step": 19100
    },
    {
      "epoch": 1.3550434320156,
      "grad_norm": 1.83572256565094,
      "learning_rate": 0.0001096582045099986,
      "loss": 0.2631,
      "step": 19110
    },
    {
      "epoch": 1.355752526147846,
      "grad_norm": 2.2356321811676025,
      "learning_rate": 0.00010961092989174113,
      "loss": 0.255,
      "step": 19120
    },
    {
      "epoch": 1.356461620280092,
      "grad_norm": 1.1318080425262451,
      "learning_rate": 0.00010956365527348366,
      "loss": 0.2392,
      "step": 19130
    },
    {
      "epoch": 1.3571707144123382,
      "grad_norm": 0.9978132843971252,
      "learning_rate": 0.00010951638065522623,
      "loss": 0.2471,
      "step": 19140
    },
    {
      "epoch": 1.3578798085445842,
      "grad_norm": 3.572194814682007,
      "learning_rate": 0.00010946910603696876,
      "loss": 0.2665,
      "step": 19150
    },
    {
      "epoch": 1.3585889026768303,
      "grad_norm": 2.0844950675964355,
      "learning_rate": 0.0001094218314187113,
      "loss": 0.2655,
      "step": 19160
    },
    {
      "epoch": 1.3592979968090764,
      "grad_norm": 2.252136468887329,
      "learning_rate": 0.00010937455680045383,
      "loss": 0.2128,
      "step": 19170
    },
    {
      "epoch": 1.3600070909413224,
      "grad_norm": 1.855061411857605,
      "learning_rate": 0.00010932728218219638,
      "loss": 0.2237,
      "step": 19180
    },
    {
      "epoch": 1.3607161850735685,
      "grad_norm": 1.3691760301589966,
      "learning_rate": 0.00010928000756393892,
      "loss": 0.2614,
      "step": 19190
    },
    {
      "epoch": 1.3614252792058146,
      "grad_norm": 1.9693832397460938,
      "learning_rate": 0.00010923273294568146,
      "loss": 0.2505,
      "step": 19200
    },
    {
      "epoch": 1.3621343733380606,
      "grad_norm": 1.5439859628677368,
      "learning_rate": 0.00010918545832742402,
      "loss": 0.2502,
      "step": 19210
    },
    {
      "epoch": 1.3628434674703067,
      "grad_norm": 1.3146203756332397,
      "learning_rate": 0.00010913818370916656,
      "loss": 0.2556,
      "step": 19220
    },
    {
      "epoch": 1.3635525616025528,
      "grad_norm": 1.7618248462677002,
      "learning_rate": 0.00010909090909090909,
      "loss": 0.2456,
      "step": 19230
    },
    {
      "epoch": 1.3642616557347989,
      "grad_norm": 0.917880654335022,
      "learning_rate": 0.00010904363447265164,
      "loss": 0.2593,
      "step": 19240
    },
    {
      "epoch": 1.364970749867045,
      "grad_norm": 1.1635538339614868,
      "learning_rate": 0.00010899635985439418,
      "loss": 0.226,
      "step": 19250
    },
    {
      "epoch": 1.365679843999291,
      "grad_norm": 1.4941984415054321,
      "learning_rate": 0.00010894908523613671,
      "loss": 0.2379,
      "step": 19260
    },
    {
      "epoch": 1.366388938131537,
      "grad_norm": 1.2206363677978516,
      "learning_rate": 0.00010890181061787928,
      "loss": 0.2406,
      "step": 19270
    },
    {
      "epoch": 1.3670980322637831,
      "grad_norm": 1.438652753829956,
      "learning_rate": 0.00010885453599962181,
      "loss": 0.282,
      "step": 19280
    },
    {
      "epoch": 1.3678071263960292,
      "grad_norm": 1.74712336063385,
      "learning_rate": 0.00010880726138136435,
      "loss": 0.2726,
      "step": 19290
    },
    {
      "epoch": 1.3685162205282753,
      "grad_norm": 1.2864619493484497,
      "learning_rate": 0.0001087599867631069,
      "loss": 0.2359,
      "step": 19300
    },
    {
      "epoch": 1.369225314660521,
      "grad_norm": 3.0413401126861572,
      "learning_rate": 0.00010871271214484944,
      "loss": 0.2383,
      "step": 19310
    },
    {
      "epoch": 1.3699344087927672,
      "grad_norm": 0.7137235403060913,
      "learning_rate": 0.00010866543752659197,
      "loss": 0.2573,
      "step": 19320
    },
    {
      "epoch": 1.3706435029250132,
      "grad_norm": 1.1992697715759277,
      "learning_rate": 0.00010861816290833454,
      "loss": 0.2682,
      "step": 19330
    },
    {
      "epoch": 1.3713525970572593,
      "grad_norm": 1.0086325407028198,
      "learning_rate": 0.00010857088829007707,
      "loss": 0.2362,
      "step": 19340
    },
    {
      "epoch": 1.3720616911895054,
      "grad_norm": 1.2310842275619507,
      "learning_rate": 0.0001085236136718196,
      "loss": 0.2482,
      "step": 19350
    },
    {
      "epoch": 1.3727707853217515,
      "grad_norm": 1.0760753154754639,
      "learning_rate": 0.00010847633905356213,
      "loss": 0.2491,
      "step": 19360
    },
    {
      "epoch": 1.3734798794539975,
      "grad_norm": 1.3756479024887085,
      "learning_rate": 0.0001084290644353047,
      "loss": 0.2196,
      "step": 19370
    },
    {
      "epoch": 1.3741889735862436,
      "grad_norm": 1.2228128910064697,
      "learning_rate": 0.00010838178981704723,
      "loss": 0.2542,
      "step": 19380
    },
    {
      "epoch": 1.3748980677184897,
      "grad_norm": 2.0225462913513184,
      "learning_rate": 0.00010833451519878977,
      "loss": 0.2635,
      "step": 19390
    },
    {
      "epoch": 1.3756071618507357,
      "grad_norm": 0.9072917699813843,
      "learning_rate": 0.00010828724058053232,
      "loss": 0.269,
      "step": 19400
    },
    {
      "epoch": 1.3763162559829818,
      "grad_norm": 0.7309947609901428,
      "learning_rate": 0.00010823996596227485,
      "loss": 0.2643,
      "step": 19410
    },
    {
      "epoch": 1.3770253501152279,
      "grad_norm": 0.8753173351287842,
      "learning_rate": 0.00010819269134401739,
      "loss": 0.2246,
      "step": 19420
    },
    {
      "epoch": 1.377734444247474,
      "grad_norm": 0.7086522579193115,
      "learning_rate": 0.00010814541672575995,
      "loss": 0.2542,
      "step": 19430
    },
    {
      "epoch": 1.3784435383797198,
      "grad_norm": 1.7075328826904297,
      "learning_rate": 0.00010809814210750249,
      "loss": 0.2338,
      "step": 19440
    },
    {
      "epoch": 1.3791526325119658,
      "grad_norm": 0.8151174783706665,
      "learning_rate": 0.00010805086748924503,
      "loss": 0.2309,
      "step": 19450
    },
    {
      "epoch": 1.379861726644212,
      "grad_norm": 1.1325315237045288,
      "learning_rate": 0.00010800359287098758,
      "loss": 0.3052,
      "step": 19460
    },
    {
      "epoch": 1.380570820776458,
      "grad_norm": 1.9156808853149414,
      "learning_rate": 0.00010795631825273011,
      "loss": 0.2294,
      "step": 19470
    },
    {
      "epoch": 1.381279914908704,
      "grad_norm": 1.2084479331970215,
      "learning_rate": 0.00010790904363447265,
      "loss": 0.2241,
      "step": 19480
    },
    {
      "epoch": 1.3819890090409501,
      "grad_norm": 0.6275148391723633,
      "learning_rate": 0.00010786176901621521,
      "loss": 0.2789,
      "step": 19490
    },
    {
      "epoch": 1.3826981031731962,
      "grad_norm": 1.6432850360870361,
      "learning_rate": 0.00010781449439795775,
      "loss": 0.2237,
      "step": 19500
    },
    {
      "epoch": 1.3834071973054423,
      "grad_norm": 1.5393164157867432,
      "learning_rate": 0.00010776721977970028,
      "loss": 0.2208,
      "step": 19510
    },
    {
      "epoch": 1.3841162914376883,
      "grad_norm": 0.9560087323188782,
      "learning_rate": 0.00010771994516144283,
      "loss": 0.2768,
      "step": 19520
    },
    {
      "epoch": 1.3848253855699344,
      "grad_norm": 0.7505351305007935,
      "learning_rate": 0.00010767267054318537,
      "loss": 0.239,
      "step": 19530
    },
    {
      "epoch": 1.3855344797021805,
      "grad_norm": 0.6410053372383118,
      "learning_rate": 0.00010762539592492791,
      "loss": 0.2649,
      "step": 19540
    },
    {
      "epoch": 1.3862435738344265,
      "grad_norm": 2.8800759315490723,
      "learning_rate": 0.00010757812130667044,
      "loss": 0.238,
      "step": 19550
    },
    {
      "epoch": 1.3869526679666726,
      "grad_norm": 1.0735344886779785,
      "learning_rate": 0.00010753084668841301,
      "loss": 0.2604,
      "step": 19560
    },
    {
      "epoch": 1.3876617620989187,
      "grad_norm": 1.4611667394638062,
      "learning_rate": 0.00010748357207015553,
      "loss": 0.2366,
      "step": 19570
    },
    {
      "epoch": 1.3883708562311647,
      "grad_norm": 1.3354014158248901,
      "learning_rate": 0.00010743629745189807,
      "loss": 0.2621,
      "step": 19580
    },
    {
      "epoch": 1.3890799503634108,
      "grad_norm": 0.9663047790527344,
      "learning_rate": 0.00010738902283364063,
      "loss": 0.2625,
      "step": 19590
    },
    {
      "epoch": 1.3897890444956569,
      "grad_norm": 2.276660919189453,
      "learning_rate": 0.00010734174821538317,
      "loss": 0.2533,
      "step": 19600
    },
    {
      "epoch": 1.390498138627903,
      "grad_norm": 0.9255921840667725,
      "learning_rate": 0.0001072944735971257,
      "loss": 0.2446,
      "step": 19610
    },
    {
      "epoch": 1.391207232760149,
      "grad_norm": 1.3131994009017944,
      "learning_rate": 0.00010724719897886825,
      "loss": 0.2482,
      "step": 19620
    },
    {
      "epoch": 1.391916326892395,
      "grad_norm": 0.9151491522789001,
      "learning_rate": 0.00010719992436061079,
      "loss": 0.2725,
      "step": 19630
    },
    {
      "epoch": 1.3926254210246412,
      "grad_norm": 0.9904502630233765,
      "learning_rate": 0.00010715264974235332,
      "loss": 0.2237,
      "step": 19640
    },
    {
      "epoch": 1.393334515156887,
      "grad_norm": 1.1546225547790527,
      "learning_rate": 0.00010710537512409589,
      "loss": 0.2479,
      "step": 19650
    },
    {
      "epoch": 1.394043609289133,
      "grad_norm": 0.9575048089027405,
      "learning_rate": 0.00010705810050583842,
      "loss": 0.2233,
      "step": 19660
    },
    {
      "epoch": 1.3947527034213791,
      "grad_norm": 1.2709498405456543,
      "learning_rate": 0.00010701082588758096,
      "loss": 0.2613,
      "step": 19670
    },
    {
      "epoch": 1.3954617975536252,
      "grad_norm": 0.8599661588668823,
      "learning_rate": 0.00010696355126932351,
      "loss": 0.2467,
      "step": 19680
    },
    {
      "epoch": 1.3961708916858713,
      "grad_norm": 1.2571535110473633,
      "learning_rate": 0.00010691627665106605,
      "loss": 0.2124,
      "step": 19690
    },
    {
      "epoch": 1.3968799858181173,
      "grad_norm": 1.5936115980148315,
      "learning_rate": 0.00010686900203280858,
      "loss": 0.2287,
      "step": 19700
    },
    {
      "epoch": 1.3975890799503634,
      "grad_norm": 0.8197820782661438,
      "learning_rate": 0.00010682172741455115,
      "loss": 0.2348,
      "step": 19710
    },
    {
      "epoch": 1.3982981740826095,
      "grad_norm": 0.9342429041862488,
      "learning_rate": 0.00010677445279629368,
      "loss": 0.2414,
      "step": 19720
    },
    {
      "epoch": 1.3990072682148555,
      "grad_norm": 1.2108620405197144,
      "learning_rate": 0.00010672717817803622,
      "loss": 0.2322,
      "step": 19730
    },
    {
      "epoch": 1.3997163623471016,
      "grad_norm": 1.316208839416504,
      "learning_rate": 0.00010667990355977876,
      "loss": 0.2299,
      "step": 19740
    },
    {
      "epoch": 1.4004254564793477,
      "grad_norm": 1.8689041137695312,
      "learning_rate": 0.0001066326289415213,
      "loss": 0.2402,
      "step": 19750
    },
    {
      "epoch": 1.4011345506115938,
      "grad_norm": 2.028928279876709,
      "learning_rate": 0.00010658535432326384,
      "loss": 0.2398,
      "step": 19760
    },
    {
      "epoch": 1.4018436447438398,
      "grad_norm": 1.3356754779815674,
      "learning_rate": 0.00010653807970500638,
      "loss": 0.2612,
      "step": 19770
    },
    {
      "epoch": 1.4025527388760857,
      "grad_norm": 1.1462055444717407,
      "learning_rate": 0.00010649080508674894,
      "loss": 0.2687,
      "step": 19780
    },
    {
      "epoch": 1.4032618330083317,
      "grad_norm": 2.5996217727661133,
      "learning_rate": 0.00010644353046849146,
      "loss": 0.2026,
      "step": 19790
    },
    {
      "epoch": 1.4039709271405778,
      "grad_norm": 1.1994435787200928,
      "learning_rate": 0.000106396255850234,
      "loss": 0.2232,
      "step": 19800
    },
    {
      "epoch": 1.4046800212728239,
      "grad_norm": 1.1035977602005005,
      "learning_rate": 0.00010634898123197656,
      "loss": 0.3036,
      "step": 19810
    },
    {
      "epoch": 1.40538911540507,
      "grad_norm": 1.115173578262329,
      "learning_rate": 0.0001063017066137191,
      "loss": 0.2656,
      "step": 19820
    },
    {
      "epoch": 1.406098209537316,
      "grad_norm": 2.2406914234161377,
      "learning_rate": 0.00010625443199546164,
      "loss": 0.2249,
      "step": 19830
    },
    {
      "epoch": 1.406807303669562,
      "grad_norm": 2.3365390300750732,
      "learning_rate": 0.00010620715737720419,
      "loss": 0.2458,
      "step": 19840
    },
    {
      "epoch": 1.4075163978018082,
      "grad_norm": 1.049734354019165,
      "learning_rate": 0.00010615988275894672,
      "loss": 0.24,
      "step": 19850
    },
    {
      "epoch": 1.4082254919340542,
      "grad_norm": 0.8967066407203674,
      "learning_rate": 0.00010611260814068926,
      "loss": 0.2392,
      "step": 19860
    },
    {
      "epoch": 1.4089345860663003,
      "grad_norm": 3.4535863399505615,
      "learning_rate": 0.00010606533352243182,
      "loss": 0.2549,
      "step": 19870
    },
    {
      "epoch": 1.4096436801985464,
      "grad_norm": 1.084787130355835,
      "learning_rate": 0.00010601805890417436,
      "loss": 0.2263,
      "step": 19880
    },
    {
      "epoch": 1.4103527743307924,
      "grad_norm": 1.6440331935882568,
      "learning_rate": 0.0001059707842859169,
      "loss": 0.2168,
      "step": 19890
    },
    {
      "epoch": 1.4110618684630385,
      "grad_norm": 1.4527751207351685,
      "learning_rate": 0.00010592350966765944,
      "loss": 0.2713,
      "step": 19900
    },
    {
      "epoch": 1.4117709625952846,
      "grad_norm": 1.5169535875320435,
      "learning_rate": 0.00010587623504940198,
      "loss": 0.2803,
      "step": 19910
    },
    {
      "epoch": 1.4124800567275306,
      "grad_norm": 1.0382405519485474,
      "learning_rate": 0.00010582896043114452,
      "loss": 0.2481,
      "step": 19920
    },
    {
      "epoch": 1.4131891508597767,
      "grad_norm": 1.539872407913208,
      "learning_rate": 0.00010578168581288708,
      "loss": 0.2637,
      "step": 19930
    },
    {
      "epoch": 1.4138982449920228,
      "grad_norm": 1.0410281419754028,
      "learning_rate": 0.00010573441119462962,
      "loss": 0.2594,
      "step": 19940
    },
    {
      "epoch": 1.4146073391242688,
      "grad_norm": 0.910828173160553,
      "learning_rate": 0.00010568713657637215,
      "loss": 0.2424,
      "step": 19950
    },
    {
      "epoch": 1.415316433256515,
      "grad_norm": 0.8945385217666626,
      "learning_rate": 0.00010563986195811469,
      "loss": 0.2501,
      "step": 19960
    },
    {
      "epoch": 1.416025527388761,
      "grad_norm": 1.267895221710205,
      "learning_rate": 0.00010559258733985724,
      "loss": 0.2575,
      "step": 19970
    },
    {
      "epoch": 1.416734621521007,
      "grad_norm": 0.9005622863769531,
      "learning_rate": 0.00010554531272159978,
      "loss": 0.2252,
      "step": 19980
    },
    {
      "epoch": 1.417443715653253,
      "grad_norm": 1.043360948562622,
      "learning_rate": 0.00010549803810334231,
      "loss": 0.2472,
      "step": 19990
    },
    {
      "epoch": 1.418152809785499,
      "grad_norm": 1.26787269115448,
      "learning_rate": 0.00010545076348508487,
      "loss": 0.2353,
      "step": 20000
    },
    {
      "epoch": 1.418861903917745,
      "grad_norm": 2.946510076522827,
      "learning_rate": 0.00010540348886682741,
      "loss": 0.2621,
      "step": 20010
    },
    {
      "epoch": 1.419570998049991,
      "grad_norm": 3.1900949478149414,
      "learning_rate": 0.00010535621424856993,
      "loss": 0.2189,
      "step": 20020
    },
    {
      "epoch": 1.4202800921822372,
      "grad_norm": 1.8736006021499634,
      "learning_rate": 0.0001053089396303125,
      "loss": 0.2517,
      "step": 20030
    },
    {
      "epoch": 1.4209891863144832,
      "grad_norm": 1.1572977304458618,
      "learning_rate": 0.00010526166501205503,
      "loss": 0.2625,
      "step": 20040
    },
    {
      "epoch": 1.4216982804467293,
      "grad_norm": 1.1409201622009277,
      "learning_rate": 0.00010521439039379757,
      "loss": 0.23,
      "step": 20050
    },
    {
      "epoch": 1.4224073745789754,
      "grad_norm": 0.9695667028427124,
      "learning_rate": 0.00010516711577554012,
      "loss": 0.2611,
      "step": 20060
    },
    {
      "epoch": 1.4231164687112214,
      "grad_norm": 0.9019922614097595,
      "learning_rate": 0.00010511984115728266,
      "loss": 0.2386,
      "step": 20070
    },
    {
      "epoch": 1.4238255628434675,
      "grad_norm": 1.0772984027862549,
      "learning_rate": 0.00010507256653902519,
      "loss": 0.2523,
      "step": 20080
    },
    {
      "epoch": 1.4245346569757136,
      "grad_norm": 1.0265380144119263,
      "learning_rate": 0.00010502529192076776,
      "loss": 0.2188,
      "step": 20090
    },
    {
      "epoch": 1.4252437511079596,
      "grad_norm": 1.2267484664916992,
      "learning_rate": 0.00010497801730251029,
      "loss": 0.2626,
      "step": 20100
    },
    {
      "epoch": 1.4259528452402057,
      "grad_norm": 1.6721609830856323,
      "learning_rate": 0.00010493074268425283,
      "loss": 0.2622,
      "step": 20110
    },
    {
      "epoch": 1.4266619393724516,
      "grad_norm": 1.0632774829864502,
      "learning_rate": 0.00010488346806599538,
      "loss": 0.2167,
      "step": 20120
    },
    {
      "epoch": 1.4273710335046976,
      "grad_norm": 1.172627329826355,
      "learning_rate": 0.00010483619344773791,
      "loss": 0.2489,
      "step": 20130
    },
    {
      "epoch": 1.4280801276369437,
      "grad_norm": 1.186136245727539,
      "learning_rate": 0.00010478891882948045,
      "loss": 0.227,
      "step": 20140
    },
    {
      "epoch": 1.4287892217691898,
      "grad_norm": 0.8161779046058655,
      "learning_rate": 0.00010474164421122299,
      "loss": 0.2543,
      "step": 20150
    },
    {
      "epoch": 1.4294983159014358,
      "grad_norm": 1.1289697885513306,
      "learning_rate": 0.00010469436959296555,
      "loss": 0.2489,
      "step": 20160
    },
    {
      "epoch": 1.430207410033682,
      "grad_norm": 1.3024556636810303,
      "learning_rate": 0.00010464709497470809,
      "loss": 0.2554,
      "step": 20170
    },
    {
      "epoch": 1.430916504165928,
      "grad_norm": 0.7715516090393066,
      "learning_rate": 0.00010459982035645062,
      "loss": 0.2626,
      "step": 20180
    },
    {
      "epoch": 1.431625598298174,
      "grad_norm": 2.2994306087493896,
      "learning_rate": 0.00010455254573819317,
      "loss": 0.2427,
      "step": 20190
    },
    {
      "epoch": 1.43233469243042,
      "grad_norm": 1.451613426208496,
      "learning_rate": 0.00010450527111993571,
      "loss": 0.2575,
      "step": 20200
    },
    {
      "epoch": 1.4330437865626662,
      "grad_norm": 6.3264665603637695,
      "learning_rate": 0.00010445799650167825,
      "loss": 0.2594,
      "step": 20210
    },
    {
      "epoch": 1.4337528806949122,
      "grad_norm": 1.7195298671722412,
      "learning_rate": 0.00010441072188342081,
      "loss": 0.253,
      "step": 20220
    },
    {
      "epoch": 1.4344619748271583,
      "grad_norm": 1.1754813194274902,
      "learning_rate": 0.00010436344726516335,
      "loss": 0.2382,
      "step": 20230
    },
    {
      "epoch": 1.4351710689594044,
      "grad_norm": 1.2147769927978516,
      "learning_rate": 0.00010431617264690587,
      "loss": 0.2519,
      "step": 20240
    },
    {
      "epoch": 1.4358801630916505,
      "grad_norm": 1.3007488250732422,
      "learning_rate": 0.00010426889802864843,
      "loss": 0.2349,
      "step": 20250
    },
    {
      "epoch": 1.4365892572238965,
      "grad_norm": 1.2132129669189453,
      "learning_rate": 0.00010422162341039097,
      "loss": 0.2367,
      "step": 20260
    },
    {
      "epoch": 1.4372983513561426,
      "grad_norm": 1.3481295108795166,
      "learning_rate": 0.0001041743487921335,
      "loss": 0.2145,
      "step": 20270
    },
    {
      "epoch": 1.4380074454883887,
      "grad_norm": 1.367928147315979,
      "learning_rate": 0.00010412707417387605,
      "loss": 0.2226,
      "step": 20280
    },
    {
      "epoch": 1.4387165396206347,
      "grad_norm": 1.5576483011245728,
      "learning_rate": 0.00010407979955561859,
      "loss": 0.2654,
      "step": 20290
    },
    {
      "epoch": 1.4394256337528808,
      "grad_norm": 0.7436299920082092,
      "learning_rate": 0.00010403252493736113,
      "loss": 0.2464,
      "step": 20300
    },
    {
      "epoch": 1.4401347278851269,
      "grad_norm": 2.7460434436798096,
      "learning_rate": 0.00010398525031910369,
      "loss": 0.2502,
      "step": 20310
    },
    {
      "epoch": 1.440843822017373,
      "grad_norm": 1.147679328918457,
      "learning_rate": 0.00010393797570084623,
      "loss": 0.2672,
      "step": 20320
    },
    {
      "epoch": 1.4415529161496188,
      "grad_norm": 3.0313689708709717,
      "learning_rate": 0.00010389070108258876,
      "loss": 0.235,
      "step": 20330
    },
    {
      "epoch": 1.4422620102818648,
      "grad_norm": 1.6525318622589111,
      "learning_rate": 0.0001038434264643313,
      "loss": 0.2403,
      "step": 20340
    },
    {
      "epoch": 1.442971104414111,
      "grad_norm": 1.0944979190826416,
      "learning_rate": 0.00010379615184607385,
      "loss": 0.2601,
      "step": 20350
    },
    {
      "epoch": 1.443680198546357,
      "grad_norm": 1.884603500366211,
      "learning_rate": 0.00010374887722781638,
      "loss": 0.2493,
      "step": 20360
    },
    {
      "epoch": 1.444389292678603,
      "grad_norm": 1.0016790628433228,
      "learning_rate": 0.00010370160260955892,
      "loss": 0.264,
      "step": 20370
    },
    {
      "epoch": 1.4450983868108491,
      "grad_norm": 2.285081624984741,
      "learning_rate": 0.00010365432799130148,
      "loss": 0.2256,
      "step": 20380
    },
    {
      "epoch": 1.4458074809430952,
      "grad_norm": 1.0472207069396973,
      "learning_rate": 0.00010360705337304402,
      "loss": 0.2189,
      "step": 20390
    },
    {
      "epoch": 1.4465165750753413,
      "grad_norm": 2.818101406097412,
      "learning_rate": 0.00010355977875478656,
      "loss": 0.2308,
      "step": 20400
    },
    {
      "epoch": 1.4472256692075873,
      "grad_norm": 0.6844751238822937,
      "learning_rate": 0.00010351250413652911,
      "loss": 0.2376,
      "step": 20410
    },
    {
      "epoch": 1.4479347633398334,
      "grad_norm": 0.9887837171554565,
      "learning_rate": 0.00010346522951827164,
      "loss": 0.2521,
      "step": 20420
    },
    {
      "epoch": 1.4486438574720795,
      "grad_norm": 0.967167317867279,
      "learning_rate": 0.00010341795490001418,
      "loss": 0.2376,
      "step": 20430
    },
    {
      "epoch": 1.4493529516043255,
      "grad_norm": 1.5013822317123413,
      "learning_rate": 0.00010337068028175674,
      "loss": 0.2725,
      "step": 20440
    },
    {
      "epoch": 1.4500620457365716,
      "grad_norm": 1.5985147953033447,
      "learning_rate": 0.00010332340566349928,
      "loss": 0.2392,
      "step": 20450
    },
    {
      "epoch": 1.4507711398688174,
      "grad_norm": 1.025415062904358,
      "learning_rate": 0.0001032761310452418,
      "loss": 0.2176,
      "step": 20460
    },
    {
      "epoch": 1.4514802340010635,
      "grad_norm": 0.947420597076416,
      "learning_rate": 0.00010322885642698437,
      "loss": 0.275,
      "step": 20470
    },
    {
      "epoch": 1.4521893281333096,
      "grad_norm": 0.8591129183769226,
      "learning_rate": 0.0001031815818087269,
      "loss": 0.2785,
      "step": 20480
    },
    {
      "epoch": 1.4528984222655557,
      "grad_norm": 1.5339014530181885,
      "learning_rate": 0.00010313430719046944,
      "loss": 0.2488,
      "step": 20490
    },
    {
      "epoch": 1.4536075163978017,
      "grad_norm": 0.6583864688873291,
      "learning_rate": 0.00010308703257221199,
      "loss": 0.2762,
      "step": 20500
    },
    {
      "epoch": 1.4543166105300478,
      "grad_norm": 1.441351056098938,
      "learning_rate": 0.00010303975795395452,
      "loss": 0.2487,
      "step": 20510
    },
    {
      "epoch": 1.4550257046622939,
      "grad_norm": 1.0196762084960938,
      "learning_rate": 0.00010299248333569706,
      "loss": 0.2403,
      "step": 20520
    },
    {
      "epoch": 1.45573479879454,
      "grad_norm": 1.0134167671203613,
      "learning_rate": 0.0001029452087174396,
      "loss": 0.2585,
      "step": 20530
    },
    {
      "epoch": 1.456443892926786,
      "grad_norm": 1.3702439069747925,
      "learning_rate": 0.00010289793409918216,
      "loss": 0.2718,
      "step": 20540
    },
    {
      "epoch": 1.457152987059032,
      "grad_norm": 1.5307321548461914,
      "learning_rate": 0.0001028506594809247,
      "loss": 0.2302,
      "step": 20550
    },
    {
      "epoch": 1.4578620811912781,
      "grad_norm": 1.2317464351654053,
      "learning_rate": 0.00010280338486266723,
      "loss": 0.2307,
      "step": 20560
    },
    {
      "epoch": 1.4585711753235242,
      "grad_norm": 1.0450794696807861,
      "learning_rate": 0.00010275611024440978,
      "loss": 0.2401,
      "step": 20570
    },
    {
      "epoch": 1.4592802694557703,
      "grad_norm": 1.125531554222107,
      "learning_rate": 0.00010270883562615232,
      "loss": 0.2224,
      "step": 20580
    },
    {
      "epoch": 1.4599893635880163,
      "grad_norm": 3.339383602142334,
      "learning_rate": 0.00010266156100789486,
      "loss": 0.2476,
      "step": 20590
    },
    {
      "epoch": 1.4606984577202624,
      "grad_norm": 1.0035547018051147,
      "learning_rate": 0.00010261428638963742,
      "loss": 0.2476,
      "step": 20600
    },
    {
      "epoch": 1.4614075518525085,
      "grad_norm": 0.9091777801513672,
      "learning_rate": 0.00010256701177137995,
      "loss": 0.247,
      "step": 20610
    },
    {
      "epoch": 1.4621166459847545,
      "grad_norm": 0.7124027013778687,
      "learning_rate": 0.00010251973715312249,
      "loss": 0.2548,
      "step": 20620
    },
    {
      "epoch": 1.4628257401170006,
      "grad_norm": 1.8017733097076416,
      "learning_rate": 0.00010247246253486504,
      "loss": 0.2581,
      "step": 20630
    },
    {
      "epoch": 1.4635348342492467,
      "grad_norm": 1.2720558643341064,
      "learning_rate": 0.00010242518791660758,
      "loss": 0.2393,
      "step": 20640
    },
    {
      "epoch": 1.4642439283814928,
      "grad_norm": 1.342187762260437,
      "learning_rate": 0.00010237791329835011,
      "loss": 0.2285,
      "step": 20650
    },
    {
      "epoch": 1.4649530225137388,
      "grad_norm": 0.9029232263565063,
      "learning_rate": 0.00010233063868009268,
      "loss": 0.2142,
      "step": 20660
    },
    {
      "epoch": 1.465662116645985,
      "grad_norm": 2.3192076683044434,
      "learning_rate": 0.00010228336406183521,
      "loss": 0.2495,
      "step": 20670
    },
    {
      "epoch": 1.4663712107782307,
      "grad_norm": 1.2100540399551392,
      "learning_rate": 0.00010223608944357775,
      "loss": 0.2522,
      "step": 20680
    },
    {
      "epoch": 1.4670803049104768,
      "grad_norm": 0.908860445022583,
      "learning_rate": 0.0001021888148253203,
      "loss": 0.2489,
      "step": 20690
    },
    {
      "epoch": 1.4677893990427229,
      "grad_norm": 0.9641916751861572,
      "learning_rate": 0.00010214154020706284,
      "loss": 0.2494,
      "step": 20700
    },
    {
      "epoch": 1.468498493174969,
      "grad_norm": 4.320644855499268,
      "learning_rate": 0.00010209426558880537,
      "loss": 0.2697,
      "step": 20710
    },
    {
      "epoch": 1.469207587307215,
      "grad_norm": 0.8605957627296448,
      "learning_rate": 0.00010204699097054794,
      "loss": 0.2193,
      "step": 20720
    },
    {
      "epoch": 1.469916681439461,
      "grad_norm": 0.9225015044212341,
      "learning_rate": 0.00010199971635229046,
      "loss": 0.2705,
      "step": 20730
    },
    {
      "epoch": 1.4706257755717072,
      "grad_norm": 1.1853017807006836,
      "learning_rate": 0.000101952441734033,
      "loss": 0.2612,
      "step": 20740
    },
    {
      "epoch": 1.4713348697039532,
      "grad_norm": 1.529754400253296,
      "learning_rate": 0.00010190516711577553,
      "loss": 0.2141,
      "step": 20750
    },
    {
      "epoch": 1.4720439638361993,
      "grad_norm": 1.3127799034118652,
      "learning_rate": 0.0001018578924975181,
      "loss": 0.2632,
      "step": 20760
    },
    {
      "epoch": 1.4727530579684454,
      "grad_norm": 1.383952021598816,
      "learning_rate": 0.00010181061787926063,
      "loss": 0.2637,
      "step": 20770
    },
    {
      "epoch": 1.4734621521006914,
      "grad_norm": 1.339553713798523,
      "learning_rate": 0.00010176334326100317,
      "loss": 0.253,
      "step": 20780
    },
    {
      "epoch": 1.4741712462329375,
      "grad_norm": 0.8513566851615906,
      "learning_rate": 0.00010171606864274572,
      "loss": 0.2606,
      "step": 20790
    },
    {
      "epoch": 1.4748803403651833,
      "grad_norm": 0.9553602337837219,
      "learning_rate": 0.00010166879402448825,
      "loss": 0.2273,
      "step": 20800
    },
    {
      "epoch": 1.4755894344974294,
      "grad_norm": 1.4624638557434082,
      "learning_rate": 0.00010162151940623079,
      "loss": 0.2485,
      "step": 20810
    },
    {
      "epoch": 1.4762985286296755,
      "grad_norm": 1.126221776008606,
      "learning_rate": 0.00010157424478797335,
      "loss": 0.2561,
      "step": 20820
    },
    {
      "epoch": 1.4770076227619215,
      "grad_norm": 0.9364869594573975,
      "learning_rate": 0.00010152697016971589,
      "loss": 0.2325,
      "step": 20830
    },
    {
      "epoch": 1.4777167168941676,
      "grad_norm": 0.8200109601020813,
      "learning_rate": 0.00010147969555145843,
      "loss": 0.2395,
      "step": 20840
    },
    {
      "epoch": 1.4784258110264137,
      "grad_norm": 0.9460185170173645,
      "learning_rate": 0.00010143242093320097,
      "loss": 0.2398,
      "step": 20850
    },
    {
      "epoch": 1.4791349051586598,
      "grad_norm": 1.303007960319519,
      "learning_rate": 0.00010138514631494351,
      "loss": 0.2574,
      "step": 20860
    },
    {
      "epoch": 1.4798439992909058,
      "grad_norm": 1.895148754119873,
      "learning_rate": 0.00010133787169668605,
      "loss": 0.244,
      "step": 20870
    },
    {
      "epoch": 1.480553093423152,
      "grad_norm": 0.8455970883369446,
      "learning_rate": 0.00010129059707842861,
      "loss": 0.2738,
      "step": 20880
    },
    {
      "epoch": 1.481262187555398,
      "grad_norm": 0.8760694265365601,
      "learning_rate": 0.00010124332246017115,
      "loss": 0.2308,
      "step": 20890
    },
    {
      "epoch": 1.481971281687644,
      "grad_norm": 0.9688374400138855,
      "learning_rate": 0.00010119604784191368,
      "loss": 0.2214,
      "step": 20900
    },
    {
      "epoch": 1.48268037581989,
      "grad_norm": 1.0543538331985474,
      "learning_rate": 0.00010114877322365623,
      "loss": 0.2562,
      "step": 20910
    },
    {
      "epoch": 1.4833894699521362,
      "grad_norm": 1.0136332511901855,
      "learning_rate": 0.00010110149860539877,
      "loss": 0.2765,
      "step": 20920
    },
    {
      "epoch": 1.4840985640843822,
      "grad_norm": 0.8955978751182556,
      "learning_rate": 0.0001010542239871413,
      "loss": 0.2339,
      "step": 20930
    },
    {
      "epoch": 1.4848076582166283,
      "grad_norm": 0.8203029632568359,
      "learning_rate": 0.00010100694936888384,
      "loss": 0.2497,
      "step": 20940
    },
    {
      "epoch": 1.4855167523488744,
      "grad_norm": 0.7438526153564453,
      "learning_rate": 0.00010095967475062639,
      "loss": 0.2466,
      "step": 20950
    },
    {
      "epoch": 1.4862258464811204,
      "grad_norm": 0.8380767107009888,
      "learning_rate": 0.00010091240013236893,
      "loss": 0.2374,
      "step": 20960
    },
    {
      "epoch": 1.4869349406133665,
      "grad_norm": 0.8322046995162964,
      "learning_rate": 0.00010086512551411146,
      "loss": 0.2639,
      "step": 20970
    },
    {
      "epoch": 1.4876440347456126,
      "grad_norm": 1.1371039152145386,
      "learning_rate": 0.00010081785089585403,
      "loss": 0.238,
      "step": 20980
    },
    {
      "epoch": 1.4883531288778586,
      "grad_norm": 0.9365184903144836,
      "learning_rate": 0.00010077057627759656,
      "loss": 0.2332,
      "step": 20990
    },
    {
      "epoch": 1.4890622230101047,
      "grad_norm": 1.171420931816101,
      "learning_rate": 0.0001007233016593391,
      "loss": 0.2103,
      "step": 21000
    },
    {
      "epoch": 1.4897713171423508,
      "grad_norm": 1.2384363412857056,
      "learning_rate": 0.00010067602704108165,
      "loss": 0.2228,
      "step": 21010
    },
    {
      "epoch": 1.4904804112745966,
      "grad_norm": 1.22170090675354,
      "learning_rate": 0.00010062875242282419,
      "loss": 0.2307,
      "step": 21020
    },
    {
      "epoch": 1.4911895054068427,
      "grad_norm": 1.3763082027435303,
      "learning_rate": 0.00010058147780456672,
      "loss": 0.2252,
      "step": 21030
    },
    {
      "epoch": 1.4918985995390888,
      "grad_norm": 1.3278512954711914,
      "learning_rate": 0.00010053420318630929,
      "loss": 0.2294,
      "step": 21040
    },
    {
      "epoch": 1.4926076936713348,
      "grad_norm": 1.4735522270202637,
      "learning_rate": 0.00010048692856805182,
      "loss": 0.2716,
      "step": 21050
    },
    {
      "epoch": 1.493316787803581,
      "grad_norm": 1.2884267568588257,
      "learning_rate": 0.00010043965394979436,
      "loss": 0.2444,
      "step": 21060
    },
    {
      "epoch": 1.494025881935827,
      "grad_norm": 0.9680600762367249,
      "learning_rate": 0.00010039237933153691,
      "loss": 0.2581,
      "step": 21070
    },
    {
      "epoch": 1.494734976068073,
      "grad_norm": 0.7940002679824829,
      "learning_rate": 0.00010034510471327945,
      "loss": 0.271,
      "step": 21080
    },
    {
      "epoch": 1.495444070200319,
      "grad_norm": 1.0149264335632324,
      "learning_rate": 0.00010029783009502198,
      "loss": 0.2473,
      "step": 21090
    },
    {
      "epoch": 1.4961531643325652,
      "grad_norm": 0.992725133895874,
      "learning_rate": 0.00010025055547676454,
      "loss": 0.2266,
      "step": 21100
    },
    {
      "epoch": 1.4968622584648112,
      "grad_norm": 1.0616440773010254,
      "learning_rate": 0.00010020328085850708,
      "loss": 0.2145,
      "step": 21110
    },
    {
      "epoch": 1.4975713525970573,
      "grad_norm": 1.0378663539886475,
      "learning_rate": 0.00010015600624024962,
      "loss": 0.2336,
      "step": 21120
    },
    {
      "epoch": 1.4982804467293034,
      "grad_norm": 1.6631438732147217,
      "learning_rate": 0.00010010873162199214,
      "loss": 0.2345,
      "step": 21130
    },
    {
      "epoch": 1.4989895408615495,
      "grad_norm": 1.1790118217468262,
      "learning_rate": 0.0001000614570037347,
      "loss": 0.2611,
      "step": 21140
    },
    {
      "epoch": 1.4996986349937953,
      "grad_norm": 0.9280483722686768,
      "learning_rate": 0.00010001418238547724,
      "loss": 0.2553,
      "step": 21150
    },
    {
      "epoch": 1.5004077291260414,
      "grad_norm": 1.5483784675598145,
      "learning_rate": 9.996690776721979e-05,
      "loss": 0.263,
      "step": 21160
    },
    {
      "epoch": 1.5011168232582874,
      "grad_norm": 0.8766058087348938,
      "learning_rate": 9.991963314896233e-05,
      "loss": 0.2479,
      "step": 21170
    },
    {
      "epoch": 1.5018259173905335,
      "grad_norm": 1.5047043561935425,
      "learning_rate": 9.987235853070486e-05,
      "loss": 0.2281,
      "step": 21180
    },
    {
      "epoch": 1.5025350115227796,
      "grad_norm": 1.3018290996551514,
      "learning_rate": 9.982508391244741e-05,
      "loss": 0.2457,
      "step": 21190
    },
    {
      "epoch": 1.5032441056550256,
      "grad_norm": 1.8980051279067993,
      "learning_rate": 9.977780929418995e-05,
      "loss": 0.2626,
      "step": 21200
    },
    {
      "epoch": 1.5039531997872717,
      "grad_norm": 0.8879042267799377,
      "learning_rate": 9.97305346759325e-05,
      "loss": 0.2713,
      "step": 21210
    },
    {
      "epoch": 1.5046622939195178,
      "grad_norm": 0.8227822184562683,
      "learning_rate": 9.968326005767505e-05,
      "loss": 0.2586,
      "step": 21220
    },
    {
      "epoch": 1.5053713880517638,
      "grad_norm": 1.559441328048706,
      "learning_rate": 9.963598543941758e-05,
      "loss": 0.2353,
      "step": 21230
    },
    {
      "epoch": 1.50608048218401,
      "grad_norm": 0.9779139161109924,
      "learning_rate": 9.958871082116012e-05,
      "loss": 0.2462,
      "step": 21240
    },
    {
      "epoch": 1.506789576316256,
      "grad_norm": 1.4754712581634521,
      "learning_rate": 9.954143620290267e-05,
      "loss": 0.2023,
      "step": 21250
    },
    {
      "epoch": 1.507498670448502,
      "grad_norm": 1.6844695806503296,
      "learning_rate": 9.949416158464521e-05,
      "loss": 0.2449,
      "step": 21260
    },
    {
      "epoch": 1.5082077645807481,
      "grad_norm": 1.9208952188491821,
      "learning_rate": 9.944688696638776e-05,
      "loss": 0.2549,
      "step": 21270
    },
    {
      "epoch": 1.5089168587129942,
      "grad_norm": 2.165299892425537,
      "learning_rate": 9.939961234813029e-05,
      "loss": 0.2573,
      "step": 21280
    },
    {
      "epoch": 1.5096259528452403,
      "grad_norm": 1.3267463445663452,
      "learning_rate": 9.935233772987283e-05,
      "loss": 0.231,
      "step": 21290
    },
    {
      "epoch": 1.5103350469774863,
      "grad_norm": 1.3353503942489624,
      "learning_rate": 9.930506311161538e-05,
      "loss": 0.2693,
      "step": 21300
    },
    {
      "epoch": 1.5110441411097324,
      "grad_norm": 0.9518617391586304,
      "learning_rate": 9.925778849335792e-05,
      "loss": 0.2488,
      "step": 21310
    },
    {
      "epoch": 1.5117532352419785,
      "grad_norm": 0.9275208711624146,
      "learning_rate": 9.921051387510047e-05,
      "loss": 0.2263,
      "step": 21320
    },
    {
      "epoch": 1.5124623293742245,
      "grad_norm": 0.6733909845352173,
      "learning_rate": 9.916323925684302e-05,
      "loss": 0.2419,
      "step": 21330
    },
    {
      "epoch": 1.5131714235064706,
      "grad_norm": 1.3148897886276245,
      "learning_rate": 9.911596463858555e-05,
      "loss": 0.2175,
      "step": 21340
    },
    {
      "epoch": 1.5138805176387167,
      "grad_norm": 1.5506232976913452,
      "learning_rate": 9.906869002032809e-05,
      "loss": 0.2028,
      "step": 21350
    },
    {
      "epoch": 1.5145896117709627,
      "grad_norm": 2.2674901485443115,
      "learning_rate": 9.902141540207062e-05,
      "loss": 0.2783,
      "step": 21360
    },
    {
      "epoch": 1.5152987059032088,
      "grad_norm": 0.753511905670166,
      "learning_rate": 9.897414078381317e-05,
      "loss": 0.2487,
      "step": 21370
    },
    {
      "epoch": 1.5160078000354547,
      "grad_norm": 1.3423303365707397,
      "learning_rate": 9.892686616555572e-05,
      "loss": 0.2205,
      "step": 21380
    },
    {
      "epoch": 1.5167168941677007,
      "grad_norm": 1.2508667707443237,
      "learning_rate": 9.887959154729826e-05,
      "loss": 0.2431,
      "step": 21390
    },
    {
      "epoch": 1.5174259882999468,
      "grad_norm": 1.2132740020751953,
      "learning_rate": 9.88323169290408e-05,
      "loss": 0.281,
      "step": 21400
    },
    {
      "epoch": 1.5181350824321929,
      "grad_norm": 1.801150918006897,
      "learning_rate": 9.878504231078335e-05,
      "loss": 0.2417,
      "step": 21410
    },
    {
      "epoch": 1.518844176564439,
      "grad_norm": 0.8044003248214722,
      "learning_rate": 9.873776769252588e-05,
      "loss": 0.2477,
      "step": 21420
    },
    {
      "epoch": 1.519553270696685,
      "grad_norm": 1.2844394445419312,
      "learning_rate": 9.869049307426843e-05,
      "loss": 0.2297,
      "step": 21430
    },
    {
      "epoch": 1.520262364828931,
      "grad_norm": 2.302828311920166,
      "learning_rate": 9.864321845601098e-05,
      "loss": 0.2387,
      "step": 21440
    },
    {
      "epoch": 1.5209714589611771,
      "grad_norm": 1.0276899337768555,
      "learning_rate": 9.859594383775352e-05,
      "loss": 0.2361,
      "step": 21450
    },
    {
      "epoch": 1.5216805530934232,
      "grad_norm": 0.8273694515228271,
      "learning_rate": 9.854866921949605e-05,
      "loss": 0.2423,
      "step": 21460
    },
    {
      "epoch": 1.522389647225669,
      "grad_norm": 1.3739888668060303,
      "learning_rate": 9.850139460123859e-05,
      "loss": 0.2546,
      "step": 21470
    },
    {
      "epoch": 1.5230987413579151,
      "grad_norm": 0.975141704082489,
      "learning_rate": 9.845411998298114e-05,
      "loss": 0.2188,
      "step": 21480
    },
    {
      "epoch": 1.5238078354901612,
      "grad_norm": 1.253100037574768,
      "learning_rate": 9.840684536472369e-05,
      "loss": 0.2209,
      "step": 21490
    },
    {
      "epoch": 1.5245169296224073,
      "grad_norm": 1.2132327556610107,
      "learning_rate": 9.835957074646623e-05,
      "loss": 0.2306,
      "step": 21500
    },
    {
      "epoch": 1.5252260237546533,
      "grad_norm": 0.7882266044616699,
      "learning_rate": 9.831229612820876e-05,
      "loss": 0.216,
      "step": 21510
    },
    {
      "epoch": 1.5259351178868994,
      "grad_norm": 1.848790168762207,
      "learning_rate": 9.826502150995131e-05,
      "loss": 0.2493,
      "step": 21520
    },
    {
      "epoch": 1.5266442120191455,
      "grad_norm": 1.5089991092681885,
      "learning_rate": 9.821774689169385e-05,
      "loss": 0.2336,
      "step": 21530
    },
    {
      "epoch": 1.5273533061513915,
      "grad_norm": 0.8219084143638611,
      "learning_rate": 9.81704722734364e-05,
      "loss": 0.2539,
      "step": 21540
    },
    {
      "epoch": 1.5280624002836376,
      "grad_norm": 0.8934189677238464,
      "learning_rate": 9.812319765517895e-05,
      "loss": 0.2504,
      "step": 21550
    },
    {
      "epoch": 1.5287714944158837,
      "grad_norm": 0.8962393403053284,
      "learning_rate": 9.807592303692149e-05,
      "loss": 0.2368,
      "step": 21560
    },
    {
      "epoch": 1.5294805885481297,
      "grad_norm": 1.5847727060317993,
      "learning_rate": 9.802864841866402e-05,
      "loss": 0.2673,
      "step": 21570
    },
    {
      "epoch": 1.5301896826803758,
      "grad_norm": 1.2760705947875977,
      "learning_rate": 9.798137380040656e-05,
      "loss": 0.2447,
      "step": 21580
    },
    {
      "epoch": 1.5308987768126219,
      "grad_norm": 1.5336575508117676,
      "learning_rate": 9.793409918214911e-05,
      "loss": 0.2424,
      "step": 21590
    },
    {
      "epoch": 1.531607870944868,
      "grad_norm": 0.8031880259513855,
      "learning_rate": 9.788682456389166e-05,
      "loss": 0.2476,
      "step": 21600
    },
    {
      "epoch": 1.532316965077114,
      "grad_norm": 0.9419750571250916,
      "learning_rate": 9.78395499456342e-05,
      "loss": 0.236,
      "step": 21610
    },
    {
      "epoch": 1.53302605920936,
      "grad_norm": 1.5188772678375244,
      "learning_rate": 9.779227532737673e-05,
      "loss": 0.2324,
      "step": 21620
    },
    {
      "epoch": 1.5337351533416061,
      "grad_norm": 0.9322875142097473,
      "learning_rate": 9.774500070911928e-05,
      "loss": 0.2554,
      "step": 21630
    },
    {
      "epoch": 1.5344442474738522,
      "grad_norm": 2.0985405445098877,
      "learning_rate": 9.769772609086182e-05,
      "loss": 0.2377,
      "step": 21640
    },
    {
      "epoch": 1.5351533416060983,
      "grad_norm": 2.484513282775879,
      "learning_rate": 9.765045147260437e-05,
      "loss": 0.2573,
      "step": 21650
    },
    {
      "epoch": 1.5358624357383444,
      "grad_norm": 1.626941204071045,
      "learning_rate": 9.76031768543469e-05,
      "loss": 0.2568,
      "step": 21660
    },
    {
      "epoch": 1.5365715298705904,
      "grad_norm": 1.7043275833129883,
      "learning_rate": 9.755590223608945e-05,
      "loss": 0.2933,
      "step": 21670
    },
    {
      "epoch": 1.5372806240028365,
      "grad_norm": 1.4362506866455078,
      "learning_rate": 9.750862761783199e-05,
      "loss": 0.2571,
      "step": 21680
    },
    {
      "epoch": 1.5379897181350826,
      "grad_norm": 0.945958137512207,
      "learning_rate": 9.746135299957452e-05,
      "loss": 0.2359,
      "step": 21690
    },
    {
      "epoch": 1.5386988122673286,
      "grad_norm": 2.1887168884277344,
      "learning_rate": 9.741407838131707e-05,
      "loss": 0.259,
      "step": 21700
    },
    {
      "epoch": 1.5394079063995747,
      "grad_norm": 0.8866830468177795,
      "learning_rate": 9.736680376305962e-05,
      "loss": 0.237,
      "step": 21710
    },
    {
      "epoch": 1.5401170005318205,
      "grad_norm": 2.2951207160949707,
      "learning_rate": 9.731952914480216e-05,
      "loss": 0.2486,
      "step": 21720
    },
    {
      "epoch": 1.5408260946640666,
      "grad_norm": 0.9378810524940491,
      "learning_rate": 9.72722545265447e-05,
      "loss": 0.2326,
      "step": 21730
    },
    {
      "epoch": 1.5415351887963127,
      "grad_norm": 0.8885775208473206,
      "learning_rate": 9.722497990828725e-05,
      "loss": 0.2439,
      "step": 21740
    },
    {
      "epoch": 1.5422442829285588,
      "grad_norm": 0.8160804510116577,
      "learning_rate": 9.717770529002978e-05,
      "loss": 0.2388,
      "step": 21750
    },
    {
      "epoch": 1.5429533770608048,
      "grad_norm": 1.0172959566116333,
      "learning_rate": 9.713043067177233e-05,
      "loss": 0.2426,
      "step": 21760
    },
    {
      "epoch": 1.543662471193051,
      "grad_norm": 1.704419732093811,
      "learning_rate": 9.708315605351487e-05,
      "loss": 0.2426,
      "step": 21770
    },
    {
      "epoch": 1.544371565325297,
      "grad_norm": 0.694932758808136,
      "learning_rate": 9.703588143525742e-05,
      "loss": 0.2081,
      "step": 21780
    },
    {
      "epoch": 1.545080659457543,
      "grad_norm": 1.1878019571304321,
      "learning_rate": 9.698860681699996e-05,
      "loss": 0.2559,
      "step": 21790
    },
    {
      "epoch": 1.545789753589789,
      "grad_norm": 3.1704320907592773,
      "learning_rate": 9.694133219874249e-05,
      "loss": 0.2319,
      "step": 21800
    },
    {
      "epoch": 1.546498847722035,
      "grad_norm": 2.3511719703674316,
      "learning_rate": 9.689405758048504e-05,
      "loss": 0.2688,
      "step": 21810
    },
    {
      "epoch": 1.547207941854281,
      "grad_norm": 1.543775200843811,
      "learning_rate": 9.684678296222759e-05,
      "loss": 0.2287,
      "step": 21820
    },
    {
      "epoch": 1.547917035986527,
      "grad_norm": 1.6104371547698975,
      "learning_rate": 9.679950834397013e-05,
      "loss": 0.2627,
      "step": 21830
    },
    {
      "epoch": 1.5486261301187731,
      "grad_norm": 1.9118674993515015,
      "learning_rate": 9.675223372571266e-05,
      "loss": 0.3039,
      "step": 21840
    },
    {
      "epoch": 1.5493352242510192,
      "grad_norm": 1.3002409934997559,
      "learning_rate": 9.670495910745521e-05,
      "loss": 0.2447,
      "step": 21850
    },
    {
      "epoch": 1.5500443183832653,
      "grad_norm": 2.3995044231414795,
      "learning_rate": 9.665768448919775e-05,
      "loss": 0.2694,
      "step": 21860
    },
    {
      "epoch": 1.5507534125155114,
      "grad_norm": 1.038487195968628,
      "learning_rate": 9.66104098709403e-05,
      "loss": 0.2671,
      "step": 21870
    },
    {
      "epoch": 1.5514625066477574,
      "grad_norm": 1.3713594675064087,
      "learning_rate": 9.656313525268284e-05,
      "loss": 0.2449,
      "step": 21880
    },
    {
      "epoch": 1.5521716007800035,
      "grad_norm": 1.4130245447158813,
      "learning_rate": 9.651586063442539e-05,
      "loss": 0.2346,
      "step": 21890
    },
    {
      "epoch": 1.5528806949122496,
      "grad_norm": 0.6778013706207275,
      "learning_rate": 9.646858601616792e-05,
      "loss": 0.2417,
      "step": 21900
    },
    {
      "epoch": 1.5535897890444956,
      "grad_norm": 0.9317623376846313,
      "learning_rate": 9.642131139791046e-05,
      "loss": 0.2362,
      "step": 21910
    },
    {
      "epoch": 1.5542988831767417,
      "grad_norm": 1.5506229400634766,
      "learning_rate": 9.637403677965301e-05,
      "loss": 0.2286,
      "step": 21920
    },
    {
      "epoch": 1.5550079773089878,
      "grad_norm": 0.9743257761001587,
      "learning_rate": 9.632676216139556e-05,
      "loss": 0.2548,
      "step": 21930
    },
    {
      "epoch": 1.5557170714412338,
      "grad_norm": 2.3221960067749023,
      "learning_rate": 9.62794875431381e-05,
      "loss": 0.2255,
      "step": 21940
    },
    {
      "epoch": 1.55642616557348,
      "grad_norm": 0.9769547581672668,
      "learning_rate": 9.623221292488064e-05,
      "loss": 0.2611,
      "step": 21950
    },
    {
      "epoch": 1.557135259705726,
      "grad_norm": 0.8418150544166565,
      "learning_rate": 9.618493830662317e-05,
      "loss": 0.2659,
      "step": 21960
    },
    {
      "epoch": 1.557844353837972,
      "grad_norm": 1.6947863101959229,
      "learning_rate": 9.613766368836572e-05,
      "loss": 0.2371,
      "step": 21970
    },
    {
      "epoch": 1.558553447970218,
      "grad_norm": 1.0763753652572632,
      "learning_rate": 9.609038907010827e-05,
      "loss": 0.2494,
      "step": 21980
    },
    {
      "epoch": 1.5592625421024642,
      "grad_norm": 2.8720152378082275,
      "learning_rate": 9.60431144518508e-05,
      "loss": 0.2228,
      "step": 21990
    },
    {
      "epoch": 1.5599716362347102,
      "grad_norm": 1.2201155424118042,
      "learning_rate": 9.599583983359335e-05,
      "loss": 0.222,
      "step": 22000
    },
    {
      "epoch": 1.5606807303669563,
      "grad_norm": 1.2137740850448608,
      "learning_rate": 9.594856521533589e-05,
      "loss": 0.2434,
      "step": 22010
    },
    {
      "epoch": 1.5613898244992024,
      "grad_norm": 0.9832647442817688,
      "learning_rate": 9.590129059707843e-05,
      "loss": 0.2229,
      "step": 22020
    },
    {
      "epoch": 1.5620989186314485,
      "grad_norm": 1.2299684286117554,
      "learning_rate": 9.585401597882098e-05,
      "loss": 0.2468,
      "step": 22030
    },
    {
      "epoch": 1.5628080127636945,
      "grad_norm": 1.269514560699463,
      "learning_rate": 9.580674136056353e-05,
      "loss": 0.2244,
      "step": 22040
    },
    {
      "epoch": 1.5635171068959406,
      "grad_norm": 0.9664850234985352,
      "learning_rate": 9.575946674230606e-05,
      "loss": 0.2537,
      "step": 22050
    },
    {
      "epoch": 1.5642262010281867,
      "grad_norm": 0.9715595245361328,
      "learning_rate": 9.571219212404861e-05,
      "loss": 0.2594,
      "step": 22060
    },
    {
      "epoch": 1.5649352951604325,
      "grad_norm": 1.9779491424560547,
      "learning_rate": 9.566491750579113e-05,
      "loss": 0.2806,
      "step": 22070
    },
    {
      "epoch": 1.5656443892926786,
      "grad_norm": 0.9704970717430115,
      "learning_rate": 9.561764288753368e-05,
      "loss": 0.2135,
      "step": 22080
    },
    {
      "epoch": 1.5663534834249246,
      "grad_norm": 0.9074795842170715,
      "learning_rate": 9.557036826927623e-05,
      "loss": 0.2477,
      "step": 22090
    },
    {
      "epoch": 1.5670625775571707,
      "grad_norm": 2.1840579509735107,
      "learning_rate": 9.552309365101877e-05,
      "loss": 0.2561,
      "step": 22100
    },
    {
      "epoch": 1.5677716716894168,
      "grad_norm": 2.5538430213928223,
      "learning_rate": 9.547581903276132e-05,
      "loss": 0.2819,
      "step": 22110
    },
    {
      "epoch": 1.5684807658216628,
      "grad_norm": 1.0485790967941284,
      "learning_rate": 9.542854441450386e-05,
      "loss": 0.2285,
      "step": 22120
    },
    {
      "epoch": 1.569189859953909,
      "grad_norm": 1.6985470056533813,
      "learning_rate": 9.538126979624639e-05,
      "loss": 0.2335,
      "step": 22130
    },
    {
      "epoch": 1.569898954086155,
      "grad_norm": 3.5486583709716797,
      "learning_rate": 9.533399517798894e-05,
      "loss": 0.2467,
      "step": 22140
    },
    {
      "epoch": 1.5706080482184008,
      "grad_norm": 1.7219778299331665,
      "learning_rate": 9.528672055973148e-05,
      "loss": 0.2791,
      "step": 22150
    },
    {
      "epoch": 1.571317142350647,
      "grad_norm": 4.057058811187744,
      "learning_rate": 9.523944594147403e-05,
      "loss": 0.2324,
      "step": 22160
    },
    {
      "epoch": 1.572026236482893,
      "grad_norm": 0.6895734667778015,
      "learning_rate": 9.519217132321658e-05,
      "loss": 0.2545,
      "step": 22170
    },
    {
      "epoch": 1.572735330615139,
      "grad_norm": 3.2437620162963867,
      "learning_rate": 9.51448967049591e-05,
      "loss": 0.2539,
      "step": 22180
    },
    {
      "epoch": 1.573444424747385,
      "grad_norm": 2.4187064170837402,
      "learning_rate": 9.509762208670165e-05,
      "loss": 0.2779,
      "step": 22190
    },
    {
      "epoch": 1.5741535188796312,
      "grad_norm": 1.0454577207565308,
      "learning_rate": 9.50503474684442e-05,
      "loss": 0.2526,
      "step": 22200
    },
    {
      "epoch": 1.5748626130118772,
      "grad_norm": 0.8703338503837585,
      "learning_rate": 9.500307285018674e-05,
      "loss": 0.2375,
      "step": 22210
    },
    {
      "epoch": 1.5755717071441233,
      "grad_norm": 0.8323863744735718,
      "learning_rate": 9.495579823192929e-05,
      "loss": 0.2646,
      "step": 22220
    },
    {
      "epoch": 1.5762808012763694,
      "grad_norm": 1.2884453535079956,
      "learning_rate": 9.490852361367182e-05,
      "loss": 0.255,
      "step": 22230
    },
    {
      "epoch": 1.5769898954086154,
      "grad_norm": 1.0027971267700195,
      "learning_rate": 9.486124899541436e-05,
      "loss": 0.2382,
      "step": 22240
    },
    {
      "epoch": 1.5776989895408615,
      "grad_norm": 1.0439273118972778,
      "learning_rate": 9.481397437715691e-05,
      "loss": 0.2442,
      "step": 22250
    },
    {
      "epoch": 1.5784080836731076,
      "grad_norm": 1.731117844581604,
      "learning_rate": 9.476669975889945e-05,
      "loss": 0.2229,
      "step": 22260
    },
    {
      "epoch": 1.5791171778053537,
      "grad_norm": 0.8108920454978943,
      "learning_rate": 9.4719425140642e-05,
      "loss": 0.2283,
      "step": 22270
    },
    {
      "epoch": 1.5798262719375997,
      "grad_norm": 0.6641179919242859,
      "learning_rate": 9.467215052238455e-05,
      "loss": 0.2427,
      "step": 22280
    },
    {
      "epoch": 1.5805353660698458,
      "grad_norm": 1.6404674053192139,
      "learning_rate": 9.462487590412707e-05,
      "loss": 0.2433,
      "step": 22290
    },
    {
      "epoch": 1.5812444602020919,
      "grad_norm": 0.811573326587677,
      "learning_rate": 9.457760128586962e-05,
      "loss": 0.2456,
      "step": 22300
    },
    {
      "epoch": 1.581953554334338,
      "grad_norm": 1.002459168434143,
      "learning_rate": 9.453032666761217e-05,
      "loss": 0.2881,
      "step": 22310
    },
    {
      "epoch": 1.582662648466584,
      "grad_norm": 1.6006935834884644,
      "learning_rate": 9.44830520493547e-05,
      "loss": 0.237,
      "step": 22320
    },
    {
      "epoch": 1.58337174259883,
      "grad_norm": 0.8524059057235718,
      "learning_rate": 9.443577743109725e-05,
      "loss": 0.2445,
      "step": 22330
    },
    {
      "epoch": 1.5840808367310761,
      "grad_norm": 1.1075527667999268,
      "learning_rate": 9.438850281283979e-05,
      "loss": 0.2465,
      "step": 22340
    },
    {
      "epoch": 1.5847899308633222,
      "grad_norm": 0.9748866558074951,
      "learning_rate": 9.434122819458233e-05,
      "loss": 0.2755,
      "step": 22350
    },
    {
      "epoch": 1.5854990249955683,
      "grad_norm": 1.530024766921997,
      "learning_rate": 9.429395357632488e-05,
      "loss": 0.21,
      "step": 22360
    },
    {
      "epoch": 1.5862081191278143,
      "grad_norm": 2.5321757793426514,
      "learning_rate": 9.424667895806741e-05,
      "loss": 0.2647,
      "step": 22370
    },
    {
      "epoch": 1.5869172132600604,
      "grad_norm": 1.3962514400482178,
      "learning_rate": 9.419940433980996e-05,
      "loss": 0.2349,
      "step": 22380
    },
    {
      "epoch": 1.5876263073923065,
      "grad_norm": 1.46037757396698,
      "learning_rate": 9.415212972155251e-05,
      "loss": 0.2337,
      "step": 22390
    },
    {
      "epoch": 1.5883354015245525,
      "grad_norm": 1.2389717102050781,
      "learning_rate": 9.410485510329504e-05,
      "loss": 0.2678,
      "step": 22400
    },
    {
      "epoch": 1.5890444956567984,
      "grad_norm": 1.4281243085861206,
      "learning_rate": 9.405758048503759e-05,
      "loss": 0.2917,
      "step": 22410
    },
    {
      "epoch": 1.5897535897890445,
      "grad_norm": 1.1033949851989746,
      "learning_rate": 9.401030586678014e-05,
      "loss": 0.23,
      "step": 22420
    },
    {
      "epoch": 1.5904626839212905,
      "grad_norm": 1.191511631011963,
      "learning_rate": 9.396303124852267e-05,
      "loss": 0.2673,
      "step": 22430
    },
    {
      "epoch": 1.5911717780535366,
      "grad_norm": 1.427752137184143,
      "learning_rate": 9.391575663026522e-05,
      "loss": 0.2578,
      "step": 22440
    },
    {
      "epoch": 1.5918808721857827,
      "grad_norm": 0.8742459416389465,
      "learning_rate": 9.386848201200776e-05,
      "loss": 0.2292,
      "step": 22450
    },
    {
      "epoch": 1.5925899663180287,
      "grad_norm": 1.1926326751708984,
      "learning_rate": 9.38212073937503e-05,
      "loss": 0.235,
      "step": 22460
    },
    {
      "epoch": 1.5932990604502748,
      "grad_norm": 2.1698966026306152,
      "learning_rate": 9.377393277549284e-05,
      "loss": 0.2122,
      "step": 22470
    },
    {
      "epoch": 1.5940081545825209,
      "grad_norm": 0.7773762345314026,
      "learning_rate": 9.372665815723538e-05,
      "loss": 0.2348,
      "step": 22480
    },
    {
      "epoch": 1.5947172487147667,
      "grad_norm": 1.057146668434143,
      "learning_rate": 9.367938353897793e-05,
      "loss": 0.2534,
      "step": 22490
    },
    {
      "epoch": 1.5954263428470128,
      "grad_norm": 1.8004817962646484,
      "learning_rate": 9.363210892072048e-05,
      "loss": 0.2941,
      "step": 22500
    },
    {
      "epoch": 1.5961354369792589,
      "grad_norm": 1.6862616539001465,
      "learning_rate": 9.3584834302463e-05,
      "loss": 0.2431,
      "step": 22510
    },
    {
      "epoch": 1.596844531111505,
      "grad_norm": 1.1136959791183472,
      "learning_rate": 9.353755968420555e-05,
      "loss": 0.2489,
      "step": 22520
    },
    {
      "epoch": 1.597553625243751,
      "grad_norm": 1.0157283544540405,
      "learning_rate": 9.34902850659481e-05,
      "loss": 0.2224,
      "step": 22530
    },
    {
      "epoch": 1.598262719375997,
      "grad_norm": 0.8791188597679138,
      "learning_rate": 9.344301044769064e-05,
      "loss": 0.2613,
      "step": 22540
    },
    {
      "epoch": 1.5989718135082431,
      "grad_norm": 2.3546299934387207,
      "learning_rate": 9.339573582943319e-05,
      "loss": 0.2335,
      "step": 22550
    },
    {
      "epoch": 1.5996809076404892,
      "grad_norm": 0.880043089389801,
      "learning_rate": 9.334846121117572e-05,
      "loss": 0.2511,
      "step": 22560
    },
    {
      "epoch": 1.6003900017727353,
      "grad_norm": 1.0937837362289429,
      "learning_rate": 9.330118659291826e-05,
      "loss": 0.223,
      "step": 22570
    },
    {
      "epoch": 1.6010990959049813,
      "grad_norm": 1.3698612451553345,
      "learning_rate": 9.325391197466081e-05,
      "loss": 0.2164,
      "step": 22580
    },
    {
      "epoch": 1.6018081900372274,
      "grad_norm": 2.0165679454803467,
      "learning_rate": 9.320663735640335e-05,
      "loss": 0.2774,
      "step": 22590
    },
    {
      "epoch": 1.6025172841694735,
      "grad_norm": 1.3153090476989746,
      "learning_rate": 9.31593627381459e-05,
      "loss": 0.2544,
      "step": 22600
    },
    {
      "epoch": 1.6032263783017195,
      "grad_norm": 1.394832730293274,
      "learning_rate": 9.311208811988845e-05,
      "loss": 0.2131,
      "step": 22610
    },
    {
      "epoch": 1.6039354724339656,
      "grad_norm": 1.0468566417694092,
      "learning_rate": 9.306481350163098e-05,
      "loss": 0.2412,
      "step": 22620
    },
    {
      "epoch": 1.6046445665662117,
      "grad_norm": 0.7673608064651489,
      "learning_rate": 9.301753888337352e-05,
      "loss": 0.2395,
      "step": 22630
    },
    {
      "epoch": 1.6053536606984578,
      "grad_norm": 2.0293450355529785,
      "learning_rate": 9.297026426511606e-05,
      "loss": 0.2443,
      "step": 22640
    },
    {
      "epoch": 1.6060627548307038,
      "grad_norm": 1.0993924140930176,
      "learning_rate": 9.29229896468586e-05,
      "loss": 0.2258,
      "step": 22650
    },
    {
      "epoch": 1.60677184896295,
      "grad_norm": 2.2226366996765137,
      "learning_rate": 9.287571502860116e-05,
      "loss": 0.2163,
      "step": 22660
    },
    {
      "epoch": 1.607480943095196,
      "grad_norm": 1.9202338457107544,
      "learning_rate": 9.282844041034369e-05,
      "loss": 0.2711,
      "step": 22670
    },
    {
      "epoch": 1.608190037227442,
      "grad_norm": 1.0462305545806885,
      "learning_rate": 9.278116579208623e-05,
      "loss": 0.2694,
      "step": 22680
    },
    {
      "epoch": 1.608899131359688,
      "grad_norm": 0.9695535898208618,
      "learning_rate": 9.273389117382878e-05,
      "loss": 0.2505,
      "step": 22690
    },
    {
      "epoch": 1.6096082254919342,
      "grad_norm": 1.2457427978515625,
      "learning_rate": 9.268661655557131e-05,
      "loss": 0.2299,
      "step": 22700
    },
    {
      "epoch": 1.6103173196241802,
      "grad_norm": 1.4493112564086914,
      "learning_rate": 9.263934193731386e-05,
      "loss": 0.2478,
      "step": 22710
    },
    {
      "epoch": 1.6110264137564263,
      "grad_norm": 0.8462590575218201,
      "learning_rate": 9.259206731905641e-05,
      "loss": 0.2479,
      "step": 22720
    },
    {
      "epoch": 1.6117355078886724,
      "grad_norm": 0.7415811419487,
      "learning_rate": 9.254479270079895e-05,
      "loss": 0.2419,
      "step": 22730
    },
    {
      "epoch": 1.6124446020209184,
      "grad_norm": 1.1036490201950073,
      "learning_rate": 9.249751808254149e-05,
      "loss": 0.2459,
      "step": 22740
    },
    {
      "epoch": 1.6131536961531643,
      "grad_norm": 0.9097210764884949,
      "learning_rate": 9.245024346428402e-05,
      "loss": 0.212,
      "step": 22750
    },
    {
      "epoch": 1.6138627902854104,
      "grad_norm": 0.957539975643158,
      "learning_rate": 9.240296884602657e-05,
      "loss": 0.2452,
      "step": 22760
    },
    {
      "epoch": 1.6145718844176564,
      "grad_norm": 1.512709140777588,
      "learning_rate": 9.235569422776912e-05,
      "loss": 0.2753,
      "step": 22770
    },
    {
      "epoch": 1.6152809785499025,
      "grad_norm": 3.2916135787963867,
      "learning_rate": 9.230841960951166e-05,
      "loss": 0.2116,
      "step": 22780
    },
    {
      "epoch": 1.6159900726821486,
      "grad_norm": 2.2113170623779297,
      "learning_rate": 9.22611449912542e-05,
      "loss": 0.248,
      "step": 22790
    },
    {
      "epoch": 1.6166991668143946,
      "grad_norm": 0.8332503437995911,
      "learning_rate": 9.221387037299674e-05,
      "loss": 0.2442,
      "step": 22800
    },
    {
      "epoch": 1.6174082609466407,
      "grad_norm": 1.6999539136886597,
      "learning_rate": 9.216659575473928e-05,
      "loss": 0.2393,
      "step": 22810
    },
    {
      "epoch": 1.6181173550788868,
      "grad_norm": 0.902827262878418,
      "learning_rate": 9.211932113648183e-05,
      "loss": 0.2513,
      "step": 22820
    },
    {
      "epoch": 1.6188264492111326,
      "grad_norm": 1.0551221370697021,
      "learning_rate": 9.207204651822438e-05,
      "loss": 0.2462,
      "step": 22830
    },
    {
      "epoch": 1.6195355433433787,
      "grad_norm": 1.5773166418075562,
      "learning_rate": 9.202477189996692e-05,
      "loss": 0.2333,
      "step": 22840
    },
    {
      "epoch": 1.6202446374756247,
      "grad_norm": 0.8373663425445557,
      "learning_rate": 9.197749728170945e-05,
      "loss": 0.2416,
      "step": 22850
    },
    {
      "epoch": 1.6209537316078708,
      "grad_norm": 1.2118470668792725,
      "learning_rate": 9.193022266345199e-05,
      "loss": 0.2434,
      "step": 22860
    },
    {
      "epoch": 1.6216628257401169,
      "grad_norm": 1.1684077978134155,
      "learning_rate": 9.188294804519454e-05,
      "loss": 0.2431,
      "step": 22870
    },
    {
      "epoch": 1.622371919872363,
      "grad_norm": 0.8830124139785767,
      "learning_rate": 9.183567342693709e-05,
      "loss": 0.2718,
      "step": 22880
    },
    {
      "epoch": 1.623081014004609,
      "grad_norm": 1.5123543739318848,
      "learning_rate": 9.178839880867963e-05,
      "loss": 0.2559,
      "step": 22890
    },
    {
      "epoch": 1.623790108136855,
      "grad_norm": 1.1301767826080322,
      "learning_rate": 9.174112419042216e-05,
      "loss": 0.2456,
      "step": 22900
    },
    {
      "epoch": 1.6244992022691012,
      "grad_norm": 0.909696102142334,
      "learning_rate": 9.169384957216471e-05,
      "loss": 0.2762,
      "step": 22910
    },
    {
      "epoch": 1.6252082964013472,
      "grad_norm": 1.5048881769180298,
      "learning_rate": 9.164657495390725e-05,
      "loss": 0.2572,
      "step": 22920
    },
    {
      "epoch": 1.6259173905335933,
      "grad_norm": 1.3009872436523438,
      "learning_rate": 9.15993003356498e-05,
      "loss": 0.2695,
      "step": 22930
    },
    {
      "epoch": 1.6266264846658394,
      "grad_norm": 1.4253292083740234,
      "learning_rate": 9.155202571739233e-05,
      "loss": 0.2326,
      "step": 22940
    },
    {
      "epoch": 1.6273355787980854,
      "grad_norm": 0.8879268765449524,
      "learning_rate": 9.150475109913488e-05,
      "loss": 0.2353,
      "step": 22950
    },
    {
      "epoch": 1.6280446729303315,
      "grad_norm": 5.063426971435547,
      "learning_rate": 9.145747648087742e-05,
      "loss": 0.2464,
      "step": 22960
    },
    {
      "epoch": 1.6287537670625776,
      "grad_norm": 0.9531437754631042,
      "learning_rate": 9.141020186261996e-05,
      "loss": 0.2367,
      "step": 22970
    },
    {
      "epoch": 1.6294628611948236,
      "grad_norm": 1.0774290561676025,
      "learning_rate": 9.13629272443625e-05,
      "loss": 0.2131,
      "step": 22980
    },
    {
      "epoch": 1.6301719553270697,
      "grad_norm": 1.099023699760437,
      "learning_rate": 9.131565262610506e-05,
      "loss": 0.248,
      "step": 22990
    },
    {
      "epoch": 1.6308810494593158,
      "grad_norm": 3.0757505893707275,
      "learning_rate": 9.126837800784759e-05,
      "loss": 0.2432,
      "step": 23000
    },
    {
      "epoch": 1.6315901435915618,
      "grad_norm": 1.2289867401123047,
      "learning_rate": 9.122110338959013e-05,
      "loss": 0.2337,
      "step": 23010
    },
    {
      "epoch": 1.632299237723808,
      "grad_norm": 2.578523635864258,
      "learning_rate": 9.117382877133268e-05,
      "loss": 0.2649,
      "step": 23020
    },
    {
      "epoch": 1.633008331856054,
      "grad_norm": 0.6576067209243774,
      "learning_rate": 9.112655415307521e-05,
      "loss": 0.2416,
      "step": 23030
    },
    {
      "epoch": 1.6337174259883,
      "grad_norm": 1.6599446535110474,
      "learning_rate": 9.107927953481776e-05,
      "loss": 0.2369,
      "step": 23040
    },
    {
      "epoch": 1.6344265201205461,
      "grad_norm": 1.7603778839111328,
      "learning_rate": 9.10320049165603e-05,
      "loss": 0.2723,
      "step": 23050
    },
    {
      "epoch": 1.6351356142527922,
      "grad_norm": 1.1428492069244385,
      "learning_rate": 9.098473029830285e-05,
      "loss": 0.2601,
      "step": 23060
    },
    {
      "epoch": 1.6358447083850383,
      "grad_norm": 0.9832437038421631,
      "learning_rate": 9.093745568004539e-05,
      "loss": 0.2517,
      "step": 23070
    },
    {
      "epoch": 1.6365538025172843,
      "grad_norm": 0.8177551031112671,
      "learning_rate": 9.089018106178792e-05,
      "loss": 0.2295,
      "step": 23080
    },
    {
      "epoch": 1.6372628966495302,
      "grad_norm": 0.9435787796974182,
      "learning_rate": 9.084290644353047e-05,
      "loss": 0.2567,
      "step": 23090
    },
    {
      "epoch": 1.6379719907817762,
      "grad_norm": 1.7976638078689575,
      "learning_rate": 9.079563182527302e-05,
      "loss": 0.2123,
      "step": 23100
    },
    {
      "epoch": 1.6386810849140223,
      "grad_norm": 0.7043121457099915,
      "learning_rate": 9.074835720701556e-05,
      "loss": 0.246,
      "step": 23110
    },
    {
      "epoch": 1.6393901790462684,
      "grad_norm": 1.507311463356018,
      "learning_rate": 9.07010825887581e-05,
      "loss": 0.2319,
      "step": 23120
    },
    {
      "epoch": 1.6400992731785144,
      "grad_norm": 0.8813678622245789,
      "learning_rate": 9.065380797050063e-05,
      "loss": 0.2459,
      "step": 23130
    },
    {
      "epoch": 1.6408083673107605,
      "grad_norm": 0.857236921787262,
      "learning_rate": 9.060653335224318e-05,
      "loss": 0.2473,
      "step": 23140
    },
    {
      "epoch": 1.6415174614430066,
      "grad_norm": 1.340800166130066,
      "learning_rate": 9.055925873398573e-05,
      "loss": 0.2391,
      "step": 23150
    },
    {
      "epoch": 1.6422265555752527,
      "grad_norm": 3.7560067176818848,
      "learning_rate": 9.051198411572827e-05,
      "loss": 0.2265,
      "step": 23160
    },
    {
      "epoch": 1.6429356497074987,
      "grad_norm": 1.5452451705932617,
      "learning_rate": 9.046470949747082e-05,
      "loss": 0.2525,
      "step": 23170
    },
    {
      "epoch": 1.6436447438397446,
      "grad_norm": 1.2668524980545044,
      "learning_rate": 9.041743487921335e-05,
      "loss": 0.2599,
      "step": 23180
    },
    {
      "epoch": 1.6443538379719906,
      "grad_norm": 0.8085569739341736,
      "learning_rate": 9.037016026095589e-05,
      "loss": 0.2185,
      "step": 23190
    },
    {
      "epoch": 1.6450629321042367,
      "grad_norm": 0.9066002368927002,
      "learning_rate": 9.032288564269844e-05,
      "loss": 0.222,
      "step": 23200
    },
    {
      "epoch": 1.6457720262364828,
      "grad_norm": 0.760573148727417,
      "learning_rate": 9.027561102444099e-05,
      "loss": 0.2464,
      "step": 23210
    },
    {
      "epoch": 1.6464811203687288,
      "grad_norm": 0.8563365340232849,
      "learning_rate": 9.022833640618353e-05,
      "loss": 0.2367,
      "step": 23220
    },
    {
      "epoch": 1.647190214500975,
      "grad_norm": 0.8231765031814575,
      "learning_rate": 9.018106178792606e-05,
      "loss": 0.2382,
      "step": 23230
    },
    {
      "epoch": 1.647899308633221,
      "grad_norm": 0.9045810103416443,
      "learning_rate": 9.01337871696686e-05,
      "loss": 0.197,
      "step": 23240
    },
    {
      "epoch": 1.648608402765467,
      "grad_norm": 0.7158458232879639,
      "learning_rate": 9.008651255141115e-05,
      "loss": 0.2192,
      "step": 23250
    },
    {
      "epoch": 1.6493174968977131,
      "grad_norm": 1.118839144706726,
      "learning_rate": 9.00392379331537e-05,
      "loss": 0.2456,
      "step": 23260
    },
    {
      "epoch": 1.6500265910299592,
      "grad_norm": 1.7350845336914062,
      "learning_rate": 8.999196331489624e-05,
      "loss": 0.2315,
      "step": 23270
    },
    {
      "epoch": 1.6507356851622053,
      "grad_norm": 1.2830848693847656,
      "learning_rate": 8.994468869663878e-05,
      "loss": 0.2578,
      "step": 23280
    },
    {
      "epoch": 1.6514447792944513,
      "grad_norm": 7.925143241882324,
      "learning_rate": 8.989741407838132e-05,
      "loss": 0.2574,
      "step": 23290
    },
    {
      "epoch": 1.6521538734266974,
      "grad_norm": 1.7490817308425903,
      "learning_rate": 8.985013946012386e-05,
      "loss": 0.2641,
      "step": 23300
    },
    {
      "epoch": 1.6528629675589435,
      "grad_norm": 1.157402515411377,
      "learning_rate": 8.980286484186641e-05,
      "loss": 0.2288,
      "step": 23310
    },
    {
      "epoch": 1.6535720616911895,
      "grad_norm": 0.673468828201294,
      "learning_rate": 8.975559022360896e-05,
      "loss": 0.2218,
      "step": 23320
    },
    {
      "epoch": 1.6542811558234356,
      "grad_norm": 1.2689330577850342,
      "learning_rate": 8.97083156053515e-05,
      "loss": 0.2224,
      "step": 23330
    },
    {
      "epoch": 1.6549902499556817,
      "grad_norm": 0.9428840279579163,
      "learning_rate": 8.966104098709403e-05,
      "loss": 0.2563,
      "step": 23340
    },
    {
      "epoch": 1.6556993440879277,
      "grad_norm": 1.8845534324645996,
      "learning_rate": 8.961376636883657e-05,
      "loss": 0.259,
      "step": 23350
    },
    {
      "epoch": 1.6564084382201738,
      "grad_norm": 1.0209282636642456,
      "learning_rate": 8.956649175057912e-05,
      "loss": 0.2669,
      "step": 23360
    },
    {
      "epoch": 1.6571175323524199,
      "grad_norm": 1.6313265562057495,
      "learning_rate": 8.951921713232167e-05,
      "loss": 0.235,
      "step": 23370
    },
    {
      "epoch": 1.657826626484666,
      "grad_norm": 1.1059238910675049,
      "learning_rate": 8.94719425140642e-05,
      "loss": 0.2127,
      "step": 23380
    },
    {
      "epoch": 1.658535720616912,
      "grad_norm": 0.8002135753631592,
      "learning_rate": 8.942466789580675e-05,
      "loss": 0.2674,
      "step": 23390
    },
    {
      "epoch": 1.659244814749158,
      "grad_norm": 1.4849025011062622,
      "learning_rate": 8.937739327754929e-05,
      "loss": 0.2494,
      "step": 23400
    },
    {
      "epoch": 1.6599539088814041,
      "grad_norm": 1.1252193450927734,
      "learning_rate": 8.933011865929182e-05,
      "loss": 0.2445,
      "step": 23410
    },
    {
      "epoch": 1.6606630030136502,
      "grad_norm": 1.2557106018066406,
      "learning_rate": 8.928284404103437e-05,
      "loss": 0.2357,
      "step": 23420
    },
    {
      "epoch": 1.661372097145896,
      "grad_norm": 1.035186767578125,
      "learning_rate": 8.923556942277691e-05,
      "loss": 0.2549,
      "step": 23430
    },
    {
      "epoch": 1.6620811912781421,
      "grad_norm": 1.7871819734573364,
      "learning_rate": 8.918829480451946e-05,
      "loss": 0.2283,
      "step": 23440
    },
    {
      "epoch": 1.6627902854103882,
      "grad_norm": 0.8744307160377502,
      "learning_rate": 8.9141020186262e-05,
      "loss": 0.2305,
      "step": 23450
    },
    {
      "epoch": 1.6634993795426343,
      "grad_norm": 0.9311103224754333,
      "learning_rate": 8.909374556800453e-05,
      "loss": 0.2505,
      "step": 23460
    },
    {
      "epoch": 1.6642084736748803,
      "grad_norm": 0.8956455588340759,
      "learning_rate": 8.904647094974708e-05,
      "loss": 0.264,
      "step": 23470
    },
    {
      "epoch": 1.6649175678071264,
      "grad_norm": 1.0453553199768066,
      "learning_rate": 8.899919633148963e-05,
      "loss": 0.2266,
      "step": 23480
    },
    {
      "epoch": 1.6656266619393725,
      "grad_norm": 1.4153417348861694,
      "learning_rate": 8.895192171323217e-05,
      "loss": 0.2254,
      "step": 23490
    },
    {
      "epoch": 1.6663357560716185,
      "grad_norm": 1.1112124919891357,
      "learning_rate": 8.890464709497472e-05,
      "loss": 0.2918,
      "step": 23500
    },
    {
      "epoch": 1.6670448502038646,
      "grad_norm": 0.9669896960258484,
      "learning_rate": 8.885737247671726e-05,
      "loss": 0.2561,
      "step": 23510
    },
    {
      "epoch": 1.6677539443361105,
      "grad_norm": 1.099556565284729,
      "learning_rate": 8.881009785845979e-05,
      "loss": 0.2571,
      "step": 23520
    },
    {
      "epoch": 1.6684630384683565,
      "grad_norm": 1.337214708328247,
      "learning_rate": 8.876282324020234e-05,
      "loss": 0.2832,
      "step": 23530
    },
    {
      "epoch": 1.6691721326006026,
      "grad_norm": 0.879385232925415,
      "learning_rate": 8.871554862194488e-05,
      "loss": 0.2171,
      "step": 23540
    },
    {
      "epoch": 1.6698812267328487,
      "grad_norm": 1.2552427053451538,
      "learning_rate": 8.866827400368743e-05,
      "loss": 0.2708,
      "step": 23550
    },
    {
      "epoch": 1.6705903208650947,
      "grad_norm": 0.8906832337379456,
      "learning_rate": 8.862099938542996e-05,
      "loss": 0.254,
      "step": 23560
    },
    {
      "epoch": 1.6712994149973408,
      "grad_norm": 1.108275055885315,
      "learning_rate": 8.85737247671725e-05,
      "loss": 0.2665,
      "step": 23570
    },
    {
      "epoch": 1.6720085091295869,
      "grad_norm": 1.690117359161377,
      "learning_rate": 8.852645014891505e-05,
      "loss": 0.2612,
      "step": 23580
    },
    {
      "epoch": 1.672717603261833,
      "grad_norm": 0.8328868746757507,
      "learning_rate": 8.84791755306576e-05,
      "loss": 0.2244,
      "step": 23590
    },
    {
      "epoch": 1.673426697394079,
      "grad_norm": 1.2103362083435059,
      "learning_rate": 8.843190091240014e-05,
      "loss": 0.2219,
      "step": 23600
    },
    {
      "epoch": 1.674135791526325,
      "grad_norm": 0.866996705532074,
      "learning_rate": 8.838462629414269e-05,
      "loss": 0.2527,
      "step": 23610
    },
    {
      "epoch": 1.6748448856585711,
      "grad_norm": 1.5115829706192017,
      "learning_rate": 8.833735167588522e-05,
      "loss": 0.2345,
      "step": 23620
    },
    {
      "epoch": 1.6755539797908172,
      "grad_norm": 1.5228252410888672,
      "learning_rate": 8.829007705762776e-05,
      "loss": 0.238,
      "step": 23630
    },
    {
      "epoch": 1.6762630739230633,
      "grad_norm": 1.184782862663269,
      "learning_rate": 8.824280243937031e-05,
      "loss": 0.2277,
      "step": 23640
    },
    {
      "epoch": 1.6769721680553094,
      "grad_norm": 1.0976430177688599,
      "learning_rate": 8.819552782111284e-05,
      "loss": 0.2546,
      "step": 23650
    },
    {
      "epoch": 1.6776812621875554,
      "grad_norm": 0.6872926354408264,
      "learning_rate": 8.81482532028554e-05,
      "loss": 0.216,
      "step": 23660
    },
    {
      "epoch": 1.6783903563198015,
      "grad_norm": 1.2827900648117065,
      "learning_rate": 8.810097858459793e-05,
      "loss": 0.2724,
      "step": 23670
    },
    {
      "epoch": 1.6790994504520476,
      "grad_norm": 1.1864864826202393,
      "learning_rate": 8.805370396634047e-05,
      "loss": 0.238,
      "step": 23680
    },
    {
      "epoch": 1.6798085445842936,
      "grad_norm": 0.6545473337173462,
      "learning_rate": 8.800642934808302e-05,
      "loss": 0.2303,
      "step": 23690
    },
    {
      "epoch": 1.6805176387165397,
      "grad_norm": 1.7568883895874023,
      "learning_rate": 8.795915472982557e-05,
      "loss": 0.2118,
      "step": 23700
    },
    {
      "epoch": 1.6812267328487858,
      "grad_norm": 0.8508402705192566,
      "learning_rate": 8.79118801115681e-05,
      "loss": 0.2477,
      "step": 23710
    },
    {
      "epoch": 1.6819358269810318,
      "grad_norm": 1.1939300298690796,
      "learning_rate": 8.786460549331065e-05,
      "loss": 0.2277,
      "step": 23720
    },
    {
      "epoch": 1.682644921113278,
      "grad_norm": 1.1519149541854858,
      "learning_rate": 8.781733087505319e-05,
      "loss": 0.2461,
      "step": 23730
    },
    {
      "epoch": 1.683354015245524,
      "grad_norm": 1.2911370992660522,
      "learning_rate": 8.777005625679573e-05,
      "loss": 0.2792,
      "step": 23740
    },
    {
      "epoch": 1.68406310937777,
      "grad_norm": 1.6463563442230225,
      "learning_rate": 8.772278163853828e-05,
      "loss": 0.2654,
      "step": 23750
    },
    {
      "epoch": 1.684772203510016,
      "grad_norm": 1.0739847421646118,
      "learning_rate": 8.767550702028081e-05,
      "loss": 0.2205,
      "step": 23760
    },
    {
      "epoch": 1.685481297642262,
      "grad_norm": 1.161621332168579,
      "learning_rate": 8.762823240202336e-05,
      "loss": 0.2713,
      "step": 23770
    },
    {
      "epoch": 1.686190391774508,
      "grad_norm": 1.2017664909362793,
      "learning_rate": 8.758095778376591e-05,
      "loss": 0.2562,
      "step": 23780
    },
    {
      "epoch": 1.686899485906754,
      "grad_norm": 0.9411613941192627,
      "learning_rate": 8.753368316550843e-05,
      "loss": 0.2778,
      "step": 23790
    },
    {
      "epoch": 1.6876085800390002,
      "grad_norm": 1.6475064754486084,
      "learning_rate": 8.748640854725098e-05,
      "loss": 0.2143,
      "step": 23800
    },
    {
      "epoch": 1.6883176741712462,
      "grad_norm": 1.5473321676254272,
      "learning_rate": 8.743913392899353e-05,
      "loss": 0.2034,
      "step": 23810
    },
    {
      "epoch": 1.6890267683034923,
      "grad_norm": 1.1021496057510376,
      "learning_rate": 8.739185931073607e-05,
      "loss": 0.2624,
      "step": 23820
    },
    {
      "epoch": 1.6897358624357384,
      "grad_norm": 1.098989725112915,
      "learning_rate": 8.734458469247862e-05,
      "loss": 0.262,
      "step": 23830
    },
    {
      "epoch": 1.6904449565679844,
      "grad_norm": 1.0854072570800781,
      "learning_rate": 8.729731007422116e-05,
      "loss": 0.2498,
      "step": 23840
    },
    {
      "epoch": 1.6911540507002305,
      "grad_norm": 1.1399953365325928,
      "learning_rate": 8.725003545596369e-05,
      "loss": 0.2339,
      "step": 23850
    },
    {
      "epoch": 1.6918631448324764,
      "grad_norm": 0.8752281665802002,
      "learning_rate": 8.720276083770624e-05,
      "loss": 0.2441,
      "step": 23860
    },
    {
      "epoch": 1.6925722389647224,
      "grad_norm": 0.6485177278518677,
      "learning_rate": 8.715548621944878e-05,
      "loss": 0.2407,
      "step": 23870
    },
    {
      "epoch": 1.6932813330969685,
      "grad_norm": 1.1592094898223877,
      "learning_rate": 8.710821160119133e-05,
      "loss": 0.244,
      "step": 23880
    },
    {
      "epoch": 1.6939904272292146,
      "grad_norm": 1.0498121976852417,
      "learning_rate": 8.706093698293388e-05,
      "loss": 0.2755,
      "step": 23890
    },
    {
      "epoch": 1.6946995213614606,
      "grad_norm": 0.8021297454833984,
      "learning_rate": 8.70136623646764e-05,
      "loss": 0.2309,
      "step": 23900
    },
    {
      "epoch": 1.6954086154937067,
      "grad_norm": 0.8179029822349548,
      "learning_rate": 8.696638774641895e-05,
      "loss": 0.2544,
      "step": 23910
    },
    {
      "epoch": 1.6961177096259528,
      "grad_norm": 0.9191147089004517,
      "learning_rate": 8.691911312816149e-05,
      "loss": 0.2479,
      "step": 23920
    },
    {
      "epoch": 1.6968268037581988,
      "grad_norm": 1.1410512924194336,
      "learning_rate": 8.687183850990404e-05,
      "loss": 0.243,
      "step": 23930
    },
    {
      "epoch": 1.697535897890445,
      "grad_norm": 1.8984408378601074,
      "learning_rate": 8.682456389164659e-05,
      "loss": 0.2373,
      "step": 23940
    },
    {
      "epoch": 1.698244992022691,
      "grad_norm": 1.3576982021331787,
      "learning_rate": 8.677728927338912e-05,
      "loss": 0.2383,
      "step": 23950
    },
    {
      "epoch": 1.698954086154937,
      "grad_norm": 1.1772032976150513,
      "learning_rate": 8.673001465513166e-05,
      "loss": 0.273,
      "step": 23960
    },
    {
      "epoch": 1.699663180287183,
      "grad_norm": 1.2991578578948975,
      "learning_rate": 8.668274003687421e-05,
      "loss": 0.2672,
      "step": 23970
    },
    {
      "epoch": 1.7003722744194292,
      "grad_norm": 0.9239555597305298,
      "learning_rate": 8.663546541861675e-05,
      "loss": 0.2847,
      "step": 23980
    },
    {
      "epoch": 1.7010813685516752,
      "grad_norm": 1.336317777633667,
      "learning_rate": 8.65881908003593e-05,
      "loss": 0.2417,
      "step": 23990
    },
    {
      "epoch": 1.7017904626839213,
      "grad_norm": 1.1479909420013428,
      "learning_rate": 8.654091618210185e-05,
      "loss": 0.2379,
      "step": 24000
    },
    {
      "epoch": 1.7024995568161674,
      "grad_norm": 0.9608703255653381,
      "learning_rate": 8.649364156384437e-05,
      "loss": 0.2589,
      "step": 24010
    },
    {
      "epoch": 1.7032086509484134,
      "grad_norm": 1.1877018213272095,
      "learning_rate": 8.644636694558692e-05,
      "loss": 0.2552,
      "step": 24020
    },
    {
      "epoch": 1.7039177450806595,
      "grad_norm": 0.9893583655357361,
      "learning_rate": 8.639909232732945e-05,
      "loss": 0.2597,
      "step": 24030
    },
    {
      "epoch": 1.7046268392129056,
      "grad_norm": 1.4002954959869385,
      "learning_rate": 8.6351817709072e-05,
      "loss": 0.2477,
      "step": 24040
    },
    {
      "epoch": 1.7053359333451517,
      "grad_norm": 2.273717164993286,
      "learning_rate": 8.630454309081455e-05,
      "loss": 0.247,
      "step": 24050
    },
    {
      "epoch": 1.7060450274773977,
      "grad_norm": 1.8297702074050903,
      "learning_rate": 8.625726847255709e-05,
      "loss": 0.2804,
      "step": 24060
    },
    {
      "epoch": 1.7067541216096438,
      "grad_norm": 1.3116201162338257,
      "learning_rate": 8.620999385429963e-05,
      "loss": 0.2272,
      "step": 24070
    },
    {
      "epoch": 1.7074632157418899,
      "grad_norm": 0.8292077779769897,
      "learning_rate": 8.616271923604218e-05,
      "loss": 0.2066,
      "step": 24080
    },
    {
      "epoch": 1.708172309874136,
      "grad_norm": 3.156198740005493,
      "learning_rate": 8.611544461778471e-05,
      "loss": 0.2482,
      "step": 24090
    },
    {
      "epoch": 1.708881404006382,
      "grad_norm": 0.9226837158203125,
      "learning_rate": 8.606816999952726e-05,
      "loss": 0.2082,
      "step": 24100
    },
    {
      "epoch": 1.709590498138628,
      "grad_norm": 1.592935562133789,
      "learning_rate": 8.60208953812698e-05,
      "loss": 0.2395,
      "step": 24110
    },
    {
      "epoch": 1.710299592270874,
      "grad_norm": 0.9770763516426086,
      "learning_rate": 8.597362076301234e-05,
      "loss": 0.2703,
      "step": 24120
    },
    {
      "epoch": 1.71100868640312,
      "grad_norm": 2.979785919189453,
      "learning_rate": 8.592634614475488e-05,
      "loss": 0.2184,
      "step": 24130
    },
    {
      "epoch": 1.711717780535366,
      "grad_norm": 1.3238552808761597,
      "learning_rate": 8.587907152649742e-05,
      "loss": 0.2766,
      "step": 24140
    },
    {
      "epoch": 1.7124268746676121,
      "grad_norm": 1.638931393623352,
      "learning_rate": 8.583179690823997e-05,
      "loss": 0.2656,
      "step": 24150
    },
    {
      "epoch": 1.7131359687998582,
      "grad_norm": 2.5377399921417236,
      "learning_rate": 8.578452228998252e-05,
      "loss": 0.2284,
      "step": 24160
    },
    {
      "epoch": 1.7138450629321043,
      "grad_norm": 2.846651077270508,
      "learning_rate": 8.573724767172506e-05,
      "loss": 0.2373,
      "step": 24170
    },
    {
      "epoch": 1.7145541570643503,
      "grad_norm": 2.8912441730499268,
      "learning_rate": 8.56899730534676e-05,
      "loss": 0.2866,
      "step": 24180
    },
    {
      "epoch": 1.7152632511965964,
      "grad_norm": 0.9841254949569702,
      "learning_rate": 8.564269843521014e-05,
      "loss": 0.2718,
      "step": 24190
    },
    {
      "epoch": 1.7159723453288422,
      "grad_norm": 0.9920488595962524,
      "learning_rate": 8.559542381695268e-05,
      "loss": 0.2365,
      "step": 24200
    },
    {
      "epoch": 1.7166814394610883,
      "grad_norm": 1.1686127185821533,
      "learning_rate": 8.554814919869523e-05,
      "loss": 0.2486,
      "step": 24210
    },
    {
      "epoch": 1.7173905335933344,
      "grad_norm": 1.2973904609680176,
      "learning_rate": 8.550087458043777e-05,
      "loss": 0.2795,
      "step": 24220
    },
    {
      "epoch": 1.7180996277255804,
      "grad_norm": 1.0115491151809692,
      "learning_rate": 8.54535999621803e-05,
      "loss": 0.2418,
      "step": 24230
    },
    {
      "epoch": 1.7188087218578265,
      "grad_norm": 1.1839543581008911,
      "learning_rate": 8.540632534392285e-05,
      "loss": 0.2753,
      "step": 24240
    },
    {
      "epoch": 1.7195178159900726,
      "grad_norm": 1.2514739036560059,
      "learning_rate": 8.535905072566539e-05,
      "loss": 0.2508,
      "step": 24250
    },
    {
      "epoch": 1.7202269101223187,
      "grad_norm": 0.9659700393676758,
      "learning_rate": 8.531177610740794e-05,
      "loss": 0.2934,
      "step": 24260
    },
    {
      "epoch": 1.7209360042545647,
      "grad_norm": 0.9979894757270813,
      "learning_rate": 8.526450148915049e-05,
      "loss": 0.2568,
      "step": 24270
    },
    {
      "epoch": 1.7216450983868108,
      "grad_norm": 0.9085845351219177,
      "learning_rate": 8.521722687089302e-05,
      "loss": 0.2287,
      "step": 24280
    },
    {
      "epoch": 1.7223541925190569,
      "grad_norm": 0.935398280620575,
      "learning_rate": 8.516995225263556e-05,
      "loss": 0.2371,
      "step": 24290
    },
    {
      "epoch": 1.723063286651303,
      "grad_norm": 1.3305463790893555,
      "learning_rate": 8.512267763437811e-05,
      "loss": 0.2396,
      "step": 24300
    },
    {
      "epoch": 1.723772380783549,
      "grad_norm": 0.970077633857727,
      "learning_rate": 8.507540301612065e-05,
      "loss": 0.2656,
      "step": 24310
    },
    {
      "epoch": 1.724481474915795,
      "grad_norm": 0.9843752384185791,
      "learning_rate": 8.50281283978632e-05,
      "loss": 0.2411,
      "step": 24320
    },
    {
      "epoch": 1.7251905690480411,
      "grad_norm": 0.9280750751495361,
      "learning_rate": 8.498085377960573e-05,
      "loss": 0.2403,
      "step": 24330
    },
    {
      "epoch": 1.7258996631802872,
      "grad_norm": 0.7846103310585022,
      "learning_rate": 8.493357916134827e-05,
      "loss": 0.2455,
      "step": 24340
    },
    {
      "epoch": 1.7266087573125333,
      "grad_norm": 1.131240725517273,
      "learning_rate": 8.488630454309082e-05,
      "loss": 0.2338,
      "step": 24350
    },
    {
      "epoch": 1.7273178514447793,
      "grad_norm": 1.6718472242355347,
      "learning_rate": 8.483902992483336e-05,
      "loss": 0.2365,
      "step": 24360
    },
    {
      "epoch": 1.7280269455770254,
      "grad_norm": 0.6929349899291992,
      "learning_rate": 8.47917553065759e-05,
      "loss": 0.2383,
      "step": 24370
    },
    {
      "epoch": 1.7287360397092715,
      "grad_norm": 1.709686040878296,
      "learning_rate": 8.474448068831845e-05,
      "loss": 0.2453,
      "step": 24380
    },
    {
      "epoch": 1.7294451338415175,
      "grad_norm": 2.038444757461548,
      "learning_rate": 8.469720607006099e-05,
      "loss": 0.2599,
      "step": 24390
    },
    {
      "epoch": 1.7301542279737636,
      "grad_norm": 2.4175069332122803,
      "learning_rate": 8.464993145180353e-05,
      "loss": 0.2697,
      "step": 24400
    },
    {
      "epoch": 1.7308633221060097,
      "grad_norm": 0.8360361456871033,
      "learning_rate": 8.460265683354606e-05,
      "loss": 0.2341,
      "step": 24410
    },
    {
      "epoch": 1.7315724162382558,
      "grad_norm": 1.6315406560897827,
      "learning_rate": 8.455538221528861e-05,
      "loss": 0.2586,
      "step": 24420
    },
    {
      "epoch": 1.7322815103705018,
      "grad_norm": 0.8004547357559204,
      "learning_rate": 8.450810759703116e-05,
      "loss": 0.2583,
      "step": 24430
    },
    {
      "epoch": 1.7329906045027479,
      "grad_norm": 1.5500448942184448,
      "learning_rate": 8.44608329787737e-05,
      "loss": 0.2649,
      "step": 24440
    },
    {
      "epoch": 1.733699698634994,
      "grad_norm": 1.118150234222412,
      "learning_rate": 8.441355836051625e-05,
      "loss": 0.216,
      "step": 24450
    },
    {
      "epoch": 1.7344087927672398,
      "grad_norm": 3.060272455215454,
      "learning_rate": 8.436628374225879e-05,
      "loss": 0.2241,
      "step": 24460
    },
    {
      "epoch": 1.7351178868994859,
      "grad_norm": 0.9320666193962097,
      "learning_rate": 8.431900912400132e-05,
      "loss": 0.2091,
      "step": 24470
    },
    {
      "epoch": 1.735826981031732,
      "grad_norm": 0.9729182720184326,
      "learning_rate": 8.427173450574387e-05,
      "loss": 0.2649,
      "step": 24480
    },
    {
      "epoch": 1.736536075163978,
      "grad_norm": 0.789009690284729,
      "learning_rate": 8.422445988748642e-05,
      "loss": 0.2664,
      "step": 24490
    },
    {
      "epoch": 1.737245169296224,
      "grad_norm": 1.2991441488265991,
      "learning_rate": 8.417718526922896e-05,
      "loss": 0.2827,
      "step": 24500
    },
    {
      "epoch": 1.7379542634284701,
      "grad_norm": 0.7497223615646362,
      "learning_rate": 8.41299106509715e-05,
      "loss": 0.2161,
      "step": 24510
    },
    {
      "epoch": 1.7386633575607162,
      "grad_norm": 0.8306403756141663,
      "learning_rate": 8.408263603271403e-05,
      "loss": 0.2326,
      "step": 24520
    },
    {
      "epoch": 1.7393724516929623,
      "grad_norm": 1.0905159711837769,
      "learning_rate": 8.403536141445658e-05,
      "loss": 0.2713,
      "step": 24530
    },
    {
      "epoch": 1.7400815458252081,
      "grad_norm": 0.8198632001876831,
      "learning_rate": 8.398808679619913e-05,
      "loss": 0.2278,
      "step": 24540
    },
    {
      "epoch": 1.7407906399574542,
      "grad_norm": 0.5521687865257263,
      "learning_rate": 8.394081217794167e-05,
      "loss": 0.2264,
      "step": 24550
    },
    {
      "epoch": 1.7414997340897003,
      "grad_norm": 1.4547120332717896,
      "learning_rate": 8.389353755968422e-05,
      "loss": 0.2334,
      "step": 24560
    },
    {
      "epoch": 1.7422088282219463,
      "grad_norm": 1.2868971824645996,
      "learning_rate": 8.384626294142675e-05,
      "loss": 0.2453,
      "step": 24570
    },
    {
      "epoch": 1.7429179223541924,
      "grad_norm": 1.3669415712356567,
      "learning_rate": 8.379898832316929e-05,
      "loss": 0.2349,
      "step": 24580
    },
    {
      "epoch": 1.7436270164864385,
      "grad_norm": 0.8338325023651123,
      "learning_rate": 8.375171370491184e-05,
      "loss": 0.2019,
      "step": 24590
    },
    {
      "epoch": 1.7443361106186845,
      "grad_norm": 2.2290773391723633,
      "learning_rate": 8.370443908665438e-05,
      "loss": 0.2505,
      "step": 24600
    },
    {
      "epoch": 1.7450452047509306,
      "grad_norm": 1.449344277381897,
      "learning_rate": 8.365716446839693e-05,
      "loss": 0.257,
      "step": 24610
    },
    {
      "epoch": 1.7457542988831767,
      "grad_norm": 1.5215198993682861,
      "learning_rate": 8.360988985013946e-05,
      "loss": 0.2441,
      "step": 24620
    },
    {
      "epoch": 1.7464633930154227,
      "grad_norm": 0.9070226550102234,
      "learning_rate": 8.3562615231882e-05,
      "loss": 0.2383,
      "step": 24630
    },
    {
      "epoch": 1.7471724871476688,
      "grad_norm": 1.6268912553787231,
      "learning_rate": 8.351534061362455e-05,
      "loss": 0.2237,
      "step": 24640
    },
    {
      "epoch": 1.7478815812799149,
      "grad_norm": 0.7089674472808838,
      "learning_rate": 8.34680659953671e-05,
      "loss": 0.2432,
      "step": 24650
    },
    {
      "epoch": 1.748590675412161,
      "grad_norm": 1.7202908992767334,
      "learning_rate": 8.342079137710963e-05,
      "loss": 0.2219,
      "step": 24660
    },
    {
      "epoch": 1.749299769544407,
      "grad_norm": 0.9216797351837158,
      "learning_rate": 8.337351675885218e-05,
      "loss": 0.2664,
      "step": 24670
    },
    {
      "epoch": 1.750008863676653,
      "grad_norm": 1.1056742668151855,
      "learning_rate": 8.332624214059472e-05,
      "loss": 0.2455,
      "step": 24680
    },
    {
      "epoch": 1.7507179578088992,
      "grad_norm": 0.9817673563957214,
      "learning_rate": 8.327896752233726e-05,
      "loss": 0.2628,
      "step": 24690
    },
    {
      "epoch": 1.7514270519411452,
      "grad_norm": 1.305577278137207,
      "learning_rate": 8.32316929040798e-05,
      "loss": 0.233,
      "step": 24700
    },
    {
      "epoch": 1.7521361460733913,
      "grad_norm": 0.7441991567611694,
      "learning_rate": 8.318441828582234e-05,
      "loss": 0.2278,
      "step": 24710
    },
    {
      "epoch": 1.7528452402056374,
      "grad_norm": 1.1278518438339233,
      "learning_rate": 8.313714366756489e-05,
      "loss": 0.2398,
      "step": 24720
    },
    {
      "epoch": 1.7535543343378834,
      "grad_norm": 0.9830392599105835,
      "learning_rate": 8.308986904930743e-05,
      "loss": 0.213,
      "step": 24730
    },
    {
      "epoch": 1.7542634284701295,
      "grad_norm": 3.5694618225097656,
      "learning_rate": 8.304259443104996e-05,
      "loss": 0.2404,
      "step": 24740
    },
    {
      "epoch": 1.7549725226023756,
      "grad_norm": 1.6214230060577393,
      "learning_rate": 8.299531981279251e-05,
      "loss": 0.2225,
      "step": 24750
    },
    {
      "epoch": 1.7556816167346216,
      "grad_norm": 0.8654144406318665,
      "learning_rate": 8.294804519453506e-05,
      "loss": 0.2611,
      "step": 24760
    },
    {
      "epoch": 1.7563907108668677,
      "grad_norm": 1.0238429307937622,
      "learning_rate": 8.29007705762776e-05,
      "loss": 0.2401,
      "step": 24770
    },
    {
      "epoch": 1.7570998049991138,
      "grad_norm": 5.866713047027588,
      "learning_rate": 8.285349595802015e-05,
      "loss": 0.2402,
      "step": 24780
    },
    {
      "epoch": 1.7578088991313598,
      "grad_norm": 0.8778892159461975,
      "learning_rate": 8.280622133976269e-05,
      "loss": 0.2656,
      "step": 24790
    },
    {
      "epoch": 1.7585179932636057,
      "grad_norm": 1.0384858846664429,
      "learning_rate": 8.275894672150522e-05,
      "loss": 0.2677,
      "step": 24800
    },
    {
      "epoch": 1.7592270873958518,
      "grad_norm": 1.2754876613616943,
      "learning_rate": 8.271167210324777e-05,
      "loss": 0.2525,
      "step": 24810
    },
    {
      "epoch": 1.7599361815280978,
      "grad_norm": 0.9581765532493591,
      "learning_rate": 8.266439748499031e-05,
      "loss": 0.2393,
      "step": 24820
    },
    {
      "epoch": 1.760645275660344,
      "grad_norm": 0.7652768492698669,
      "learning_rate": 8.261712286673286e-05,
      "loss": 0.2202,
      "step": 24830
    },
    {
      "epoch": 1.76135436979259,
      "grad_norm": 1.4276561737060547,
      "learning_rate": 8.25698482484754e-05,
      "loss": 0.2383,
      "step": 24840
    },
    {
      "epoch": 1.762063463924836,
      "grad_norm": 1.594107747077942,
      "learning_rate": 8.252257363021793e-05,
      "loss": 0.2742,
      "step": 24850
    },
    {
      "epoch": 1.762772558057082,
      "grad_norm": 0.8077985644340515,
      "learning_rate": 8.247529901196048e-05,
      "loss": 0.2286,
      "step": 24860
    },
    {
      "epoch": 1.7634816521893282,
      "grad_norm": 1.3633819818496704,
      "learning_rate": 8.242802439370303e-05,
      "loss": 0.279,
      "step": 24870
    },
    {
      "epoch": 1.7641907463215742,
      "grad_norm": 0.779988169670105,
      "learning_rate": 8.238074977544557e-05,
      "loss": 0.2402,
      "step": 24880
    },
    {
      "epoch": 1.76489984045382,
      "grad_norm": 1.9290601015090942,
      "learning_rate": 8.233347515718812e-05,
      "loss": 0.2656,
      "step": 24890
    },
    {
      "epoch": 1.7656089345860662,
      "grad_norm": 1.1470664739608765,
      "learning_rate": 8.228620053893064e-05,
      "loss": 0.2375,
      "step": 24900
    },
    {
      "epoch": 1.7663180287183122,
      "grad_norm": 0.8261861801147461,
      "learning_rate": 8.223892592067319e-05,
      "loss": 0.2281,
      "step": 24910
    },
    {
      "epoch": 1.7670271228505583,
      "grad_norm": 0.8999007344245911,
      "learning_rate": 8.219165130241574e-05,
      "loss": 0.2799,
      "step": 24920
    },
    {
      "epoch": 1.7677362169828044,
      "grad_norm": 0.8761836886405945,
      "learning_rate": 8.214437668415828e-05,
      "loss": 0.2385,
      "step": 24930
    },
    {
      "epoch": 1.7684453111150504,
      "grad_norm": 1.1133663654327393,
      "learning_rate": 8.209710206590083e-05,
      "loss": 0.2437,
      "step": 24940
    },
    {
      "epoch": 1.7691544052472965,
      "grad_norm": 1.998940348625183,
      "learning_rate": 8.204982744764336e-05,
      "loss": 0.2563,
      "step": 24950
    },
    {
      "epoch": 1.7698634993795426,
      "grad_norm": 0.8708418011665344,
      "learning_rate": 8.20025528293859e-05,
      "loss": 0.2612,
      "step": 24960
    },
    {
      "epoch": 1.7705725935117886,
      "grad_norm": 1.5149486064910889,
      "learning_rate": 8.195527821112845e-05,
      "loss": 0.2412,
      "step": 24970
    },
    {
      "epoch": 1.7712816876440347,
      "grad_norm": 1.5820953845977783,
      "learning_rate": 8.1908003592871e-05,
      "loss": 0.2709,
      "step": 24980
    },
    {
      "epoch": 1.7719907817762808,
      "grad_norm": 1.1514325141906738,
      "learning_rate": 8.186072897461353e-05,
      "loss": 0.2174,
      "step": 24990
    },
    {
      "epoch": 1.7726998759085268,
      "grad_norm": 0.7283450961112976,
      "learning_rate": 8.181345435635608e-05,
      "loss": 0.2482,
      "step": 25000
    },
    {
      "epoch": 1.773408970040773,
      "grad_norm": 1.120449423789978,
      "learning_rate": 8.176617973809861e-05,
      "loss": 0.2605,
      "step": 25010
    },
    {
      "epoch": 1.774118064173019,
      "grad_norm": 1.1648967266082764,
      "learning_rate": 8.171890511984116e-05,
      "loss": 0.2253,
      "step": 25020
    },
    {
      "epoch": 1.774827158305265,
      "grad_norm": 1.2135721445083618,
      "learning_rate": 8.167163050158371e-05,
      "loss": 0.2428,
      "step": 25030
    },
    {
      "epoch": 1.7755362524375111,
      "grad_norm": 0.7359926104545593,
      "learning_rate": 8.162435588332624e-05,
      "loss": 0.2071,
      "step": 25040
    },
    {
      "epoch": 1.7762453465697572,
      "grad_norm": 0.7824536561965942,
      "learning_rate": 8.157708126506879e-05,
      "loss": 0.2124,
      "step": 25050
    },
    {
      "epoch": 1.7769544407020033,
      "grad_norm": 1.1173521280288696,
      "learning_rate": 8.152980664681133e-05,
      "loss": 0.2532,
      "step": 25060
    },
    {
      "epoch": 1.7776635348342493,
      "grad_norm": 1.3091208934783936,
      "learning_rate": 8.148253202855387e-05,
      "loss": 0.244,
      "step": 25070
    },
    {
      "epoch": 1.7783726289664954,
      "grad_norm": 0.8952734470367432,
      "learning_rate": 8.143525741029642e-05,
      "loss": 0.2617,
      "step": 25080
    },
    {
      "epoch": 1.7790817230987415,
      "grad_norm": 0.8440861701965332,
      "learning_rate": 8.138798279203897e-05,
      "loss": 0.2167,
      "step": 25090
    },
    {
      "epoch": 1.7797908172309875,
      "grad_norm": 1.4461839199066162,
      "learning_rate": 8.13407081737815e-05,
      "loss": 0.2582,
      "step": 25100
    },
    {
      "epoch": 1.7804999113632336,
      "grad_norm": 1.0977692604064941,
      "learning_rate": 8.129343355552405e-05,
      "loss": 0.2171,
      "step": 25110
    },
    {
      "epoch": 1.7812090054954797,
      "grad_norm": 1.2968412637710571,
      "learning_rate": 8.124615893726659e-05,
      "loss": 0.2484,
      "step": 25120
    },
    {
      "epoch": 1.7819180996277257,
      "grad_norm": 1.8271591663360596,
      "learning_rate": 8.119888431900912e-05,
      "loss": 0.2479,
      "step": 25130
    },
    {
      "epoch": 1.7826271937599716,
      "grad_norm": 0.8611829876899719,
      "learning_rate": 8.115160970075167e-05,
      "loss": 0.2719,
      "step": 25140
    },
    {
      "epoch": 1.7833362878922177,
      "grad_norm": 0.843036413192749,
      "learning_rate": 8.110433508249421e-05,
      "loss": 0.2209,
      "step": 25150
    },
    {
      "epoch": 1.7840453820244637,
      "grad_norm": 1.4597138166427612,
      "learning_rate": 8.105706046423676e-05,
      "loss": 0.257,
      "step": 25160
    },
    {
      "epoch": 1.7847544761567098,
      "grad_norm": 1.3039454221725464,
      "learning_rate": 8.10097858459793e-05,
      "loss": 0.2303,
      "step": 25170
    },
    {
      "epoch": 1.7854635702889559,
      "grad_norm": 0.9709658026695251,
      "learning_rate": 8.096251122772183e-05,
      "loss": 0.2368,
      "step": 25180
    },
    {
      "epoch": 1.786172664421202,
      "grad_norm": 0.7710831165313721,
      "learning_rate": 8.091523660946438e-05,
      "loss": 0.2646,
      "step": 25190
    },
    {
      "epoch": 1.786881758553448,
      "grad_norm": 1.5401952266693115,
      "learning_rate": 8.086796199120692e-05,
      "loss": 0.2372,
      "step": 25200
    },
    {
      "epoch": 1.787590852685694,
      "grad_norm": 1.03195059299469,
      "learning_rate": 8.082068737294947e-05,
      "loss": 0.2505,
      "step": 25210
    },
    {
      "epoch": 1.7882999468179401,
      "grad_norm": 1.4035911560058594,
      "learning_rate": 8.077341275469202e-05,
      "loss": 0.2499,
      "step": 25220
    },
    {
      "epoch": 1.789009040950186,
      "grad_norm": 1.434899091720581,
      "learning_rate": 8.072613813643455e-05,
      "loss": 0.2738,
      "step": 25230
    },
    {
      "epoch": 1.789718135082432,
      "grad_norm": 1.3973802328109741,
      "learning_rate": 8.067886351817709e-05,
      "loss": 0.2751,
      "step": 25240
    },
    {
      "epoch": 1.7904272292146781,
      "grad_norm": 1.2120778560638428,
      "learning_rate": 8.063158889991964e-05,
      "loss": 0.2914,
      "step": 25250
    },
    {
      "epoch": 1.7911363233469242,
      "grad_norm": 0.8952540159225464,
      "learning_rate": 8.058431428166218e-05,
      "loss": 0.2394,
      "step": 25260
    },
    {
      "epoch": 1.7918454174791703,
      "grad_norm": 0.9784893989562988,
      "learning_rate": 8.053703966340473e-05,
      "loss": 0.2689,
      "step": 25270
    },
    {
      "epoch": 1.7925545116114163,
      "grad_norm": 1.2933114767074585,
      "learning_rate": 8.048976504514726e-05,
      "loss": 0.2385,
      "step": 25280
    },
    {
      "epoch": 1.7932636057436624,
      "grad_norm": 0.9092695116996765,
      "learning_rate": 8.04424904268898e-05,
      "loss": 0.2676,
      "step": 25290
    },
    {
      "epoch": 1.7939726998759085,
      "grad_norm": 1.1321380138397217,
      "learning_rate": 8.039521580863235e-05,
      "loss": 0.2155,
      "step": 25300
    },
    {
      "epoch": 1.7946817940081545,
      "grad_norm": 1.8270435333251953,
      "learning_rate": 8.034794119037489e-05,
      "loss": 0.2054,
      "step": 25310
    },
    {
      "epoch": 1.7953908881404006,
      "grad_norm": 1.4262770414352417,
      "learning_rate": 8.030066657211744e-05,
      "loss": 0.231,
      "step": 25320
    },
    {
      "epoch": 1.7960999822726467,
      "grad_norm": 1.375916600227356,
      "learning_rate": 8.025339195385999e-05,
      "loss": 0.2466,
      "step": 25330
    },
    {
      "epoch": 1.7968090764048927,
      "grad_norm": 1.1493451595306396,
      "learning_rate": 8.020611733560252e-05,
      "loss": 0.2254,
      "step": 25340
    },
    {
      "epoch": 1.7975181705371388,
      "grad_norm": 0.9427198171615601,
      "learning_rate": 8.015884271734506e-05,
      "loss": 0.2385,
      "step": 25350
    },
    {
      "epoch": 1.7982272646693849,
      "grad_norm": 1.352985143661499,
      "learning_rate": 8.011156809908761e-05,
      "loss": 0.2604,
      "step": 25360
    },
    {
      "epoch": 1.798936358801631,
      "grad_norm": 2.5925941467285156,
      "learning_rate": 8.006429348083014e-05,
      "loss": 0.2252,
      "step": 25370
    },
    {
      "epoch": 1.799645452933877,
      "grad_norm": 0.8458178639411926,
      "learning_rate": 8.00170188625727e-05,
      "loss": 0.2446,
      "step": 25380
    },
    {
      "epoch": 1.800354547066123,
      "grad_norm": 2.4208755493164062,
      "learning_rate": 7.996974424431523e-05,
      "loss": 0.2672,
      "step": 25390
    },
    {
      "epoch": 1.8010636411983691,
      "grad_norm": 1.4027678966522217,
      "learning_rate": 7.992246962605777e-05,
      "loss": 0.2719,
      "step": 25400
    },
    {
      "epoch": 1.8017727353306152,
      "grad_norm": 1.4373971223831177,
      "learning_rate": 7.987519500780032e-05,
      "loss": 0.2438,
      "step": 25410
    },
    {
      "epoch": 1.8024818294628613,
      "grad_norm": 1.404661774635315,
      "learning_rate": 7.982792038954285e-05,
      "loss": 0.2379,
      "step": 25420
    },
    {
      "epoch": 1.8031909235951074,
      "grad_norm": 2.139157295227051,
      "learning_rate": 7.97806457712854e-05,
      "loss": 0.2456,
      "step": 25430
    },
    {
      "epoch": 1.8039000177273534,
      "grad_norm": 1.4071881771087646,
      "learning_rate": 7.973337115302795e-05,
      "loss": 0.2327,
      "step": 25440
    },
    {
      "epoch": 1.8046091118595995,
      "grad_norm": 0.654809832572937,
      "learning_rate": 7.968609653477049e-05,
      "loss": 0.2181,
      "step": 25450
    },
    {
      "epoch": 1.8053182059918456,
      "grad_norm": 1.0958346128463745,
      "learning_rate": 7.963882191651303e-05,
      "loss": 0.2512,
      "step": 25460
    },
    {
      "epoch": 1.8060273001240916,
      "grad_norm": 0.8456234931945801,
      "learning_rate": 7.959154729825557e-05,
      "loss": 0.2236,
      "step": 25470
    },
    {
      "epoch": 1.8067363942563375,
      "grad_norm": 0.6479316353797913,
      "learning_rate": 7.954427267999811e-05,
      "loss": 0.2053,
      "step": 25480
    },
    {
      "epoch": 1.8074454883885835,
      "grad_norm": 0.9555249214172363,
      "learning_rate": 7.949699806174066e-05,
      "loss": 0.2397,
      "step": 25490
    },
    {
      "epoch": 1.8081545825208296,
      "grad_norm": 1.057175636291504,
      "learning_rate": 7.94497234434832e-05,
      "loss": 0.2547,
      "step": 25500
    },
    {
      "epoch": 1.8088636766530757,
      "grad_norm": 1.21394681930542,
      "learning_rate": 7.940244882522573e-05,
      "loss": 0.241,
      "step": 25510
    },
    {
      "epoch": 1.8095727707853217,
      "grad_norm": 1.6609152555465698,
      "learning_rate": 7.935517420696828e-05,
      "loss": 0.2298,
      "step": 25520
    },
    {
      "epoch": 1.8102818649175678,
      "grad_norm": 1.2569847106933594,
      "learning_rate": 7.930789958871082e-05,
      "loss": 0.2092,
      "step": 25530
    },
    {
      "epoch": 1.8109909590498139,
      "grad_norm": 3.502858877182007,
      "learning_rate": 7.926062497045337e-05,
      "loss": 0.2563,
      "step": 25540
    },
    {
      "epoch": 1.81170005318206,
      "grad_norm": 2.2395412921905518,
      "learning_rate": 7.921335035219592e-05,
      "loss": 0.2294,
      "step": 25550
    },
    {
      "epoch": 1.812409147314306,
      "grad_norm": 0.9320001602172852,
      "learning_rate": 7.916607573393846e-05,
      "loss": 0.2881,
      "step": 25560
    },
    {
      "epoch": 1.8131182414465519,
      "grad_norm": 1.9683705568313599,
      "learning_rate": 7.911880111568099e-05,
      "loss": 0.2579,
      "step": 25570
    },
    {
      "epoch": 1.813827335578798,
      "grad_norm": 0.8722184300422668,
      "learning_rate": 7.907152649742354e-05,
      "loss": 0.228,
      "step": 25580
    },
    {
      "epoch": 1.814536429711044,
      "grad_norm": 1.0980627536773682,
      "learning_rate": 7.902425187916608e-05,
      "loss": 0.2305,
      "step": 25590
    },
    {
      "epoch": 1.81524552384329,
      "grad_norm": 0.943587601184845,
      "learning_rate": 7.897697726090863e-05,
      "loss": 0.2403,
      "step": 25600
    },
    {
      "epoch": 1.8159546179755361,
      "grad_norm": 0.7078378200531006,
      "learning_rate": 7.892970264265116e-05,
      "loss": 0.2097,
      "step": 25610
    },
    {
      "epoch": 1.8166637121077822,
      "grad_norm": 1.6145522594451904,
      "learning_rate": 7.88824280243937e-05,
      "loss": 0.238,
      "step": 25620
    },
    {
      "epoch": 1.8173728062400283,
      "grad_norm": 0.9334238171577454,
      "learning_rate": 7.883515340613625e-05,
      "loss": 0.2885,
      "step": 25630
    },
    {
      "epoch": 1.8180819003722744,
      "grad_norm": 0.8747162222862244,
      "learning_rate": 7.878787878787879e-05,
      "loss": 0.2355,
      "step": 25640
    },
    {
      "epoch": 1.8187909945045204,
      "grad_norm": 0.9864741563796997,
      "learning_rate": 7.874060416962134e-05,
      "loss": 0.2311,
      "step": 25650
    },
    {
      "epoch": 1.8195000886367665,
      "grad_norm": 0.6906574964523315,
      "learning_rate": 7.869332955136389e-05,
      "loss": 0.2294,
      "step": 25660
    },
    {
      "epoch": 1.8202091827690126,
      "grad_norm": 1.4144724607467651,
      "learning_rate": 7.864605493310642e-05,
      "loss": 0.2316,
      "step": 25670
    },
    {
      "epoch": 1.8209182769012586,
      "grad_norm": 0.9337274432182312,
      "learning_rate": 7.859878031484896e-05,
      "loss": 0.2456,
      "step": 25680
    },
    {
      "epoch": 1.8216273710335047,
      "grad_norm": 0.812328577041626,
      "learning_rate": 7.85515056965915e-05,
      "loss": 0.2529,
      "step": 25690
    },
    {
      "epoch": 1.8223364651657508,
      "grad_norm": 1.712388038635254,
      "learning_rate": 7.850423107833405e-05,
      "loss": 0.2546,
      "step": 25700
    },
    {
      "epoch": 1.8230455592979968,
      "grad_norm": 1.8307634592056274,
      "learning_rate": 7.84569564600766e-05,
      "loss": 0.2574,
      "step": 25710
    },
    {
      "epoch": 1.823754653430243,
      "grad_norm": 1.8043670654296875,
      "learning_rate": 7.840968184181913e-05,
      "loss": 0.2392,
      "step": 25720
    },
    {
      "epoch": 1.824463747562489,
      "grad_norm": 1.1233357191085815,
      "learning_rate": 7.836240722356167e-05,
      "loss": 0.2451,
      "step": 25730
    },
    {
      "epoch": 1.825172841694735,
      "grad_norm": 0.9332435727119446,
      "learning_rate": 7.831513260530422e-05,
      "loss": 0.2439,
      "step": 25740
    },
    {
      "epoch": 1.825881935826981,
      "grad_norm": 0.7886866331100464,
      "learning_rate": 7.826785798704675e-05,
      "loss": 0.2364,
      "step": 25750
    },
    {
      "epoch": 1.8265910299592272,
      "grad_norm": 1.8301044702529907,
      "learning_rate": 7.82205833687893e-05,
      "loss": 0.2488,
      "step": 25760
    },
    {
      "epoch": 1.8273001240914732,
      "grad_norm": 1.1727492809295654,
      "learning_rate": 7.817330875053185e-05,
      "loss": 0.2309,
      "step": 25770
    },
    {
      "epoch": 1.8280092182237193,
      "grad_norm": 1.1412930488586426,
      "learning_rate": 7.812603413227439e-05,
      "loss": 0.2395,
      "step": 25780
    },
    {
      "epoch": 1.8287183123559654,
      "grad_norm": 1.8842724561691284,
      "learning_rate": 7.807875951401693e-05,
      "loss": 0.2469,
      "step": 25790
    },
    {
      "epoch": 1.8294274064882114,
      "grad_norm": 1.1735707521438599,
      "learning_rate": 7.803148489575946e-05,
      "loss": 0.2644,
      "step": 25800
    },
    {
      "epoch": 1.8301365006204575,
      "grad_norm": 1.1960092782974243,
      "learning_rate": 7.798421027750201e-05,
      "loss": 0.2333,
      "step": 25810
    },
    {
      "epoch": 1.8308455947527036,
      "grad_norm": 1.1800005435943604,
      "learning_rate": 7.793693565924456e-05,
      "loss": 0.2494,
      "step": 25820
    },
    {
      "epoch": 1.8315546888849494,
      "grad_norm": 0.6048620343208313,
      "learning_rate": 7.78896610409871e-05,
      "loss": 0.2223,
      "step": 25830
    },
    {
      "epoch": 1.8322637830171955,
      "grad_norm": 0.903657078742981,
      "learning_rate": 7.784238642272963e-05,
      "loss": 0.229,
      "step": 25840
    },
    {
      "epoch": 1.8329728771494416,
      "grad_norm": 1.270757794380188,
      "learning_rate": 7.779511180447218e-05,
      "loss": 0.2338,
      "step": 25850
    },
    {
      "epoch": 1.8336819712816876,
      "grad_norm": 1.2788772583007812,
      "learning_rate": 7.774783718621472e-05,
      "loss": 0.2527,
      "step": 25860
    },
    {
      "epoch": 1.8343910654139337,
      "grad_norm": 1.415736436843872,
      "learning_rate": 7.770056256795727e-05,
      "loss": 0.2526,
      "step": 25870
    },
    {
      "epoch": 1.8351001595461798,
      "grad_norm": 2.3390793800354004,
      "learning_rate": 7.765328794969981e-05,
      "loss": 0.2342,
      "step": 25880
    },
    {
      "epoch": 1.8358092536784258,
      "grad_norm": 0.7742133736610413,
      "learning_rate": 7.760601333144236e-05,
      "loss": 0.2513,
      "step": 25890
    },
    {
      "epoch": 1.836518347810672,
      "grad_norm": 1.3326733112335205,
      "learning_rate": 7.755873871318489e-05,
      "loss": 0.2662,
      "step": 25900
    },
    {
      "epoch": 1.8372274419429178,
      "grad_norm": 1.1836111545562744,
      "learning_rate": 7.751146409492743e-05,
      "loss": 0.2861,
      "step": 25910
    },
    {
      "epoch": 1.8379365360751638,
      "grad_norm": 1.7356799840927124,
      "learning_rate": 7.746418947666998e-05,
      "loss": 0.2243,
      "step": 25920
    },
    {
      "epoch": 1.83864563020741,
      "grad_norm": 1.7958697080612183,
      "learning_rate": 7.741691485841253e-05,
      "loss": 0.2714,
      "step": 25930
    },
    {
      "epoch": 1.839354724339656,
      "grad_norm": 1.9412933588027954,
      "learning_rate": 7.736964024015507e-05,
      "loss": 0.2651,
      "step": 25940
    },
    {
      "epoch": 1.840063818471902,
      "grad_norm": 1.23551344871521,
      "learning_rate": 7.73223656218976e-05,
      "loss": 0.2735,
      "step": 25950
    },
    {
      "epoch": 1.840772912604148,
      "grad_norm": 1.038262128829956,
      "learning_rate": 7.727509100364015e-05,
      "loss": 0.2685,
      "step": 25960
    },
    {
      "epoch": 1.8414820067363942,
      "grad_norm": 1.4146093130111694,
      "learning_rate": 7.722781638538269e-05,
      "loss": 0.2433,
      "step": 25970
    },
    {
      "epoch": 1.8421911008686402,
      "grad_norm": 1.2671316862106323,
      "learning_rate": 7.718054176712524e-05,
      "loss": 0.2889,
      "step": 25980
    },
    {
      "epoch": 1.8429001950008863,
      "grad_norm": 1.003614902496338,
      "learning_rate": 7.713326714886777e-05,
      "loss": 0.2311,
      "step": 25990
    },
    {
      "epoch": 1.8436092891331324,
      "grad_norm": 0.7654542326927185,
      "learning_rate": 7.708599253061032e-05,
      "loss": 0.26,
      "step": 26000
    },
    {
      "epoch": 1.8443183832653784,
      "grad_norm": 1.3378385305404663,
      "learning_rate": 7.703871791235286e-05,
      "loss": 0.216,
      "step": 26010
    },
    {
      "epoch": 1.8450274773976245,
      "grad_norm": 1.2140090465545654,
      "learning_rate": 7.69914432940954e-05,
      "loss": 0.2582,
      "step": 26020
    },
    {
      "epoch": 1.8457365715298706,
      "grad_norm": 1.7646100521087646,
      "learning_rate": 7.694416867583795e-05,
      "loss": 0.2409,
      "step": 26030
    },
    {
      "epoch": 1.8464456656621167,
      "grad_norm": 0.8752478361129761,
      "learning_rate": 7.68968940575805e-05,
      "loss": 0.2047,
      "step": 26040
    },
    {
      "epoch": 1.8471547597943627,
      "grad_norm": 1.0859805345535278,
      "learning_rate": 7.684961943932303e-05,
      "loss": 0.215,
      "step": 26050
    },
    {
      "epoch": 1.8478638539266088,
      "grad_norm": 1.016711711883545,
      "learning_rate": 7.680234482106557e-05,
      "loss": 0.2175,
      "step": 26060
    },
    {
      "epoch": 1.8485729480588549,
      "grad_norm": 1.3207181692123413,
      "learning_rate": 7.675507020280812e-05,
      "loss": 0.2559,
      "step": 26070
    },
    {
      "epoch": 1.849282042191101,
      "grad_norm": 1.7624160051345825,
      "learning_rate": 7.670779558455065e-05,
      "loss": 0.2607,
      "step": 26080
    },
    {
      "epoch": 1.849991136323347,
      "grad_norm": 1.0293824672698975,
      "learning_rate": 7.66605209662932e-05,
      "loss": 0.2576,
      "step": 26090
    },
    {
      "epoch": 1.850700230455593,
      "grad_norm": 0.8922054767608643,
      "learning_rate": 7.661324634803574e-05,
      "loss": 0.2351,
      "step": 26100
    },
    {
      "epoch": 1.8514093245878391,
      "grad_norm": 0.9754472970962524,
      "learning_rate": 7.656597172977829e-05,
      "loss": 0.2326,
      "step": 26110
    },
    {
      "epoch": 1.8521184187200852,
      "grad_norm": 1.0373456478118896,
      "learning_rate": 7.651869711152083e-05,
      "loss": 0.255,
      "step": 26120
    },
    {
      "epoch": 1.8528275128523313,
      "grad_norm": 1.0942292213439941,
      "learning_rate": 7.647142249326336e-05,
      "loss": 0.2279,
      "step": 26130
    },
    {
      "epoch": 1.8535366069845773,
      "grad_norm": 1.638304352760315,
      "learning_rate": 7.642414787500591e-05,
      "loss": 0.2438,
      "step": 26140
    },
    {
      "epoch": 1.8542457011168234,
      "grad_norm": 1.204061508178711,
      "learning_rate": 7.637687325674846e-05,
      "loss": 0.2665,
      "step": 26150
    },
    {
      "epoch": 1.8549547952490695,
      "grad_norm": 2.7162232398986816,
      "learning_rate": 7.6329598638491e-05,
      "loss": 0.2252,
      "step": 26160
    },
    {
      "epoch": 1.8556638893813153,
      "grad_norm": 1.1695910692214966,
      "learning_rate": 7.628232402023354e-05,
      "loss": 0.247,
      "step": 26170
    },
    {
      "epoch": 1.8563729835135614,
      "grad_norm": 1.0019797086715698,
      "learning_rate": 7.623504940197607e-05,
      "loss": 0.2339,
      "step": 26180
    },
    {
      "epoch": 1.8570820776458075,
      "grad_norm": 1.2024065256118774,
      "learning_rate": 7.618777478371862e-05,
      "loss": 0.2251,
      "step": 26190
    },
    {
      "epoch": 1.8577911717780535,
      "grad_norm": 0.9306928515434265,
      "learning_rate": 7.614050016546117e-05,
      "loss": 0.2239,
      "step": 26200
    },
    {
      "epoch": 1.8585002659102996,
      "grad_norm": 0.7458248734474182,
      "learning_rate": 7.609322554720371e-05,
      "loss": 0.2145,
      "step": 26210
    },
    {
      "epoch": 1.8592093600425457,
      "grad_norm": 1.1382336616516113,
      "learning_rate": 7.604595092894626e-05,
      "loss": 0.2274,
      "step": 26220
    },
    {
      "epoch": 1.8599184541747917,
      "grad_norm": 1.592274785041809,
      "learning_rate": 7.59986763106888e-05,
      "loss": 0.2549,
      "step": 26230
    },
    {
      "epoch": 1.8606275483070378,
      "grad_norm": 1.113694429397583,
      "learning_rate": 7.595140169243133e-05,
      "loss": 0.2359,
      "step": 26240
    },
    {
      "epoch": 1.8613366424392837,
      "grad_norm": 1.1990059614181519,
      "learning_rate": 7.590412707417388e-05,
      "loss": 0.2279,
      "step": 26250
    },
    {
      "epoch": 1.8620457365715297,
      "grad_norm": 0.9928380846977234,
      "learning_rate": 7.585685245591643e-05,
      "loss": 0.2113,
      "step": 26260
    },
    {
      "epoch": 1.8627548307037758,
      "grad_norm": 0.8423486351966858,
      "learning_rate": 7.580957783765897e-05,
      "loss": 0.2247,
      "step": 26270
    },
    {
      "epoch": 1.8634639248360219,
      "grad_norm": 2.178636312484741,
      "learning_rate": 7.57623032194015e-05,
      "loss": 0.2459,
      "step": 26280
    },
    {
      "epoch": 1.864173018968268,
      "grad_norm": 3.474755048751831,
      "learning_rate": 7.571502860114404e-05,
      "loss": 0.2294,
      "step": 26290
    },
    {
      "epoch": 1.864882113100514,
      "grad_norm": 3.2947745323181152,
      "learning_rate": 7.566775398288659e-05,
      "loss": 0.2493,
      "step": 26300
    },
    {
      "epoch": 1.86559120723276,
      "grad_norm": 0.907097339630127,
      "learning_rate": 7.562047936462914e-05,
      "loss": 0.2269,
      "step": 26310
    },
    {
      "epoch": 1.8663003013650061,
      "grad_norm": 0.8835634589195251,
      "learning_rate": 7.557320474637167e-05,
      "loss": 0.2125,
      "step": 26320
    },
    {
      "epoch": 1.8670093954972522,
      "grad_norm": 1.3195480108261108,
      "learning_rate": 7.552593012811422e-05,
      "loss": 0.2344,
      "step": 26330
    },
    {
      "epoch": 1.8677184896294983,
      "grad_norm": 2.592719078063965,
      "learning_rate": 7.547865550985676e-05,
      "loss": 0.2241,
      "step": 26340
    },
    {
      "epoch": 1.8684275837617443,
      "grad_norm": 1.5212894678115845,
      "learning_rate": 7.54313808915993e-05,
      "loss": 0.2412,
      "step": 26350
    },
    {
      "epoch": 1.8691366778939904,
      "grad_norm": 0.7927391529083252,
      "learning_rate": 7.538410627334185e-05,
      "loss": 0.2417,
      "step": 26360
    },
    {
      "epoch": 1.8698457720262365,
      "grad_norm": 1.775575041770935,
      "learning_rate": 7.533683165508438e-05,
      "loss": 0.221,
      "step": 26370
    },
    {
      "epoch": 1.8705548661584825,
      "grad_norm": 0.8516351580619812,
      "learning_rate": 7.528955703682693e-05,
      "loss": 0.2421,
      "step": 26380
    },
    {
      "epoch": 1.8712639602907286,
      "grad_norm": 1.2351651191711426,
      "learning_rate": 7.524228241856948e-05,
      "loss": 0.2219,
      "step": 26390
    },
    {
      "epoch": 1.8719730544229747,
      "grad_norm": 1.0749186277389526,
      "learning_rate": 7.5195007800312e-05,
      "loss": 0.2432,
      "step": 26400
    },
    {
      "epoch": 1.8726821485552207,
      "grad_norm": 0.8808673024177551,
      "learning_rate": 7.514773318205456e-05,
      "loss": 0.2398,
      "step": 26410
    },
    {
      "epoch": 1.8733912426874668,
      "grad_norm": 1.3529012203216553,
      "learning_rate": 7.51004585637971e-05,
      "loss": 0.2459,
      "step": 26420
    },
    {
      "epoch": 1.8741003368197129,
      "grad_norm": 0.6931401491165161,
      "learning_rate": 7.505318394553964e-05,
      "loss": 0.2408,
      "step": 26430
    },
    {
      "epoch": 1.874809430951959,
      "grad_norm": 0.9967162013053894,
      "learning_rate": 7.500590932728219e-05,
      "loss": 0.2401,
      "step": 26440
    },
    {
      "epoch": 1.875518525084205,
      "grad_norm": 1.4981874227523804,
      "learning_rate": 7.495863470902473e-05,
      "loss": 0.2309,
      "step": 26450
    },
    {
      "epoch": 1.876227619216451,
      "grad_norm": 1.4569439888000488,
      "learning_rate": 7.491136009076726e-05,
      "loss": 0.239,
      "step": 26460
    },
    {
      "epoch": 1.8769367133486972,
      "grad_norm": 1.3057752847671509,
      "learning_rate": 7.486408547250981e-05,
      "loss": 0.2263,
      "step": 26470
    },
    {
      "epoch": 1.8776458074809432,
      "grad_norm": 0.8079548478126526,
      "learning_rate": 7.481681085425235e-05,
      "loss": 0.1934,
      "step": 26480
    },
    {
      "epoch": 1.8783549016131893,
      "grad_norm": 1.0301086902618408,
      "learning_rate": 7.47695362359949e-05,
      "loss": 0.2414,
      "step": 26490
    },
    {
      "epoch": 1.8790639957454354,
      "grad_norm": 1.1468911170959473,
      "learning_rate": 7.472226161773745e-05,
      "loss": 0.2548,
      "step": 26500
    },
    {
      "epoch": 1.8797730898776812,
      "grad_norm": 0.7348488569259644,
      "learning_rate": 7.467498699947997e-05,
      "loss": 0.2307,
      "step": 26510
    },
    {
      "epoch": 1.8804821840099273,
      "grad_norm": 0.8184707760810852,
      "learning_rate": 7.462771238122252e-05,
      "loss": 0.2288,
      "step": 26520
    },
    {
      "epoch": 1.8811912781421734,
      "grad_norm": 1.4508159160614014,
      "learning_rate": 7.458043776296507e-05,
      "loss": 0.2605,
      "step": 26530
    },
    {
      "epoch": 1.8819003722744194,
      "grad_norm": 2.2508578300476074,
      "learning_rate": 7.453316314470761e-05,
      "loss": 0.2514,
      "step": 26540
    },
    {
      "epoch": 1.8826094664066655,
      "grad_norm": 1.4556025266647339,
      "learning_rate": 7.448588852645016e-05,
      "loss": 0.2446,
      "step": 26550
    },
    {
      "epoch": 1.8833185605389116,
      "grad_norm": 1.369438886642456,
      "learning_rate": 7.44386139081927e-05,
      "loss": 0.2177,
      "step": 26560
    },
    {
      "epoch": 1.8840276546711576,
      "grad_norm": 1.0473225116729736,
      "learning_rate": 7.439133928993523e-05,
      "loss": 0.2636,
      "step": 26570
    },
    {
      "epoch": 1.8847367488034037,
      "grad_norm": 2.148772954940796,
      "learning_rate": 7.434406467167778e-05,
      "loss": 0.2473,
      "step": 26580
    },
    {
      "epoch": 1.8854458429356498,
      "grad_norm": 1.2041923999786377,
      "learning_rate": 7.429679005342032e-05,
      "loss": 0.2641,
      "step": 26590
    },
    {
      "epoch": 1.8861549370678956,
      "grad_norm": 1.5944479703903198,
      "learning_rate": 7.424951543516287e-05,
      "loss": 0.2077,
      "step": 26600
    },
    {
      "epoch": 1.8868640312001417,
      "grad_norm": 1.5705249309539795,
      "learning_rate": 7.420224081690542e-05,
      "loss": 0.2598,
      "step": 26610
    },
    {
      "epoch": 1.8875731253323877,
      "grad_norm": 0.9577001333236694,
      "learning_rate": 7.415496619864794e-05,
      "loss": 0.2175,
      "step": 26620
    },
    {
      "epoch": 1.8882822194646338,
      "grad_norm": 1.22653067111969,
      "learning_rate": 7.410769158039049e-05,
      "loss": 0.2365,
      "step": 26630
    },
    {
      "epoch": 1.8889913135968799,
      "grad_norm": 0.7655432820320129,
      "learning_rate": 7.406041696213304e-05,
      "loss": 0.2417,
      "step": 26640
    },
    {
      "epoch": 1.889700407729126,
      "grad_norm": 1.8541706800460815,
      "learning_rate": 7.401314234387558e-05,
      "loss": 0.2643,
      "step": 26650
    },
    {
      "epoch": 1.890409501861372,
      "grad_norm": 1.0785685777664185,
      "learning_rate": 7.396586772561813e-05,
      "loss": 0.2336,
      "step": 26660
    },
    {
      "epoch": 1.891118595993618,
      "grad_norm": 1.6747891902923584,
      "learning_rate": 7.391859310736066e-05,
      "loss": 0.2276,
      "step": 26670
    },
    {
      "epoch": 1.8918276901258642,
      "grad_norm": 1.0528950691223145,
      "learning_rate": 7.38713184891032e-05,
      "loss": 0.2572,
      "step": 26680
    },
    {
      "epoch": 1.8925367842581102,
      "grad_norm": 2.646423578262329,
      "learning_rate": 7.382404387084575e-05,
      "loss": 0.2319,
      "step": 26690
    },
    {
      "epoch": 1.8932458783903563,
      "grad_norm": 0.9261425137519836,
      "learning_rate": 7.377676925258828e-05,
      "loss": 0.2302,
      "step": 26700
    },
    {
      "epoch": 1.8939549725226024,
      "grad_norm": 2.1287524700164795,
      "learning_rate": 7.372949463433083e-05,
      "loss": 0.2679,
      "step": 26710
    },
    {
      "epoch": 1.8946640666548484,
      "grad_norm": 1.2519519329071045,
      "learning_rate": 7.368222001607338e-05,
      "loss": 0.2145,
      "step": 26720
    },
    {
      "epoch": 1.8953731607870945,
      "grad_norm": 1.1521064043045044,
      "learning_rate": 7.363494539781591e-05,
      "loss": 0.2439,
      "step": 26730
    },
    {
      "epoch": 1.8960822549193406,
      "grad_norm": 0.9260565638542175,
      "learning_rate": 7.358767077955846e-05,
      "loss": 0.2321,
      "step": 26740
    },
    {
      "epoch": 1.8967913490515866,
      "grad_norm": 1.4425357580184937,
      "learning_rate": 7.3540396161301e-05,
      "loss": 0.2663,
      "step": 26750
    },
    {
      "epoch": 1.8975004431838327,
      "grad_norm": 1.3295462131500244,
      "learning_rate": 7.349312154304354e-05,
      "loss": 0.2683,
      "step": 26760
    },
    {
      "epoch": 1.8982095373160788,
      "grad_norm": 0.9238731861114502,
      "learning_rate": 7.344584692478609e-05,
      "loss": 0.2175,
      "step": 26770
    },
    {
      "epoch": 1.8989186314483248,
      "grad_norm": 1.0852230787277222,
      "learning_rate": 7.339857230652863e-05,
      "loss": 0.2594,
      "step": 26780
    },
    {
      "epoch": 1.899627725580571,
      "grad_norm": 1.5139083862304688,
      "learning_rate": 7.335129768827117e-05,
      "loss": 0.2411,
      "step": 26790
    },
    {
      "epoch": 1.900336819712817,
      "grad_norm": 4.344278812408447,
      "learning_rate": 7.330402307001372e-05,
      "loss": 0.2569,
      "step": 26800
    },
    {
      "epoch": 1.901045913845063,
      "grad_norm": 1.3013741970062256,
      "learning_rate": 7.325674845175625e-05,
      "loss": 0.2618,
      "step": 26810
    },
    {
      "epoch": 1.9017550079773091,
      "grad_norm": 0.973054826259613,
      "learning_rate": 7.32094738334988e-05,
      "loss": 0.237,
      "step": 26820
    },
    {
      "epoch": 1.9024641021095552,
      "grad_norm": 1.0602903366088867,
      "learning_rate": 7.316219921524135e-05,
      "loss": 0.2471,
      "step": 26830
    },
    {
      "epoch": 1.9031731962418013,
      "grad_norm": 2.444445848464966,
      "learning_rate": 7.311492459698387e-05,
      "loss": 0.2106,
      "step": 26840
    },
    {
      "epoch": 1.903882290374047,
      "grad_norm": 1.3050981760025024,
      "learning_rate": 7.306764997872642e-05,
      "loss": 0.2909,
      "step": 26850
    },
    {
      "epoch": 1.9045913845062932,
      "grad_norm": 1.1170543432235718,
      "learning_rate": 7.302037536046896e-05,
      "loss": 0.2432,
      "step": 26860
    },
    {
      "epoch": 1.9053004786385392,
      "grad_norm": 1.0680501461029053,
      "learning_rate": 7.297310074221151e-05,
      "loss": 0.2534,
      "step": 26870
    },
    {
      "epoch": 1.9060095727707853,
      "grad_norm": 1.025498628616333,
      "learning_rate": 7.292582612395406e-05,
      "loss": 0.2453,
      "step": 26880
    },
    {
      "epoch": 1.9067186669030314,
      "grad_norm": 1.5577937364578247,
      "learning_rate": 7.28785515056966e-05,
      "loss": 0.2653,
      "step": 26890
    },
    {
      "epoch": 1.9074277610352774,
      "grad_norm": 1.032277226448059,
      "learning_rate": 7.283127688743913e-05,
      "loss": 0.258,
      "step": 26900
    },
    {
      "epoch": 1.9081368551675235,
      "grad_norm": 0.7337713837623596,
      "learning_rate": 7.278400226918168e-05,
      "loss": 0.2252,
      "step": 26910
    },
    {
      "epoch": 1.9088459492997696,
      "grad_norm": 0.8431169390678406,
      "learning_rate": 7.273672765092422e-05,
      "loss": 0.2687,
      "step": 26920
    },
    {
      "epoch": 1.9095550434320157,
      "grad_norm": 0.8116247057914734,
      "learning_rate": 7.268945303266677e-05,
      "loss": 0.2562,
      "step": 26930
    },
    {
      "epoch": 1.9102641375642615,
      "grad_norm": 0.9725112915039062,
      "learning_rate": 7.264217841440932e-05,
      "loss": 0.254,
      "step": 26940
    },
    {
      "epoch": 1.9109732316965076,
      "grad_norm": 1.497485876083374,
      "learning_rate": 7.259490379615184e-05,
      "loss": 0.252,
      "step": 26950
    },
    {
      "epoch": 1.9116823258287536,
      "grad_norm": 0.7923049330711365,
      "learning_rate": 7.254762917789439e-05,
      "loss": 0.249,
      "step": 26960
    },
    {
      "epoch": 1.9123914199609997,
      "grad_norm": 1.3919239044189453,
      "learning_rate": 7.250035455963693e-05,
      "loss": 0.2505,
      "step": 26970
    },
    {
      "epoch": 1.9131005140932458,
      "grad_norm": 0.8023979067802429,
      "learning_rate": 7.245307994137948e-05,
      "loss": 0.2205,
      "step": 26980
    },
    {
      "epoch": 1.9138096082254918,
      "grad_norm": 1.140333890914917,
      "learning_rate": 7.240580532312203e-05,
      "loss": 0.2442,
      "step": 26990
    },
    {
      "epoch": 1.914518702357738,
      "grad_norm": 0.8856426477432251,
      "learning_rate": 7.235853070486456e-05,
      "loss": 0.2521,
      "step": 27000
    },
    {
      "epoch": 1.915227796489984,
      "grad_norm": 0.7747094631195068,
      "learning_rate": 7.23112560866071e-05,
      "loss": 0.2534,
      "step": 27010
    },
    {
      "epoch": 1.91593689062223,
      "grad_norm": 1.2630956172943115,
      "learning_rate": 7.226398146834965e-05,
      "loss": 0.2594,
      "step": 27020
    },
    {
      "epoch": 1.9166459847544761,
      "grad_norm": 1.706693172454834,
      "learning_rate": 7.221670685009219e-05,
      "loss": 0.264,
      "step": 27030
    },
    {
      "epoch": 1.9173550788867222,
      "grad_norm": 0.8369713425636292,
      "learning_rate": 7.216943223183474e-05,
      "loss": 0.2377,
      "step": 27040
    },
    {
      "epoch": 1.9180641730189683,
      "grad_norm": 3.8745040893554688,
      "learning_rate": 7.212215761357729e-05,
      "loss": 0.214,
      "step": 27050
    },
    {
      "epoch": 1.9187732671512143,
      "grad_norm": 3.9257829189300537,
      "learning_rate": 7.207488299531982e-05,
      "loss": 0.2268,
      "step": 27060
    },
    {
      "epoch": 1.9194823612834604,
      "grad_norm": 1.25234854221344,
      "learning_rate": 7.202760837706236e-05,
      "loss": 0.238,
      "step": 27070
    },
    {
      "epoch": 1.9201914554157065,
      "grad_norm": 1.468283772468567,
      "learning_rate": 7.19803337588049e-05,
      "loss": 0.2262,
      "step": 27080
    },
    {
      "epoch": 1.9209005495479525,
      "grad_norm": 1.0521509647369385,
      "learning_rate": 7.193305914054744e-05,
      "loss": 0.2133,
      "step": 27090
    },
    {
      "epoch": 1.9216096436801986,
      "grad_norm": 0.9295065999031067,
      "learning_rate": 7.188578452229e-05,
      "loss": 0.2558,
      "step": 27100
    },
    {
      "epoch": 1.9223187378124447,
      "grad_norm": 0.7230724096298218,
      "learning_rate": 7.183850990403253e-05,
      "loss": 0.2412,
      "step": 27110
    },
    {
      "epoch": 1.9230278319446907,
      "grad_norm": 2.287814140319824,
      "learning_rate": 7.179123528577507e-05,
      "loss": 0.2584,
      "step": 27120
    },
    {
      "epoch": 1.9237369260769368,
      "grad_norm": 2.0736122131347656,
      "learning_rate": 7.174396066751762e-05,
      "loss": 0.2689,
      "step": 27130
    },
    {
      "epoch": 1.9244460202091829,
      "grad_norm": 1.4365146160125732,
      "learning_rate": 7.169668604926015e-05,
      "loss": 0.2529,
      "step": 27140
    },
    {
      "epoch": 1.925155114341429,
      "grad_norm": 2.867959976196289,
      "learning_rate": 7.16494114310027e-05,
      "loss": 0.2519,
      "step": 27150
    },
    {
      "epoch": 1.925864208473675,
      "grad_norm": 1.0683547258377075,
      "learning_rate": 7.160213681274524e-05,
      "loss": 0.2409,
      "step": 27160
    },
    {
      "epoch": 1.926573302605921,
      "grad_norm": 1.1459860801696777,
      "learning_rate": 7.155486219448779e-05,
      "loss": 0.2503,
      "step": 27170
    },
    {
      "epoch": 1.9272823967381671,
      "grad_norm": 1.5158686637878418,
      "learning_rate": 7.150758757623032e-05,
      "loss": 0.2352,
      "step": 27180
    },
    {
      "epoch": 1.927991490870413,
      "grad_norm": 1.7355555295944214,
      "learning_rate": 7.146031295797286e-05,
      "loss": 0.257,
      "step": 27190
    },
    {
      "epoch": 1.928700585002659,
      "grad_norm": 1.3880927562713623,
      "learning_rate": 7.141303833971541e-05,
      "loss": 0.2597,
      "step": 27200
    },
    {
      "epoch": 1.9294096791349051,
      "grad_norm": 1.7112473249435425,
      "learning_rate": 7.136576372145796e-05,
      "loss": 0.2469,
      "step": 27210
    },
    {
      "epoch": 1.9301187732671512,
      "grad_norm": 0.9930483102798462,
      "learning_rate": 7.13184891032005e-05,
      "loss": 0.2768,
      "step": 27220
    },
    {
      "epoch": 1.9308278673993973,
      "grad_norm": 2.6637678146362305,
      "learning_rate": 7.127121448494303e-05,
      "loss": 0.2163,
      "step": 27230
    },
    {
      "epoch": 1.9315369615316433,
      "grad_norm": 1.0177133083343506,
      "learning_rate": 7.122393986668558e-05,
      "loss": 0.2482,
      "step": 27240
    },
    {
      "epoch": 1.9322460556638894,
      "grad_norm": 0.988288938999176,
      "learning_rate": 7.117666524842812e-05,
      "loss": 0.2512,
      "step": 27250
    },
    {
      "epoch": 1.9329551497961355,
      "grad_norm": 0.9277389049530029,
      "learning_rate": 7.112939063017067e-05,
      "loss": 0.2713,
      "step": 27260
    },
    {
      "epoch": 1.9336642439283815,
      "grad_norm": 0.8711179494857788,
      "learning_rate": 7.10821160119132e-05,
      "loss": 0.2492,
      "step": 27270
    },
    {
      "epoch": 1.9343733380606274,
      "grad_norm": 2.52227783203125,
      "learning_rate": 7.103484139365576e-05,
      "loss": 0.2352,
      "step": 27280
    },
    {
      "epoch": 1.9350824321928735,
      "grad_norm": 0.7088712453842163,
      "learning_rate": 7.098756677539829e-05,
      "loss": 0.2565,
      "step": 27290
    },
    {
      "epoch": 1.9357915263251195,
      "grad_norm": 0.9552231431007385,
      "learning_rate": 7.094029215714083e-05,
      "loss": 0.2386,
      "step": 27300
    },
    {
      "epoch": 1.9365006204573656,
      "grad_norm": 1.179718017578125,
      "learning_rate": 7.089301753888338e-05,
      "loss": 0.2311,
      "step": 27310
    },
    {
      "epoch": 1.9372097145896117,
      "grad_norm": 1.7234171628952026,
      "learning_rate": 7.084574292062593e-05,
      "loss": 0.2483,
      "step": 27320
    },
    {
      "epoch": 1.9379188087218577,
      "grad_norm": 0.8012826442718506,
      "learning_rate": 7.079846830236846e-05,
      "loss": 0.2556,
      "step": 27330
    },
    {
      "epoch": 1.9386279028541038,
      "grad_norm": 1.4061509370803833,
      "learning_rate": 7.0751193684111e-05,
      "loss": 0.2327,
      "step": 27340
    },
    {
      "epoch": 1.9393369969863499,
      "grad_norm": 1.1071491241455078,
      "learning_rate": 7.070391906585354e-05,
      "loss": 0.2534,
      "step": 27350
    },
    {
      "epoch": 1.940046091118596,
      "grad_norm": 1.0743063688278198,
      "learning_rate": 7.065664444759609e-05,
      "loss": 0.2279,
      "step": 27360
    },
    {
      "epoch": 1.940755185250842,
      "grad_norm": 1.067352294921875,
      "learning_rate": 7.060936982933864e-05,
      "loss": 0.2447,
      "step": 27370
    },
    {
      "epoch": 1.941464279383088,
      "grad_norm": 1.8171089887619019,
      "learning_rate": 7.056209521108117e-05,
      "loss": 0.2557,
      "step": 27380
    },
    {
      "epoch": 1.9421733735153341,
      "grad_norm": 1.3851730823516846,
      "learning_rate": 7.051482059282372e-05,
      "loss": 0.2427,
      "step": 27390
    },
    {
      "epoch": 1.9428824676475802,
      "grad_norm": 1.4932613372802734,
      "learning_rate": 7.046754597456626e-05,
      "loss": 0.2632,
      "step": 27400
    },
    {
      "epoch": 1.9435915617798263,
      "grad_norm": 2.196939706802368,
      "learning_rate": 7.04202713563088e-05,
      "loss": 0.2299,
      "step": 27410
    },
    {
      "epoch": 1.9443006559120724,
      "grad_norm": 0.8955675959587097,
      "learning_rate": 7.037299673805134e-05,
      "loss": 0.2891,
      "step": 27420
    },
    {
      "epoch": 1.9450097500443184,
      "grad_norm": 1.5758038759231567,
      "learning_rate": 7.03257221197939e-05,
      "loss": 0.2266,
      "step": 27430
    },
    {
      "epoch": 1.9457188441765645,
      "grad_norm": 1.2427951097488403,
      "learning_rate": 7.027844750153643e-05,
      "loss": 0.2324,
      "step": 27440
    },
    {
      "epoch": 1.9464279383088106,
      "grad_norm": 0.9669604301452637,
      "learning_rate": 7.023117288327897e-05,
      "loss": 0.2298,
      "step": 27450
    },
    {
      "epoch": 1.9471370324410566,
      "grad_norm": 0.9815337657928467,
      "learning_rate": 7.01838982650215e-05,
      "loss": 0.2693,
      "step": 27460
    },
    {
      "epoch": 1.9478461265733027,
      "grad_norm": 0.9571704864501953,
      "learning_rate": 7.013662364676405e-05,
      "loss": 0.2399,
      "step": 27470
    },
    {
      "epoch": 1.9485552207055488,
      "grad_norm": 1.5573912858963013,
      "learning_rate": 7.00893490285066e-05,
      "loss": 0.2408,
      "step": 27480
    },
    {
      "epoch": 1.9492643148377948,
      "grad_norm": 0.8403120040893555,
      "learning_rate": 7.004207441024914e-05,
      "loss": 0.2275,
      "step": 27490
    },
    {
      "epoch": 1.949973408970041,
      "grad_norm": 0.6672243475914001,
      "learning_rate": 6.999479979199169e-05,
      "loss": 0.2358,
      "step": 27500
    },
    {
      "epoch": 1.950682503102287,
      "grad_norm": 1.1134206056594849,
      "learning_rate": 6.994752517373423e-05,
      "loss": 0.2512,
      "step": 27510
    },
    {
      "epoch": 1.951391597234533,
      "grad_norm": 1.0509639978408813,
      "learning_rate": 6.990025055547676e-05,
      "loss": 0.2581,
      "step": 27520
    },
    {
      "epoch": 1.952100691366779,
      "grad_norm": 1.051317811012268,
      "learning_rate": 6.985297593721931e-05,
      "loss": 0.2079,
      "step": 27530
    },
    {
      "epoch": 1.952809785499025,
      "grad_norm": 1.589234709739685,
      "learning_rate": 6.980570131896186e-05,
      "loss": 0.2094,
      "step": 27540
    },
    {
      "epoch": 1.953518879631271,
      "grad_norm": 1.3138682842254639,
      "learning_rate": 6.97584267007044e-05,
      "loss": 0.2403,
      "step": 27550
    },
    {
      "epoch": 1.954227973763517,
      "grad_norm": 1.5045421123504639,
      "learning_rate": 6.971115208244693e-05,
      "loss": 0.2268,
      "step": 27560
    },
    {
      "epoch": 1.9549370678957632,
      "grad_norm": 3.4322378635406494,
      "learning_rate": 6.966387746418947e-05,
      "loss": 0.2325,
      "step": 27570
    },
    {
      "epoch": 1.9556461620280092,
      "grad_norm": 1.3470975160598755,
      "learning_rate": 6.961660284593202e-05,
      "loss": 0.2888,
      "step": 27580
    },
    {
      "epoch": 1.9563552561602553,
      "grad_norm": 1.021926760673523,
      "learning_rate": 6.956932822767457e-05,
      "loss": 0.2609,
      "step": 27590
    },
    {
      "epoch": 1.9570643502925014,
      "grad_norm": 1.3971288204193115,
      "learning_rate": 6.95220536094171e-05,
      "loss": 0.2402,
      "step": 27600
    },
    {
      "epoch": 1.9577734444247474,
      "grad_norm": 1.4511981010437012,
      "learning_rate": 6.947477899115966e-05,
      "loss": 0.2597,
      "step": 27610
    },
    {
      "epoch": 1.9584825385569933,
      "grad_norm": 1.1551865339279175,
      "learning_rate": 6.942750437290219e-05,
      "loss": 0.2562,
      "step": 27620
    },
    {
      "epoch": 1.9591916326892393,
      "grad_norm": 0.8233416080474854,
      "learning_rate": 6.938022975464473e-05,
      "loss": 0.2367,
      "step": 27630
    },
    {
      "epoch": 1.9599007268214854,
      "grad_norm": 1.2566624879837036,
      "learning_rate": 6.933295513638728e-05,
      "loss": 0.2685,
      "step": 27640
    },
    {
      "epoch": 1.9606098209537315,
      "grad_norm": 1.0129657983779907,
      "learning_rate": 6.928568051812981e-05,
      "loss": 0.1868,
      "step": 27650
    },
    {
      "epoch": 1.9613189150859776,
      "grad_norm": 1.0849635601043701,
      "learning_rate": 6.923840589987236e-05,
      "loss": 0.2179,
      "step": 27660
    },
    {
      "epoch": 1.9620280092182236,
      "grad_norm": 1.6023637056350708,
      "learning_rate": 6.91911312816149e-05,
      "loss": 0.2292,
      "step": 27670
    },
    {
      "epoch": 1.9627371033504697,
      "grad_norm": 1.0531344413757324,
      "learning_rate": 6.914385666335744e-05,
      "loss": 0.2306,
      "step": 27680
    },
    {
      "epoch": 1.9634461974827158,
      "grad_norm": 1.3750200271606445,
      "learning_rate": 6.909658204509999e-05,
      "loss": 0.237,
      "step": 27690
    },
    {
      "epoch": 1.9641552916149618,
      "grad_norm": 1.8158780336380005,
      "learning_rate": 6.904930742684254e-05,
      "loss": 0.2692,
      "step": 27700
    },
    {
      "epoch": 1.964864385747208,
      "grad_norm": 2.088059425354004,
      "learning_rate": 6.900203280858507e-05,
      "loss": 0.2391,
      "step": 27710
    },
    {
      "epoch": 1.965573479879454,
      "grad_norm": 0.9696289300918579,
      "learning_rate": 6.895475819032762e-05,
      "loss": 0.2373,
      "step": 27720
    },
    {
      "epoch": 1.9662825740117,
      "grad_norm": 0.7040741443634033,
      "learning_rate": 6.890748357207016e-05,
      "loss": 0.2417,
      "step": 27730
    },
    {
      "epoch": 1.966991668143946,
      "grad_norm": 2.559425115585327,
      "learning_rate": 6.88602089538127e-05,
      "loss": 0.2055,
      "step": 27740
    },
    {
      "epoch": 1.9677007622761922,
      "grad_norm": 1.6012017726898193,
      "learning_rate": 6.881293433555525e-05,
      "loss": 0.2509,
      "step": 27750
    },
    {
      "epoch": 1.9684098564084382,
      "grad_norm": 2.2266275882720947,
      "learning_rate": 6.876565971729778e-05,
      "loss": 0.2594,
      "step": 27760
    },
    {
      "epoch": 1.9691189505406843,
      "grad_norm": 0.881680428981781,
      "learning_rate": 6.871838509904033e-05,
      "loss": 0.2362,
      "step": 27770
    },
    {
      "epoch": 1.9698280446729304,
      "grad_norm": 2.429826259613037,
      "learning_rate": 6.867111048078287e-05,
      "loss": 0.2648,
      "step": 27780
    },
    {
      "epoch": 1.9705371388051764,
      "grad_norm": 0.7317372560501099,
      "learning_rate": 6.86238358625254e-05,
      "loss": 0.2299,
      "step": 27790
    },
    {
      "epoch": 1.9712462329374225,
      "grad_norm": 0.9928094148635864,
      "learning_rate": 6.857656124426795e-05,
      "loss": 0.2526,
      "step": 27800
    },
    {
      "epoch": 1.9719553270696686,
      "grad_norm": 1.494530200958252,
      "learning_rate": 6.85292866260105e-05,
      "loss": 0.2159,
      "step": 27810
    },
    {
      "epoch": 1.9726644212019147,
      "grad_norm": 1.314231514930725,
      "learning_rate": 6.848201200775304e-05,
      "loss": 0.2462,
      "step": 27820
    },
    {
      "epoch": 1.9733735153341607,
      "grad_norm": 0.9574023485183716,
      "learning_rate": 6.843473738949559e-05,
      "loss": 0.2387,
      "step": 27830
    },
    {
      "epoch": 1.9740826094664068,
      "grad_norm": 1.6091097593307495,
      "learning_rate": 6.838746277123813e-05,
      "loss": 0.2485,
      "step": 27840
    },
    {
      "epoch": 1.9747917035986529,
      "grad_norm": 0.8518813848495483,
      "learning_rate": 6.834018815298066e-05,
      "loss": 0.2785,
      "step": 27850
    },
    {
      "epoch": 1.975500797730899,
      "grad_norm": 0.8302555084228516,
      "learning_rate": 6.829291353472321e-05,
      "loss": 0.2393,
      "step": 27860
    },
    {
      "epoch": 1.976209891863145,
      "grad_norm": 1.0904570817947388,
      "learning_rate": 6.824563891646575e-05,
      "loss": 0.2556,
      "step": 27870
    },
    {
      "epoch": 1.9769189859953908,
      "grad_norm": 1.0645830631256104,
      "learning_rate": 6.81983642982083e-05,
      "loss": 0.2437,
      "step": 27880
    },
    {
      "epoch": 1.977628080127637,
      "grad_norm": 1.8696297407150269,
      "learning_rate": 6.815108967995084e-05,
      "loss": 0.2307,
      "step": 27890
    },
    {
      "epoch": 1.978337174259883,
      "grad_norm": 1.9306645393371582,
      "learning_rate": 6.810381506169337e-05,
      "loss": 0.2323,
      "step": 27900
    },
    {
      "epoch": 1.979046268392129,
      "grad_norm": 1.5872242450714111,
      "learning_rate": 6.805654044343592e-05,
      "loss": 0.2517,
      "step": 27910
    },
    {
      "epoch": 1.9797553625243751,
      "grad_norm": 1.7448934316635132,
      "learning_rate": 6.800926582517847e-05,
      "loss": 0.1987,
      "step": 27920
    },
    {
      "epoch": 1.9804644566566212,
      "grad_norm": 1.4436852931976318,
      "learning_rate": 6.796199120692101e-05,
      "loss": 0.2633,
      "step": 27930
    },
    {
      "epoch": 1.9811735507888673,
      "grad_norm": 0.9226223230361938,
      "learning_rate": 6.791471658866356e-05,
      "loss": 0.2614,
      "step": 27940
    },
    {
      "epoch": 1.9818826449211133,
      "grad_norm": 0.9377720952033997,
      "learning_rate": 6.78674419704061e-05,
      "loss": 0.2252,
      "step": 27950
    },
    {
      "epoch": 1.9825917390533592,
      "grad_norm": 1.7146583795547485,
      "learning_rate": 6.782016735214863e-05,
      "loss": 0.2463,
      "step": 27960
    },
    {
      "epoch": 1.9833008331856052,
      "grad_norm": 1.864241600036621,
      "learning_rate": 6.777289273389118e-05,
      "loss": 0.2362,
      "step": 27970
    },
    {
      "epoch": 1.9840099273178513,
      "grad_norm": 1.194276213645935,
      "learning_rate": 6.772561811563372e-05,
      "loss": 0.2635,
      "step": 27980
    },
    {
      "epoch": 1.9847190214500974,
      "grad_norm": 1.085358738899231,
      "learning_rate": 6.767834349737627e-05,
      "loss": 0.2574,
      "step": 27990
    },
    {
      "epoch": 1.9854281155823434,
      "grad_norm": 0.6145483255386353,
      "learning_rate": 6.76310688791188e-05,
      "loss": 0.2536,
      "step": 28000
    },
    {
      "epoch": 1.9861372097145895,
      "grad_norm": 1.157349705696106,
      "learning_rate": 6.758379426086134e-05,
      "loss": 0.2526,
      "step": 28010
    },
    {
      "epoch": 1.9868463038468356,
      "grad_norm": 0.7605499029159546,
      "learning_rate": 6.753651964260389e-05,
      "loss": 0.2376,
      "step": 28020
    },
    {
      "epoch": 1.9875553979790817,
      "grad_norm": 1.470137357711792,
      "learning_rate": 6.748924502434644e-05,
      "loss": 0.2058,
      "step": 28030
    },
    {
      "epoch": 1.9882644921113277,
      "grad_norm": 0.8688297271728516,
      "learning_rate": 6.744197040608897e-05,
      "loss": 0.2658,
      "step": 28040
    },
    {
      "epoch": 1.9889735862435738,
      "grad_norm": 0.8552697896957397,
      "learning_rate": 6.739469578783152e-05,
      "loss": 0.2492,
      "step": 28050
    },
    {
      "epoch": 1.9896826803758199,
      "grad_norm": 1.7364915609359741,
      "learning_rate": 6.734742116957406e-05,
      "loss": 0.2309,
      "step": 28060
    },
    {
      "epoch": 1.990391774508066,
      "grad_norm": 1.8456958532333374,
      "learning_rate": 6.73001465513166e-05,
      "loss": 0.2527,
      "step": 28070
    },
    {
      "epoch": 1.991100868640312,
      "grad_norm": 1.321685791015625,
      "learning_rate": 6.725287193305915e-05,
      "loss": 0.23,
      "step": 28080
    },
    {
      "epoch": 1.991809962772558,
      "grad_norm": 0.7969338893890381,
      "learning_rate": 6.720559731480168e-05,
      "loss": 0.2601,
      "step": 28090
    },
    {
      "epoch": 1.9925190569048041,
      "grad_norm": 1.2540490627288818,
      "learning_rate": 6.715832269654423e-05,
      "loss": 0.2449,
      "step": 28100
    },
    {
      "epoch": 1.9932281510370502,
      "grad_norm": 4.117729663848877,
      "learning_rate": 6.711104807828677e-05,
      "loss": 0.2171,
      "step": 28110
    },
    {
      "epoch": 1.9939372451692963,
      "grad_norm": 0.7207353115081787,
      "learning_rate": 6.70637734600293e-05,
      "loss": 0.2499,
      "step": 28120
    },
    {
      "epoch": 1.9946463393015423,
      "grad_norm": 0.9563771486282349,
      "learning_rate": 6.701649884177186e-05,
      "loss": 0.237,
      "step": 28130
    },
    {
      "epoch": 1.9953554334337884,
      "grad_norm": 1.2559325695037842,
      "learning_rate": 6.696922422351439e-05,
      "loss": 0.2646,
      "step": 28140
    },
    {
      "epoch": 1.9960645275660345,
      "grad_norm": 2.0376992225646973,
      "learning_rate": 6.692194960525694e-05,
      "loss": 0.2513,
      "step": 28150
    },
    {
      "epoch": 1.9967736216982805,
      "grad_norm": 1.8906171321868896,
      "learning_rate": 6.687467498699949e-05,
      "loss": 0.2321,
      "step": 28160
    },
    {
      "epoch": 1.9974827158305266,
      "grad_norm": 1.2031022310256958,
      "learning_rate": 6.682740036874203e-05,
      "loss": 0.2464,
      "step": 28170
    },
    {
      "epoch": 1.9981918099627727,
      "grad_norm": 2.3944509029388428,
      "learning_rate": 6.678012575048456e-05,
      "loss": 0.2363,
      "step": 28180
    },
    {
      "epoch": 1.9989009040950187,
      "grad_norm": 0.9660888314247131,
      "learning_rate": 6.673285113222711e-05,
      "loss": 0.2352,
      "step": 28190
    },
    {
      "epoch": 1.9996099982272648,
      "grad_norm": 1.3348329067230225,
      "learning_rate": 6.668557651396965e-05,
      "loss": 0.2382,
      "step": 28200
    },
    {
      "epoch": 2.0,
      "eval_runtime": 1265.9895,
      "eval_samples_per_second": 5.57,
      "eval_steps_per_second": 5.57,
      "step": 28206
    },
    {
      "epoch": 2.0002836376528985,
      "grad_norm": 1.290963888168335,
      "learning_rate": 6.66383018957122e-05,
      "loss": 0.2494,
      "step": 28210
    },
    {
      "epoch": 2.0009927317851446,
      "grad_norm": 1.0832642316818237,
      "learning_rate": 6.659102727745474e-05,
      "loss": 0.228,
      "step": 28220
    },
    {
      "epoch": 2.0017018259173907,
      "grad_norm": 1.3595224618911743,
      "learning_rate": 6.654375265919727e-05,
      "loss": 0.2303,
      "step": 28230
    },
    {
      "epoch": 2.0024109200496367,
      "grad_norm": 1.4760175943374634,
      "learning_rate": 6.649647804093982e-05,
      "loss": 0.24,
      "step": 28240
    },
    {
      "epoch": 2.003120014181883,
      "grad_norm": 1.499370813369751,
      "learning_rate": 6.644920342268236e-05,
      "loss": 0.234,
      "step": 28250
    },
    {
      "epoch": 2.003829108314129,
      "grad_norm": 0.8684335947036743,
      "learning_rate": 6.640192880442491e-05,
      "loss": 0.2535,
      "step": 28260
    },
    {
      "epoch": 2.004538202446375,
      "grad_norm": 2.514697551727295,
      "learning_rate": 6.635465418616746e-05,
      "loss": 0.2542,
      "step": 28270
    },
    {
      "epoch": 2.005247296578621,
      "grad_norm": 1.037095546722412,
      "learning_rate": 6.630737956791e-05,
      "loss": 0.2175,
      "step": 28280
    },
    {
      "epoch": 2.005956390710867,
      "grad_norm": 0.9456494450569153,
      "learning_rate": 6.626010494965253e-05,
      "loss": 0.2197,
      "step": 28290
    },
    {
      "epoch": 2.006665484843113,
      "grad_norm": 1.7857460975646973,
      "learning_rate": 6.621283033139508e-05,
      "loss": 0.2917,
      "step": 28300
    },
    {
      "epoch": 2.0073745789753588,
      "grad_norm": 0.8900026679039001,
      "learning_rate": 6.616555571313762e-05,
      "loss": 0.247,
      "step": 28310
    },
    {
      "epoch": 2.008083673107605,
      "grad_norm": 1.1039328575134277,
      "learning_rate": 6.611828109488017e-05,
      "loss": 0.2319,
      "step": 28320
    },
    {
      "epoch": 2.008792767239851,
      "grad_norm": 0.6987956166267395,
      "learning_rate": 6.607100647662272e-05,
      "loss": 0.2229,
      "step": 28330
    },
    {
      "epoch": 2.009501861372097,
      "grad_norm": 1.956634759902954,
      "learning_rate": 6.602373185836524e-05,
      "loss": 0.2537,
      "step": 28340
    },
    {
      "epoch": 2.010210955504343,
      "grad_norm": 1.3566869497299194,
      "learning_rate": 6.597645724010779e-05,
      "loss": 0.241,
      "step": 28350
    },
    {
      "epoch": 2.010920049636589,
      "grad_norm": 1.0542688369750977,
      "learning_rate": 6.592918262185033e-05,
      "loss": 0.2219,
      "step": 28360
    },
    {
      "epoch": 2.011629143768835,
      "grad_norm": 1.3068948984146118,
      "learning_rate": 6.588190800359288e-05,
      "loss": 0.2482,
      "step": 28370
    },
    {
      "epoch": 2.0123382379010812,
      "grad_norm": 1.8132622241973877,
      "learning_rate": 6.583463338533543e-05,
      "loss": 0.2479,
      "step": 28380
    },
    {
      "epoch": 2.0130473320333273,
      "grad_norm": 1.258243441581726,
      "learning_rate": 6.578735876707796e-05,
      "loss": 0.2401,
      "step": 28390
    },
    {
      "epoch": 2.0137564261655734,
      "grad_norm": 1.323972225189209,
      "learning_rate": 6.57400841488205e-05,
      "loss": 0.2373,
      "step": 28400
    },
    {
      "epoch": 2.0144655202978194,
      "grad_norm": 1.6410698890686035,
      "learning_rate": 6.569280953056305e-05,
      "loss": 0.2757,
      "step": 28410
    },
    {
      "epoch": 2.0151746144300655,
      "grad_norm": 2.4344427585601807,
      "learning_rate": 6.564553491230558e-05,
      "loss": 0.2528,
      "step": 28420
    },
    {
      "epoch": 2.0158837085623116,
      "grad_norm": 0.8924049139022827,
      "learning_rate": 6.559826029404813e-05,
      "loss": 0.2338,
      "step": 28430
    },
    {
      "epoch": 2.0165928026945577,
      "grad_norm": 1.610706090927124,
      "learning_rate": 6.555098567579067e-05,
      "loss": 0.2438,
      "step": 28440
    },
    {
      "epoch": 2.0173018968268037,
      "grad_norm": 0.9419122934341431,
      "learning_rate": 6.55037110575332e-05,
      "loss": 0.2643,
      "step": 28450
    },
    {
      "epoch": 2.01801099095905,
      "grad_norm": 2.1024792194366455,
      "learning_rate": 6.545643643927576e-05,
      "loss": 0.2239,
      "step": 28460
    },
    {
      "epoch": 2.018720085091296,
      "grad_norm": 1.2042280435562134,
      "learning_rate": 6.540916182101829e-05,
      "loss": 0.2361,
      "step": 28470
    },
    {
      "epoch": 2.019429179223542,
      "grad_norm": 2.781323194503784,
      "learning_rate": 6.536188720276084e-05,
      "loss": 0.2315,
      "step": 28480
    },
    {
      "epoch": 2.020138273355788,
      "grad_norm": 1.3635939359664917,
      "learning_rate": 6.531461258450339e-05,
      "loss": 0.2939,
      "step": 28490
    },
    {
      "epoch": 2.020847367488034,
      "grad_norm": 1.598087191581726,
      "learning_rate": 6.526733796624593e-05,
      "loss": 0.2667,
      "step": 28500
    },
    {
      "epoch": 2.02155646162028,
      "grad_norm": 2.310422420501709,
      "learning_rate": 6.522006334798846e-05,
      "loss": 0.2415,
      "step": 28510
    },
    {
      "epoch": 2.022265555752526,
      "grad_norm": 1.1976768970489502,
      "learning_rate": 6.517278872973101e-05,
      "loss": 0.2319,
      "step": 28520
    },
    {
      "epoch": 2.0229746498847723,
      "grad_norm": 2.1324939727783203,
      "learning_rate": 6.512551411147355e-05,
      "loss": 0.2372,
      "step": 28530
    },
    {
      "epoch": 2.0236837440170183,
      "grad_norm": 1.0318868160247803,
      "learning_rate": 6.50782394932161e-05,
      "loss": 0.2328,
      "step": 28540
    },
    {
      "epoch": 2.0243928381492644,
      "grad_norm": 1.3562275171279907,
      "learning_rate": 6.503096487495864e-05,
      "loss": 0.2275,
      "step": 28550
    },
    {
      "epoch": 2.0251019322815105,
      "grad_norm": 2.6415560245513916,
      "learning_rate": 6.498369025670117e-05,
      "loss": 0.2529,
      "step": 28560
    },
    {
      "epoch": 2.0258110264137565,
      "grad_norm": 0.8191651701927185,
      "learning_rate": 6.493641563844372e-05,
      "loss": 0.2287,
      "step": 28570
    },
    {
      "epoch": 2.0265201205460026,
      "grad_norm": 0.7847105860710144,
      "learning_rate": 6.488914102018626e-05,
      "loss": 0.2267,
      "step": 28580
    },
    {
      "epoch": 2.0272292146782487,
      "grad_norm": 1.3510853052139282,
      "learning_rate": 6.484186640192881e-05,
      "loss": 0.2528,
      "step": 28590
    },
    {
      "epoch": 2.0279383088104947,
      "grad_norm": 2.0990450382232666,
      "learning_rate": 6.479459178367136e-05,
      "loss": 0.2289,
      "step": 28600
    },
    {
      "epoch": 2.028647402942741,
      "grad_norm": 0.857512354850769,
      "learning_rate": 6.47473171654139e-05,
      "loss": 0.252,
      "step": 28610
    },
    {
      "epoch": 2.029356497074987,
      "grad_norm": 0.6107717752456665,
      "learning_rate": 6.470004254715643e-05,
      "loss": 0.2334,
      "step": 28620
    },
    {
      "epoch": 2.030065591207233,
      "grad_norm": 0.7240951061248779,
      "learning_rate": 6.465276792889897e-05,
      "loss": 0.2472,
      "step": 28630
    },
    {
      "epoch": 2.030774685339479,
      "grad_norm": 1.1979926824569702,
      "learning_rate": 6.460549331064152e-05,
      "loss": 0.2346,
      "step": 28640
    },
    {
      "epoch": 2.031483779471725,
      "grad_norm": 1.0353602170944214,
      "learning_rate": 6.455821869238407e-05,
      "loss": 0.2456,
      "step": 28650
    },
    {
      "epoch": 2.0321928736039707,
      "grad_norm": 1.032780647277832,
      "learning_rate": 6.45109440741266e-05,
      "loss": 0.2663,
      "step": 28660
    },
    {
      "epoch": 2.032901967736217,
      "grad_norm": 1.7586867809295654,
      "learning_rate": 6.446366945586914e-05,
      "loss": 0.2122,
      "step": 28670
    },
    {
      "epoch": 2.033611061868463,
      "grad_norm": 1.2536038160324097,
      "learning_rate": 6.441639483761169e-05,
      "loss": 0.2372,
      "step": 28680
    },
    {
      "epoch": 2.034320156000709,
      "grad_norm": 1.0649868249893188,
      "learning_rate": 6.436912021935423e-05,
      "loss": 0.2318,
      "step": 28690
    },
    {
      "epoch": 2.035029250132955,
      "grad_norm": 1.246863842010498,
      "learning_rate": 6.432184560109678e-05,
      "loss": 0.2452,
      "step": 28700
    },
    {
      "epoch": 2.035738344265201,
      "grad_norm": 1.5927443504333496,
      "learning_rate": 6.427457098283933e-05,
      "loss": 0.266,
      "step": 28710
    },
    {
      "epoch": 2.036447438397447,
      "grad_norm": 1.2341676950454712,
      "learning_rate": 6.422729636458186e-05,
      "loss": 0.2562,
      "step": 28720
    },
    {
      "epoch": 2.037156532529693,
      "grad_norm": 0.9365014433860779,
      "learning_rate": 6.41800217463244e-05,
      "loss": 0.2558,
      "step": 28730
    },
    {
      "epoch": 2.0378656266619393,
      "grad_norm": 1.3958747386932373,
      "learning_rate": 6.413274712806694e-05,
      "loss": 0.229,
      "step": 28740
    },
    {
      "epoch": 2.0385747207941853,
      "grad_norm": 0.9684353470802307,
      "learning_rate": 6.408547250980948e-05,
      "loss": 0.2378,
      "step": 28750
    },
    {
      "epoch": 2.0392838149264314,
      "grad_norm": 0.8799896240234375,
      "learning_rate": 6.403819789155203e-05,
      "loss": 0.2685,
      "step": 28760
    },
    {
      "epoch": 2.0399929090586775,
      "grad_norm": 3.6740710735321045,
      "learning_rate": 6.399092327329457e-05,
      "loss": 0.269,
      "step": 28770
    },
    {
      "epoch": 2.0407020031909235,
      "grad_norm": 1.174438714981079,
      "learning_rate": 6.394364865503711e-05,
      "loss": 0.2363,
      "step": 28780
    },
    {
      "epoch": 2.0414110973231696,
      "grad_norm": 1.1004849672317505,
      "learning_rate": 6.389637403677966e-05,
      "loss": 0.2649,
      "step": 28790
    },
    {
      "epoch": 2.0421201914554157,
      "grad_norm": 1.6737442016601562,
      "learning_rate": 6.38490994185222e-05,
      "loss": 0.2385,
      "step": 28800
    },
    {
      "epoch": 2.0428292855876617,
      "grad_norm": 1.080159068107605,
      "learning_rate": 6.380182480026474e-05,
      "loss": 0.2534,
      "step": 28810
    },
    {
      "epoch": 2.043538379719908,
      "grad_norm": 0.9379512071609497,
      "learning_rate": 6.375455018200729e-05,
      "loss": 0.2175,
      "step": 28820
    },
    {
      "epoch": 2.044247473852154,
      "grad_norm": 0.9954947233200073,
      "learning_rate": 6.370727556374983e-05,
      "loss": 0.2177,
      "step": 28830
    },
    {
      "epoch": 2.0449565679844,
      "grad_norm": 4.909007549285889,
      "learning_rate": 6.366000094549237e-05,
      "loss": 0.2474,
      "step": 28840
    },
    {
      "epoch": 2.045665662116646,
      "grad_norm": 1.0262501239776611,
      "learning_rate": 6.36127263272349e-05,
      "loss": 0.2303,
      "step": 28850
    },
    {
      "epoch": 2.046374756248892,
      "grad_norm": 0.9834608435630798,
      "learning_rate": 6.356545170897745e-05,
      "loss": 0.2447,
      "step": 28860
    },
    {
      "epoch": 2.047083850381138,
      "grad_norm": 1.2021902799606323,
      "learning_rate": 6.351817709072e-05,
      "loss": 0.2461,
      "step": 28870
    },
    {
      "epoch": 2.0477929445133842,
      "grad_norm": 1.237883448600769,
      "learning_rate": 6.347090247246254e-05,
      "loss": 0.262,
      "step": 28880
    },
    {
      "epoch": 2.0485020386456303,
      "grad_norm": 1.0576027631759644,
      "learning_rate": 6.342362785420507e-05,
      "loss": 0.2396,
      "step": 28890
    },
    {
      "epoch": 2.0492111327778764,
      "grad_norm": 1.0227389335632324,
      "learning_rate": 6.337635323594762e-05,
      "loss": 0.2421,
      "step": 28900
    },
    {
      "epoch": 2.0499202269101224,
      "grad_norm": 1.1487082242965698,
      "learning_rate": 6.332907861769016e-05,
      "loss": 0.2068,
      "step": 28910
    },
    {
      "epoch": 2.0506293210423685,
      "grad_norm": 0.9392826557159424,
      "learning_rate": 6.328180399943271e-05,
      "loss": 0.2151,
      "step": 28920
    },
    {
      "epoch": 2.0513384151746146,
      "grad_norm": 0.9946120381355286,
      "learning_rate": 6.323452938117525e-05,
      "loss": 0.2431,
      "step": 28930
    },
    {
      "epoch": 2.0520475093068606,
      "grad_norm": 1.1184552907943726,
      "learning_rate": 6.31872547629178e-05,
      "loss": 0.2452,
      "step": 28940
    },
    {
      "epoch": 2.0527566034391067,
      "grad_norm": 3.081512212753296,
      "learning_rate": 6.313998014466033e-05,
      "loss": 0.264,
      "step": 28950
    },
    {
      "epoch": 2.0534656975713528,
      "grad_norm": 1.3776932954788208,
      "learning_rate": 6.309270552640287e-05,
      "loss": 0.2485,
      "step": 28960
    },
    {
      "epoch": 2.054174791703599,
      "grad_norm": 1.216651201248169,
      "learning_rate": 6.304543090814542e-05,
      "loss": 0.2378,
      "step": 28970
    },
    {
      "epoch": 2.054883885835845,
      "grad_norm": 0.9795972108840942,
      "learning_rate": 6.299815628988797e-05,
      "loss": 0.2216,
      "step": 28980
    },
    {
      "epoch": 2.0555929799680905,
      "grad_norm": 1.2884666919708252,
      "learning_rate": 6.29508816716305e-05,
      "loss": 0.2416,
      "step": 28990
    },
    {
      "epoch": 2.0563020741003366,
      "grad_norm": 1.5065935850143433,
      "learning_rate": 6.290360705337305e-05,
      "loss": 0.2326,
      "step": 29000
    },
    {
      "epoch": 2.0570111682325827,
      "grad_norm": 0.8581270575523376,
      "learning_rate": 6.285633243511559e-05,
      "loss": 0.2551,
      "step": 29010
    },
    {
      "epoch": 2.0577202623648287,
      "grad_norm": 0.7562877535820007,
      "learning_rate": 6.280905781685813e-05,
      "loss": 0.2469,
      "step": 29020
    },
    {
      "epoch": 2.058429356497075,
      "grad_norm": 1.2305045127868652,
      "learning_rate": 6.276178319860068e-05,
      "loss": 0.2598,
      "step": 29030
    },
    {
      "epoch": 2.059138450629321,
      "grad_norm": 1.078306794166565,
      "learning_rate": 6.271450858034321e-05,
      "loss": 0.2588,
      "step": 29040
    },
    {
      "epoch": 2.059847544761567,
      "grad_norm": 2.3956117630004883,
      "learning_rate": 6.266723396208576e-05,
      "loss": 0.2329,
      "step": 29050
    },
    {
      "epoch": 2.060556638893813,
      "grad_norm": 3.1868343353271484,
      "learning_rate": 6.26199593438283e-05,
      "loss": 0.2436,
      "step": 29060
    },
    {
      "epoch": 2.061265733026059,
      "grad_norm": 1.7956643104553223,
      "learning_rate": 6.257268472557084e-05,
      "loss": 0.2449,
      "step": 29070
    },
    {
      "epoch": 2.061974827158305,
      "grad_norm": 2.4692728519439697,
      "learning_rate": 6.252541010731339e-05,
      "loss": 0.2374,
      "step": 29080
    },
    {
      "epoch": 2.0626839212905512,
      "grad_norm": 1.0016855001449585,
      "learning_rate": 6.247813548905594e-05,
      "loss": 0.2424,
      "step": 29090
    },
    {
      "epoch": 2.0633930154227973,
      "grad_norm": 1.765848159790039,
      "learning_rate": 6.243086087079847e-05,
      "loss": 0.2859,
      "step": 29100
    },
    {
      "epoch": 2.0641021095550434,
      "grad_norm": 3.1917314529418945,
      "learning_rate": 6.238358625254102e-05,
      "loss": 0.2462,
      "step": 29110
    },
    {
      "epoch": 2.0648112036872894,
      "grad_norm": 1.0098909139633179,
      "learning_rate": 6.233631163428354e-05,
      "loss": 0.233,
      "step": 29120
    },
    {
      "epoch": 2.0655202978195355,
      "grad_norm": 1.2284908294677734,
      "learning_rate": 6.22890370160261e-05,
      "loss": 0.239,
      "step": 29130
    },
    {
      "epoch": 2.0662293919517816,
      "grad_norm": 0.9158666133880615,
      "learning_rate": 6.224176239776864e-05,
      "loss": 0.2404,
      "step": 29140
    },
    {
      "epoch": 2.0669384860840276,
      "grad_norm": 1.335572361946106,
      "learning_rate": 6.219448777951118e-05,
      "loss": 0.2535,
      "step": 29150
    },
    {
      "epoch": 2.0676475802162737,
      "grad_norm": 1.947960615158081,
      "learning_rate": 6.214721316125373e-05,
      "loss": 0.2684,
      "step": 29160
    },
    {
      "epoch": 2.0683566743485198,
      "grad_norm": 1.0113784074783325,
      "learning_rate": 6.209993854299627e-05,
      "loss": 0.2562,
      "step": 29170
    },
    {
      "epoch": 2.069065768480766,
      "grad_norm": 2.09570574760437,
      "learning_rate": 6.20526639247388e-05,
      "loss": 0.2405,
      "step": 29180
    },
    {
      "epoch": 2.069774862613012,
      "grad_norm": 1.5080859661102295,
      "learning_rate": 6.200538930648135e-05,
      "loss": 0.2144,
      "step": 29190
    },
    {
      "epoch": 2.070483956745258,
      "grad_norm": 1.3006572723388672,
      "learning_rate": 6.19581146882239e-05,
      "loss": 0.237,
      "step": 29200
    },
    {
      "epoch": 2.071193050877504,
      "grad_norm": 1.1971471309661865,
      "learning_rate": 6.191084006996644e-05,
      "loss": 0.2476,
      "step": 29210
    },
    {
      "epoch": 2.07190214500975,
      "grad_norm": 1.002643346786499,
      "learning_rate": 6.186356545170899e-05,
      "loss": 0.2729,
      "step": 29220
    },
    {
      "epoch": 2.072611239141996,
      "grad_norm": 0.9056338667869568,
      "learning_rate": 6.181629083345151e-05,
      "loss": 0.2187,
      "step": 29230
    },
    {
      "epoch": 2.0733203332742423,
      "grad_norm": 0.8665527105331421,
      "learning_rate": 6.176901621519406e-05,
      "loss": 0.2256,
      "step": 29240
    },
    {
      "epoch": 2.0740294274064883,
      "grad_norm": 2.6137888431549072,
      "learning_rate": 6.172174159693661e-05,
      "loss": 0.274,
      "step": 29250
    },
    {
      "epoch": 2.0747385215387344,
      "grad_norm": 1.1941559314727783,
      "learning_rate": 6.167446697867915e-05,
      "loss": 0.2713,
      "step": 29260
    },
    {
      "epoch": 2.0754476156709805,
      "grad_norm": 0.8103973865509033,
      "learning_rate": 6.16271923604217e-05,
      "loss": 0.2768,
      "step": 29270
    },
    {
      "epoch": 2.0761567098032265,
      "grad_norm": 0.9640453457832336,
      "learning_rate": 6.157991774216423e-05,
      "loss": 0.2415,
      "step": 29280
    },
    {
      "epoch": 2.0768658039354726,
      "grad_norm": 1.3077270984649658,
      "learning_rate": 6.153264312390677e-05,
      "loss": 0.2212,
      "step": 29290
    },
    {
      "epoch": 2.0775748980677187,
      "grad_norm": 0.898208737373352,
      "learning_rate": 6.148536850564932e-05,
      "loss": 0.2659,
      "step": 29300
    },
    {
      "epoch": 2.0782839921999647,
      "grad_norm": 1.8492592573165894,
      "learning_rate": 6.143809388739187e-05,
      "loss": 0.2438,
      "step": 29310
    },
    {
      "epoch": 2.078993086332211,
      "grad_norm": 0.966624915599823,
      "learning_rate": 6.13908192691344e-05,
      "loss": 0.2394,
      "step": 29320
    },
    {
      "epoch": 2.079702180464457,
      "grad_norm": 2.4247655868530273,
      "learning_rate": 6.134354465087696e-05,
      "loss": 0.25,
      "step": 29330
    },
    {
      "epoch": 2.0804112745967025,
      "grad_norm": 0.7438564300537109,
      "learning_rate": 6.129627003261948e-05,
      "loss": 0.2589,
      "step": 29340
    },
    {
      "epoch": 2.0811203687289486,
      "grad_norm": 1.0253517627716064,
      "learning_rate": 6.124899541436203e-05,
      "loss": 0.2351,
      "step": 29350
    },
    {
      "epoch": 2.0818294628611946,
      "grad_norm": 0.8342818021774292,
      "learning_rate": 6.120172079610458e-05,
      "loss": 0.2535,
      "step": 29360
    },
    {
      "epoch": 2.0825385569934407,
      "grad_norm": 1.8283257484436035,
      "learning_rate": 6.115444617784711e-05,
      "loss": 0.2595,
      "step": 29370
    },
    {
      "epoch": 2.0832476511256868,
      "grad_norm": 1.47128164768219,
      "learning_rate": 6.110717155958966e-05,
      "loss": 0.2254,
      "step": 29380
    },
    {
      "epoch": 2.083956745257933,
      "grad_norm": 0.7802992463111877,
      "learning_rate": 6.10598969413322e-05,
      "loss": 0.2464,
      "step": 29390
    },
    {
      "epoch": 2.084665839390179,
      "grad_norm": 2.0978569984436035,
      "learning_rate": 6.1012622323074744e-05,
      "loss": 0.2343,
      "step": 29400
    },
    {
      "epoch": 2.085374933522425,
      "grad_norm": 0.9615776538848877,
      "learning_rate": 6.096534770481729e-05,
      "loss": 0.2481,
      "step": 29410
    },
    {
      "epoch": 2.086084027654671,
      "grad_norm": 0.9206284880638123,
      "learning_rate": 6.091807308655982e-05,
      "loss": 0.2379,
      "step": 29420
    },
    {
      "epoch": 2.086793121786917,
      "grad_norm": 1.4228742122650146,
      "learning_rate": 6.087079846830237e-05,
      "loss": 0.2242,
      "step": 29430
    },
    {
      "epoch": 2.087502215919163,
      "grad_norm": 1.5263772010803223,
      "learning_rate": 6.0823523850044916e-05,
      "loss": 0.2535,
      "step": 29440
    },
    {
      "epoch": 2.0882113100514093,
      "grad_norm": 1.1884658336639404,
      "learning_rate": 6.077624923178745e-05,
      "loss": 0.2611,
      "step": 29450
    },
    {
      "epoch": 2.0889204041836553,
      "grad_norm": 0.9036626219749451,
      "learning_rate": 6.0728974613529995e-05,
      "loss": 0.2522,
      "step": 29460
    },
    {
      "epoch": 2.0896294983159014,
      "grad_norm": 0.8242747187614441,
      "learning_rate": 6.0681699995272545e-05,
      "loss": 0.2695,
      "step": 29470
    },
    {
      "epoch": 2.0903385924481475,
      "grad_norm": 1.7336375713348389,
      "learning_rate": 6.063442537701508e-05,
      "loss": 0.2606,
      "step": 29480
    },
    {
      "epoch": 2.0910476865803935,
      "grad_norm": 1.148154377937317,
      "learning_rate": 6.0587150758757625e-05,
      "loss": 0.2172,
      "step": 29490
    },
    {
      "epoch": 2.0917567807126396,
      "grad_norm": 1.2606927156448364,
      "learning_rate": 6.0539876140500174e-05,
      "loss": 0.2645,
      "step": 29500
    },
    {
      "epoch": 2.0924658748448857,
      "grad_norm": 1.255225419998169,
      "learning_rate": 6.049260152224271e-05,
      "loss": 0.2483,
      "step": 29510
    },
    {
      "epoch": 2.0931749689771317,
      "grad_norm": 2.3002583980560303,
      "learning_rate": 6.0445326903985254e-05,
      "loss": 0.2333,
      "step": 29520
    },
    {
      "epoch": 2.093884063109378,
      "grad_norm": 1.1180667877197266,
      "learning_rate": 6.039805228572779e-05,
      "loss": 0.2299,
      "step": 29530
    },
    {
      "epoch": 2.094593157241624,
      "grad_norm": 0.9229941368103027,
      "learning_rate": 6.035077766747034e-05,
      "loss": 0.2691,
      "step": 29540
    },
    {
      "epoch": 2.09530225137387,
      "grad_norm": 1.2895081043243408,
      "learning_rate": 6.030350304921288e-05,
      "loss": 0.2569,
      "step": 29550
    },
    {
      "epoch": 2.096011345506116,
      "grad_norm": 2.6785621643066406,
      "learning_rate": 6.025622843095542e-05,
      "loss": 0.2398,
      "step": 29560
    },
    {
      "epoch": 2.096720439638362,
      "grad_norm": 1.8669582605361938,
      "learning_rate": 6.020895381269796e-05,
      "loss": 0.2505,
      "step": 29570
    },
    {
      "epoch": 2.097429533770608,
      "grad_norm": 1.1137058734893799,
      "learning_rate": 6.016167919444051e-05,
      "loss": 0.2259,
      "step": 29580
    },
    {
      "epoch": 2.098138627902854,
      "grad_norm": 1.3444052934646606,
      "learning_rate": 6.011440457618305e-05,
      "loss": 0.2519,
      "step": 29590
    },
    {
      "epoch": 2.0988477220351003,
      "grad_norm": 2.968829870223999,
      "learning_rate": 6.006712995792559e-05,
      "loss": 0.2624,
      "step": 29600
    },
    {
      "epoch": 2.0995568161673464,
      "grad_norm": 1.0072575807571411,
      "learning_rate": 6.001985533966813e-05,
      "loss": 0.2676,
      "step": 29610
    },
    {
      "epoch": 2.1002659102995924,
      "grad_norm": 0.8456319570541382,
      "learning_rate": 5.997258072141068e-05,
      "loss": 0.2458,
      "step": 29620
    },
    {
      "epoch": 2.1009750044318385,
      "grad_norm": 1.0306761264801025,
      "learning_rate": 5.992530610315322e-05,
      "loss": 0.2329,
      "step": 29630
    },
    {
      "epoch": 2.1016840985640846,
      "grad_norm": 1.45444917678833,
      "learning_rate": 5.987803148489576e-05,
      "loss": 0.2575,
      "step": 29640
    },
    {
      "epoch": 2.1023931926963306,
      "grad_norm": 1.25063955783844,
      "learning_rate": 5.983075686663831e-05,
      "loss": 0.2577,
      "step": 29650
    },
    {
      "epoch": 2.1031022868285767,
      "grad_norm": 1.6916708946228027,
      "learning_rate": 5.978348224838085e-05,
      "loss": 0.2251,
      "step": 29660
    },
    {
      "epoch": 2.1038113809608228,
      "grad_norm": 1.0971299409866333,
      "learning_rate": 5.9736207630123386e-05,
      "loss": 0.2981,
      "step": 29670
    },
    {
      "epoch": 2.104520475093069,
      "grad_norm": 1.4071439504623413,
      "learning_rate": 5.9688933011865936e-05,
      "loss": 0.23,
      "step": 29680
    },
    {
      "epoch": 2.1052295692253145,
      "grad_norm": 1.2892695665359497,
      "learning_rate": 5.964165839360848e-05,
      "loss": 0.264,
      "step": 29690
    },
    {
      "epoch": 2.1059386633575605,
      "grad_norm": 1.8197740316390991,
      "learning_rate": 5.9594383775351016e-05,
      "loss": 0.2334,
      "step": 29700
    },
    {
      "epoch": 2.1066477574898066,
      "grad_norm": 0.9220862984657288,
      "learning_rate": 5.954710915709356e-05,
      "loss": 0.2321,
      "step": 29710
    },
    {
      "epoch": 2.1073568516220527,
      "grad_norm": 1.1494375467300415,
      "learning_rate": 5.9499834538836095e-05,
      "loss": 0.2193,
      "step": 29720
    },
    {
      "epoch": 2.1080659457542987,
      "grad_norm": 1.5764644145965576,
      "learning_rate": 5.9452559920578645e-05,
      "loss": 0.2261,
      "step": 29730
    },
    {
      "epoch": 2.108775039886545,
      "grad_norm": 1.1547435522079468,
      "learning_rate": 5.940528530232119e-05,
      "loss": 0.2285,
      "step": 29740
    },
    {
      "epoch": 2.109484134018791,
      "grad_norm": 1.0296915769577026,
      "learning_rate": 5.9358010684063724e-05,
      "loss": 0.2664,
      "step": 29750
    },
    {
      "epoch": 2.110193228151037,
      "grad_norm": 1.130103588104248,
      "learning_rate": 5.9310736065806274e-05,
      "loss": 0.2602,
      "step": 29760
    },
    {
      "epoch": 2.110902322283283,
      "grad_norm": 1.664488673210144,
      "learning_rate": 5.926346144754882e-05,
      "loss": 0.2657,
      "step": 29770
    },
    {
      "epoch": 2.111611416415529,
      "grad_norm": 0.8552115559577942,
      "learning_rate": 5.921618682929135e-05,
      "loss": 0.2415,
      "step": 29780
    },
    {
      "epoch": 2.112320510547775,
      "grad_norm": 0.9905089735984802,
      "learning_rate": 5.91689122110339e-05,
      "loss": 0.2357,
      "step": 29790
    },
    {
      "epoch": 2.113029604680021,
      "grad_norm": 1.1924304962158203,
      "learning_rate": 5.9121637592776446e-05,
      "loss": 0.2384,
      "step": 29800
    },
    {
      "epoch": 2.1137386988122673,
      "grad_norm": 1.201920986175537,
      "learning_rate": 5.907436297451898e-05,
      "loss": 0.2033,
      "step": 29810
    },
    {
      "epoch": 2.1144477929445133,
      "grad_norm": 1.665679931640625,
      "learning_rate": 5.9027088356261526e-05,
      "loss": 0.2423,
      "step": 29820
    },
    {
      "epoch": 2.1151568870767594,
      "grad_norm": 1.5050275325775146,
      "learning_rate": 5.897981373800406e-05,
      "loss": 0.244,
      "step": 29830
    },
    {
      "epoch": 2.1158659812090055,
      "grad_norm": 1.20610773563385,
      "learning_rate": 5.893253911974661e-05,
      "loss": 0.2267,
      "step": 29840
    },
    {
      "epoch": 2.1165750753412516,
      "grad_norm": 0.8007798194885254,
      "learning_rate": 5.8885264501489155e-05,
      "loss": 0.2555,
      "step": 29850
    },
    {
      "epoch": 2.1172841694734976,
      "grad_norm": 1.0741559267044067,
      "learning_rate": 5.883798988323169e-05,
      "loss": 0.2635,
      "step": 29860
    },
    {
      "epoch": 2.1179932636057437,
      "grad_norm": 1.7064847946166992,
      "learning_rate": 5.879071526497424e-05,
      "loss": 0.2419,
      "step": 29870
    },
    {
      "epoch": 2.1187023577379898,
      "grad_norm": 1.3271212577819824,
      "learning_rate": 5.8743440646716784e-05,
      "loss": 0.2273,
      "step": 29880
    },
    {
      "epoch": 2.119411451870236,
      "grad_norm": 0.8000016808509827,
      "learning_rate": 5.869616602845932e-05,
      "loss": 0.2825,
      "step": 29890
    },
    {
      "epoch": 2.120120546002482,
      "grad_norm": 0.9172964692115784,
      "learning_rate": 5.864889141020187e-05,
      "loss": 0.2419,
      "step": 29900
    },
    {
      "epoch": 2.120829640134728,
      "grad_norm": 1.7339832782745361,
      "learning_rate": 5.86016167919444e-05,
      "loss": 0.2783,
      "step": 29910
    },
    {
      "epoch": 2.121538734266974,
      "grad_norm": 1.0308524370193481,
      "learning_rate": 5.855434217368695e-05,
      "loss": 0.2303,
      "step": 29920
    },
    {
      "epoch": 2.12224782839922,
      "grad_norm": 1.8141804933547974,
      "learning_rate": 5.850706755542949e-05,
      "loss": 0.2356,
      "step": 29930
    },
    {
      "epoch": 2.122956922531466,
      "grad_norm": 1.1737421751022339,
      "learning_rate": 5.845979293717203e-05,
      "loss": 0.2372,
      "step": 29940
    },
    {
      "epoch": 2.1236660166637122,
      "grad_norm": 0.9618869423866272,
      "learning_rate": 5.841251831891458e-05,
      "loss": 0.2217,
      "step": 29950
    },
    {
      "epoch": 2.1243751107959583,
      "grad_norm": 1.1156185865402222,
      "learning_rate": 5.836524370065712e-05,
      "loss": 0.2057,
      "step": 29960
    },
    {
      "epoch": 2.1250842049282044,
      "grad_norm": 2.6463658809661865,
      "learning_rate": 5.831796908239966e-05,
      "loss": 0.258,
      "step": 29970
    },
    {
      "epoch": 2.1257932990604504,
      "grad_norm": 1.5900509357452393,
      "learning_rate": 5.827069446414221e-05,
      "loss": 0.266,
      "step": 29980
    },
    {
      "epoch": 2.1265023931926965,
      "grad_norm": 0.9050711393356323,
      "learning_rate": 5.822341984588475e-05,
      "loss": 0.2611,
      "step": 29990
    },
    {
      "epoch": 2.1272114873249426,
      "grad_norm": 0.8591293096542358,
      "learning_rate": 5.817614522762729e-05,
      "loss": 0.22,
      "step": 30000
    },
    {
      "epoch": 2.1279205814571887,
      "grad_norm": 1.0341691970825195,
      "learning_rate": 5.812887060936984e-05,
      "loss": 0.2442,
      "step": 30010
    },
    {
      "epoch": 2.1286296755894343,
      "grad_norm": 1.2073696851730347,
      "learning_rate": 5.808159599111237e-05,
      "loss": 0.2438,
      "step": 30020
    },
    {
      "epoch": 2.1293387697216803,
      "grad_norm": 1.1906468868255615,
      "learning_rate": 5.8034321372854916e-05,
      "loss": 0.2361,
      "step": 30030
    },
    {
      "epoch": 2.1300478638539264,
      "grad_norm": 0.9286103248596191,
      "learning_rate": 5.798704675459746e-05,
      "loss": 0.2454,
      "step": 30040
    },
    {
      "epoch": 2.1307569579861725,
      "grad_norm": 1.052674651145935,
      "learning_rate": 5.7939772136339996e-05,
      "loss": 0.2719,
      "step": 30050
    },
    {
      "epoch": 2.1314660521184186,
      "grad_norm": 0.9745470285415649,
      "learning_rate": 5.7892497518082546e-05,
      "loss": 0.2302,
      "step": 30060
    },
    {
      "epoch": 2.1321751462506646,
      "grad_norm": 1.311928629875183,
      "learning_rate": 5.784522289982509e-05,
      "loss": 0.2242,
      "step": 30070
    },
    {
      "epoch": 2.1328842403829107,
      "grad_norm": 1.5840351581573486,
      "learning_rate": 5.7797948281567625e-05,
      "loss": 0.2425,
      "step": 30080
    },
    {
      "epoch": 2.1335933345151568,
      "grad_norm": 1.1143814325332642,
      "learning_rate": 5.7750673663310175e-05,
      "loss": 0.239,
      "step": 30090
    },
    {
      "epoch": 2.134302428647403,
      "grad_norm": 1.041861653327942,
      "learning_rate": 5.770339904505271e-05,
      "loss": 0.2491,
      "step": 30100
    },
    {
      "epoch": 2.135011522779649,
      "grad_norm": 0.8855530619621277,
      "learning_rate": 5.7656124426795254e-05,
      "loss": 0.2325,
      "step": 30110
    },
    {
      "epoch": 2.135720616911895,
      "grad_norm": 1.1995190382003784,
      "learning_rate": 5.7608849808537804e-05,
      "loss": 0.2558,
      "step": 30120
    },
    {
      "epoch": 2.136429711044141,
      "grad_norm": 0.9135388731956482,
      "learning_rate": 5.7561575190280334e-05,
      "loss": 0.2647,
      "step": 30130
    },
    {
      "epoch": 2.137138805176387,
      "grad_norm": 1.0571839809417725,
      "learning_rate": 5.7514300572022883e-05,
      "loss": 0.2408,
      "step": 30140
    },
    {
      "epoch": 2.137847899308633,
      "grad_norm": 0.7876631021499634,
      "learning_rate": 5.7467025953765427e-05,
      "loss": 0.2387,
      "step": 30150
    },
    {
      "epoch": 2.1385569934408792,
      "grad_norm": 0.8730542659759521,
      "learning_rate": 5.741975133550796e-05,
      "loss": 0.2287,
      "step": 30160
    },
    {
      "epoch": 2.1392660875731253,
      "grad_norm": 1.030470848083496,
      "learning_rate": 5.737247671725051e-05,
      "loss": 0.24,
      "step": 30170
    },
    {
      "epoch": 2.1399751817053714,
      "grad_norm": 1.6221867799758911,
      "learning_rate": 5.7325202098993056e-05,
      "loss": 0.2508,
      "step": 30180
    },
    {
      "epoch": 2.1406842758376174,
      "grad_norm": 1.0994340181350708,
      "learning_rate": 5.727792748073559e-05,
      "loss": 0.261,
      "step": 30190
    },
    {
      "epoch": 2.1413933699698635,
      "grad_norm": 1.3039342164993286,
      "learning_rate": 5.723065286247814e-05,
      "loss": 0.2378,
      "step": 30200
    },
    {
      "epoch": 2.1421024641021096,
      "grad_norm": 1.2718555927276611,
      "learning_rate": 5.718337824422068e-05,
      "loss": 0.2438,
      "step": 30210
    },
    {
      "epoch": 2.1428115582343557,
      "grad_norm": 0.9932212233543396,
      "learning_rate": 5.713610362596322e-05,
      "loss": 0.2498,
      "step": 30220
    },
    {
      "epoch": 2.1435206523666017,
      "grad_norm": 1.5313467979431152,
      "learning_rate": 5.708882900770577e-05,
      "loss": 0.2282,
      "step": 30230
    },
    {
      "epoch": 2.144229746498848,
      "grad_norm": 0.9430537819862366,
      "learning_rate": 5.70415543894483e-05,
      "loss": 0.2335,
      "step": 30240
    },
    {
      "epoch": 2.144938840631094,
      "grad_norm": 1.443880558013916,
      "learning_rate": 5.699427977119085e-05,
      "loss": 0.2501,
      "step": 30250
    },
    {
      "epoch": 2.14564793476334,
      "grad_norm": 1.0297613143920898,
      "learning_rate": 5.69470051529334e-05,
      "loss": 0.2325,
      "step": 30260
    },
    {
      "epoch": 2.146357028895586,
      "grad_norm": 1.0509378910064697,
      "learning_rate": 5.689973053467593e-05,
      "loss": 0.2552,
      "step": 30270
    },
    {
      "epoch": 2.147066123027832,
      "grad_norm": 2.249950885772705,
      "learning_rate": 5.685245591641848e-05,
      "loss": 0.2409,
      "step": 30280
    },
    {
      "epoch": 2.147775217160078,
      "grad_norm": 0.7892433404922485,
      "learning_rate": 5.680518129816102e-05,
      "loss": 0.2509,
      "step": 30290
    },
    {
      "epoch": 2.148484311292324,
      "grad_norm": 1.0802758932113647,
      "learning_rate": 5.675790667990356e-05,
      "loss": 0.2337,
      "step": 30300
    },
    {
      "epoch": 2.1491934054245703,
      "grad_norm": 0.8994438052177429,
      "learning_rate": 5.671063206164611e-05,
      "loss": 0.2578,
      "step": 30310
    },
    {
      "epoch": 2.1499024995568163,
      "grad_norm": 1.0745015144348145,
      "learning_rate": 5.6663357443388645e-05,
      "loss": 0.2517,
      "step": 30320
    },
    {
      "epoch": 2.1506115936890624,
      "grad_norm": 0.9526563286781311,
      "learning_rate": 5.661608282513119e-05,
      "loss": 0.2472,
      "step": 30330
    },
    {
      "epoch": 2.1513206878213085,
      "grad_norm": 1.6755262613296509,
      "learning_rate": 5.656880820687374e-05,
      "loss": 0.2322,
      "step": 30340
    },
    {
      "epoch": 2.152029781953554,
      "grad_norm": 0.7188332676887512,
      "learning_rate": 5.6521533588616274e-05,
      "loss": 0.2852,
      "step": 30350
    },
    {
      "epoch": 2.1527388760858006,
      "grad_norm": 3.023402452468872,
      "learning_rate": 5.647425897035882e-05,
      "loss": 0.2315,
      "step": 30360
    },
    {
      "epoch": 2.1534479702180462,
      "grad_norm": 1.0460526943206787,
      "learning_rate": 5.642698435210137e-05,
      "loss": 0.2462,
      "step": 30370
    },
    {
      "epoch": 2.1541570643502923,
      "grad_norm": 1.1776776313781738,
      "learning_rate": 5.63797097338439e-05,
      "loss": 0.2474,
      "step": 30380
    },
    {
      "epoch": 2.1548661584825384,
      "grad_norm": 1.2475848197937012,
      "learning_rate": 5.633243511558645e-05,
      "loss": 0.2303,
      "step": 30390
    },
    {
      "epoch": 2.1555752526147844,
      "grad_norm": 0.9060238003730774,
      "learning_rate": 5.628516049732898e-05,
      "loss": 0.2236,
      "step": 30400
    },
    {
      "epoch": 2.1562843467470305,
      "grad_norm": 1.0040361881256104,
      "learning_rate": 5.6237885879071526e-05,
      "loss": 0.2261,
      "step": 30410
    },
    {
      "epoch": 2.1569934408792766,
      "grad_norm": 4.092061519622803,
      "learning_rate": 5.6190611260814076e-05,
      "loss": 0.2828,
      "step": 30420
    },
    {
      "epoch": 2.1577025350115226,
      "grad_norm": 0.9091020226478577,
      "learning_rate": 5.614333664255661e-05,
      "loss": 0.2298,
      "step": 30430
    },
    {
      "epoch": 2.1584116291437687,
      "grad_norm": 0.9990788102149963,
      "learning_rate": 5.6096062024299155e-05,
      "loss": 0.2301,
      "step": 30440
    },
    {
      "epoch": 2.159120723276015,
      "grad_norm": 1.4669890403747559,
      "learning_rate": 5.6048787406041705e-05,
      "loss": 0.2434,
      "step": 30450
    },
    {
      "epoch": 2.159829817408261,
      "grad_norm": 1.7166625261306763,
      "learning_rate": 5.600151278778424e-05,
      "loss": 0.2543,
      "step": 30460
    },
    {
      "epoch": 2.160538911540507,
      "grad_norm": 0.7288267612457275,
      "learning_rate": 5.5954238169526784e-05,
      "loss": 0.2344,
      "step": 30470
    },
    {
      "epoch": 2.161248005672753,
      "grad_norm": 2.126920461654663,
      "learning_rate": 5.5906963551269334e-05,
      "loss": 0.2311,
      "step": 30480
    },
    {
      "epoch": 2.161957099804999,
      "grad_norm": 1.8255906105041504,
      "learning_rate": 5.5859688933011864e-05,
      "loss": 0.2648,
      "step": 30490
    },
    {
      "epoch": 2.162666193937245,
      "grad_norm": 1.2246956825256348,
      "learning_rate": 5.5812414314754414e-05,
      "loss": 0.2531,
      "step": 30500
    },
    {
      "epoch": 2.163375288069491,
      "grad_norm": 1.2356629371643066,
      "learning_rate": 5.576513969649695e-05,
      "loss": 0.2478,
      "step": 30510
    },
    {
      "epoch": 2.1640843822017373,
      "grad_norm": 1.0839558839797974,
      "learning_rate": 5.571786507823949e-05,
      "loss": 0.2313,
      "step": 30520
    },
    {
      "epoch": 2.1647934763339833,
      "grad_norm": 0.6517597436904907,
      "learning_rate": 5.567059045998204e-05,
      "loss": 0.25,
      "step": 30530
    },
    {
      "epoch": 2.1655025704662294,
      "grad_norm": 1.645211100578308,
      "learning_rate": 5.562331584172458e-05,
      "loss": 0.2694,
      "step": 30540
    },
    {
      "epoch": 2.1662116645984755,
      "grad_norm": 6.43524694442749,
      "learning_rate": 5.557604122346712e-05,
      "loss": 0.2298,
      "step": 30550
    },
    {
      "epoch": 2.1669207587307215,
      "grad_norm": 1.1717863082885742,
      "learning_rate": 5.552876660520967e-05,
      "loss": 0.2735,
      "step": 30560
    },
    {
      "epoch": 2.1676298528629676,
      "grad_norm": 0.9703916907310486,
      "learning_rate": 5.548149198695221e-05,
      "loss": 0.2333,
      "step": 30570
    },
    {
      "epoch": 2.1683389469952137,
      "grad_norm": 1.6882810592651367,
      "learning_rate": 5.543421736869475e-05,
      "loss": 0.221,
      "step": 30580
    },
    {
      "epoch": 2.1690480411274597,
      "grad_norm": 1.3318051099777222,
      "learning_rate": 5.53869427504373e-05,
      "loss": 0.2533,
      "step": 30590
    },
    {
      "epoch": 2.169757135259706,
      "grad_norm": 0.9564707279205322,
      "learning_rate": 5.533966813217983e-05,
      "loss": 0.2544,
      "step": 30600
    },
    {
      "epoch": 2.170466229391952,
      "grad_norm": 1.3606767654418945,
      "learning_rate": 5.529239351392238e-05,
      "loss": 0.2111,
      "step": 30610
    },
    {
      "epoch": 2.171175323524198,
      "grad_norm": 0.9619858860969543,
      "learning_rate": 5.524511889566492e-05,
      "loss": 0.2312,
      "step": 30620
    },
    {
      "epoch": 2.171884417656444,
      "grad_norm": 1.3512269258499146,
      "learning_rate": 5.519784427740746e-05,
      "loss": 0.2347,
      "step": 30630
    },
    {
      "epoch": 2.17259351178869,
      "grad_norm": 1.6673341989517212,
      "learning_rate": 5.515056965915001e-05,
      "loss": 0.2719,
      "step": 30640
    },
    {
      "epoch": 2.173302605920936,
      "grad_norm": 1.1248642206192017,
      "learning_rate": 5.5103295040892546e-05,
      "loss": 0.2763,
      "step": 30650
    },
    {
      "epoch": 2.1740117000531822,
      "grad_norm": 1.367220163345337,
      "learning_rate": 5.505602042263509e-05,
      "loss": 0.2683,
      "step": 30660
    },
    {
      "epoch": 2.1747207941854283,
      "grad_norm": 1.0500420331954956,
      "learning_rate": 5.500874580437764e-05,
      "loss": 0.2554,
      "step": 30670
    },
    {
      "epoch": 2.1754298883176744,
      "grad_norm": 1.0740262269973755,
      "learning_rate": 5.4961471186120175e-05,
      "loss": 0.2431,
      "step": 30680
    },
    {
      "epoch": 2.1761389824499204,
      "grad_norm": 1.0263618230819702,
      "learning_rate": 5.491419656786272e-05,
      "loss": 0.253,
      "step": 30690
    },
    {
      "epoch": 2.176848076582166,
      "grad_norm": 1.2355399131774902,
      "learning_rate": 5.4866921949605255e-05,
      "loss": 0.2415,
      "step": 30700
    },
    {
      "epoch": 2.1775571707144126,
      "grad_norm": 1.2816834449768066,
      "learning_rate": 5.48196473313478e-05,
      "loss": 0.2333,
      "step": 30710
    },
    {
      "epoch": 2.178266264846658,
      "grad_norm": 1.0637153387069702,
      "learning_rate": 5.477237271309035e-05,
      "loss": 0.2504,
      "step": 30720
    },
    {
      "epoch": 2.1789753589789043,
      "grad_norm": 1.7533587217330933,
      "learning_rate": 5.4725098094832884e-05,
      "loss": 0.236,
      "step": 30730
    },
    {
      "epoch": 2.1796844531111503,
      "grad_norm": 1.1201748847961426,
      "learning_rate": 5.467782347657543e-05,
      "loss": 0.2616,
      "step": 30740
    },
    {
      "epoch": 2.1803935472433964,
      "grad_norm": 1.5429446697235107,
      "learning_rate": 5.463054885831798e-05,
      "loss": 0.2588,
      "step": 30750
    },
    {
      "epoch": 2.1811026413756425,
      "grad_norm": 0.9627746939659119,
      "learning_rate": 5.458327424006051e-05,
      "loss": 0.2214,
      "step": 30760
    },
    {
      "epoch": 2.1818117355078885,
      "grad_norm": 0.8781096935272217,
      "learning_rate": 5.4535999621803056e-05,
      "loss": 0.2382,
      "step": 30770
    },
    {
      "epoch": 2.1825208296401346,
      "grad_norm": 1.2363803386688232,
      "learning_rate": 5.4488725003545606e-05,
      "loss": 0.2486,
      "step": 30780
    },
    {
      "epoch": 2.1832299237723807,
      "grad_norm": 3.180224657058716,
      "learning_rate": 5.444145038528814e-05,
      "loss": 0.2365,
      "step": 30790
    },
    {
      "epoch": 2.1839390179046267,
      "grad_norm": 0.9332857131958008,
      "learning_rate": 5.4394175767030685e-05,
      "loss": 0.231,
      "step": 30800
    },
    {
      "epoch": 2.184648112036873,
      "grad_norm": 0.9886841773986816,
      "learning_rate": 5.434690114877322e-05,
      "loss": 0.2567,
      "step": 30810
    },
    {
      "epoch": 2.185357206169119,
      "grad_norm": 1.1951955556869507,
      "learning_rate": 5.4299626530515765e-05,
      "loss": 0.2331,
      "step": 30820
    },
    {
      "epoch": 2.186066300301365,
      "grad_norm": 1.0429954528808594,
      "learning_rate": 5.4252351912258315e-05,
      "loss": 0.2489,
      "step": 30830
    },
    {
      "epoch": 2.186775394433611,
      "grad_norm": 1.360396385192871,
      "learning_rate": 5.420507729400085e-05,
      "loss": 0.2601,
      "step": 30840
    },
    {
      "epoch": 2.187484488565857,
      "grad_norm": 1.8463551998138428,
      "learning_rate": 5.4157802675743394e-05,
      "loss": 0.2399,
      "step": 30850
    },
    {
      "epoch": 2.188193582698103,
      "grad_norm": 0.8746946454048157,
      "learning_rate": 5.4110528057485944e-05,
      "loss": 0.2279,
      "step": 30860
    },
    {
      "epoch": 2.1889026768303492,
      "grad_norm": 1.1618249416351318,
      "learning_rate": 5.406325343922848e-05,
      "loss": 0.2415,
      "step": 30870
    },
    {
      "epoch": 2.1896117709625953,
      "grad_norm": 1.6357580423355103,
      "learning_rate": 5.401597882097102e-05,
      "loss": 0.2462,
      "step": 30880
    },
    {
      "epoch": 2.1903208650948414,
      "grad_norm": 1.465247392654419,
      "learning_rate": 5.396870420271356e-05,
      "loss": 0.3008,
      "step": 30890
    },
    {
      "epoch": 2.1910299592270874,
      "grad_norm": 1.5104117393493652,
      "learning_rate": 5.392142958445611e-05,
      "loss": 0.2272,
      "step": 30900
    },
    {
      "epoch": 2.1917390533593335,
      "grad_norm": 1.8506523370742798,
      "learning_rate": 5.387415496619865e-05,
      "loss": 0.2851,
      "step": 30910
    },
    {
      "epoch": 2.1924481474915796,
      "grad_norm": 1.2383350133895874,
      "learning_rate": 5.382688034794119e-05,
      "loss": 0.2584,
      "step": 30920
    },
    {
      "epoch": 2.1931572416238256,
      "grad_norm": 1.1984678506851196,
      "learning_rate": 5.377960572968374e-05,
      "loss": 0.2237,
      "step": 30930
    },
    {
      "epoch": 2.1938663357560717,
      "grad_norm": 1.6060031652450562,
      "learning_rate": 5.373233111142628e-05,
      "loss": 0.2554,
      "step": 30940
    },
    {
      "epoch": 2.1945754298883178,
      "grad_norm": 1.3101576566696167,
      "learning_rate": 5.368505649316882e-05,
      "loss": 0.2334,
      "step": 30950
    },
    {
      "epoch": 2.195284524020564,
      "grad_norm": 1.017640233039856,
      "learning_rate": 5.363778187491136e-05,
      "loss": 0.2433,
      "step": 30960
    },
    {
      "epoch": 2.19599361815281,
      "grad_norm": 0.7526683807373047,
      "learning_rate": 5.359050725665391e-05,
      "loss": 0.2109,
      "step": 30970
    },
    {
      "epoch": 2.196702712285056,
      "grad_norm": 0.7245502471923828,
      "learning_rate": 5.354323263839645e-05,
      "loss": 0.2094,
      "step": 30980
    },
    {
      "epoch": 2.197411806417302,
      "grad_norm": 0.9517006874084473,
      "learning_rate": 5.349595802013899e-05,
      "loss": 0.2304,
      "step": 30990
    },
    {
      "epoch": 2.198120900549548,
      "grad_norm": 0.9505895972251892,
      "learning_rate": 5.3448683401881526e-05,
      "loss": 0.2419,
      "step": 31000
    },
    {
      "epoch": 2.198829994681794,
      "grad_norm": 2.28542160987854,
      "learning_rate": 5.3401408783624076e-05,
      "loss": 0.224,
      "step": 31010
    },
    {
      "epoch": 2.1995390888140403,
      "grad_norm": 0.9243953227996826,
      "learning_rate": 5.335413416536662e-05,
      "loss": 0.2145,
      "step": 31020
    },
    {
      "epoch": 2.2002481829462863,
      "grad_norm": 1.4681123495101929,
      "learning_rate": 5.3306859547109156e-05,
      "loss": 0.2332,
      "step": 31030
    },
    {
      "epoch": 2.2009572770785324,
      "grad_norm": 1.2058491706848145,
      "learning_rate": 5.3259584928851706e-05,
      "loss": 0.2405,
      "step": 31040
    },
    {
      "epoch": 2.201666371210778,
      "grad_norm": 2.988992929458618,
      "learning_rate": 5.321231031059425e-05,
      "loss": 0.2334,
      "step": 31050
    },
    {
      "epoch": 2.202375465343024,
      "grad_norm": 1.1333527565002441,
      "learning_rate": 5.3165035692336785e-05,
      "loss": 0.2414,
      "step": 31060
    },
    {
      "epoch": 2.20308455947527,
      "grad_norm": 1.4723279476165771,
      "learning_rate": 5.311776107407933e-05,
      "loss": 0.2237,
      "step": 31070
    },
    {
      "epoch": 2.2037936536075162,
      "grad_norm": 1.1852178573608398,
      "learning_rate": 5.307048645582188e-05,
      "loss": 0.2381,
      "step": 31080
    },
    {
      "epoch": 2.2045027477397623,
      "grad_norm": 0.82345050573349,
      "learning_rate": 5.3023211837564414e-05,
      "loss": 0.22,
      "step": 31090
    },
    {
      "epoch": 2.2052118418720084,
      "grad_norm": 1.4705718755722046,
      "learning_rate": 5.297593721930696e-05,
      "loss": 0.252,
      "step": 31100
    },
    {
      "epoch": 2.2059209360042544,
      "grad_norm": 1.1879220008850098,
      "learning_rate": 5.2928662601049493e-05,
      "loss": 0.2395,
      "step": 31110
    },
    {
      "epoch": 2.2066300301365005,
      "grad_norm": 1.064460039138794,
      "learning_rate": 5.288138798279204e-05,
      "loss": 0.2409,
      "step": 31120
    },
    {
      "epoch": 2.2073391242687466,
      "grad_norm": 0.9113345146179199,
      "learning_rate": 5.2834113364534586e-05,
      "loss": 0.2237,
      "step": 31130
    },
    {
      "epoch": 2.2080482184009926,
      "grad_norm": 1.9508183002471924,
      "learning_rate": 5.278683874627712e-05,
      "loss": 0.2705,
      "step": 31140
    },
    {
      "epoch": 2.2087573125332387,
      "grad_norm": 1.3065471649169922,
      "learning_rate": 5.273956412801967e-05,
      "loss": 0.244,
      "step": 31150
    },
    {
      "epoch": 2.2094664066654848,
      "grad_norm": 3.290355682373047,
      "learning_rate": 5.2692289509762216e-05,
      "loss": 0.2511,
      "step": 31160
    },
    {
      "epoch": 2.210175500797731,
      "grad_norm": 1.6162527799606323,
      "learning_rate": 5.264501489150475e-05,
      "loss": 0.2737,
      "step": 31170
    },
    {
      "epoch": 2.210884594929977,
      "grad_norm": 1.1429939270019531,
      "learning_rate": 5.2597740273247295e-05,
      "loss": 0.2186,
      "step": 31180
    },
    {
      "epoch": 2.211593689062223,
      "grad_norm": 0.8685960173606873,
      "learning_rate": 5.255046565498983e-05,
      "loss": 0.2118,
      "step": 31190
    },
    {
      "epoch": 2.212302783194469,
      "grad_norm": 1.342613697052002,
      "learning_rate": 5.250319103673238e-05,
      "loss": 0.2563,
      "step": 31200
    },
    {
      "epoch": 2.213011877326715,
      "grad_norm": 1.8671355247497559,
      "learning_rate": 5.2455916418474924e-05,
      "loss": 0.2164,
      "step": 31210
    },
    {
      "epoch": 2.213720971458961,
      "grad_norm": 0.9460815787315369,
      "learning_rate": 5.240864180021746e-05,
      "loss": 0.232,
      "step": 31220
    },
    {
      "epoch": 2.2144300655912073,
      "grad_norm": 1.0147839784622192,
      "learning_rate": 5.236136718196001e-05,
      "loss": 0.2531,
      "step": 31230
    },
    {
      "epoch": 2.2151391597234533,
      "grad_norm": 1.6532894372940063,
      "learning_rate": 5.231409256370255e-05,
      "loss": 0.2339,
      "step": 31240
    },
    {
      "epoch": 2.2158482538556994,
      "grad_norm": 1.8104033470153809,
      "learning_rate": 5.226681794544509e-05,
      "loss": 0.277,
      "step": 31250
    },
    {
      "epoch": 2.2165573479879455,
      "grad_norm": 0.8675766587257385,
      "learning_rate": 5.221954332718764e-05,
      "loss": 0.2373,
      "step": 31260
    },
    {
      "epoch": 2.2172664421201915,
      "grad_norm": 1.7303723096847534,
      "learning_rate": 5.217226870893018e-05,
      "loss": 0.2608,
      "step": 31270
    },
    {
      "epoch": 2.2179755362524376,
      "grad_norm": 0.989744246006012,
      "learning_rate": 5.212499409067272e-05,
      "loss": 0.2374,
      "step": 31280
    },
    {
      "epoch": 2.2186846303846837,
      "grad_norm": 1.7105540037155151,
      "learning_rate": 5.207771947241526e-05,
      "loss": 0.238,
      "step": 31290
    },
    {
      "epoch": 2.2193937245169297,
      "grad_norm": 0.973281741142273,
      "learning_rate": 5.20304448541578e-05,
      "loss": 0.2501,
      "step": 31300
    },
    {
      "epoch": 2.220102818649176,
      "grad_norm": 0.9030383229255676,
      "learning_rate": 5.198317023590035e-05,
      "loss": 0.2722,
      "step": 31310
    },
    {
      "epoch": 2.220811912781422,
      "grad_norm": 1.2084479331970215,
      "learning_rate": 5.193589561764289e-05,
      "loss": 0.2346,
      "step": 31320
    },
    {
      "epoch": 2.221521006913668,
      "grad_norm": 0.963988184928894,
      "learning_rate": 5.188862099938543e-05,
      "loss": 0.2276,
      "step": 31330
    },
    {
      "epoch": 2.222230101045914,
      "grad_norm": 1.1426832675933838,
      "learning_rate": 5.184134638112798e-05,
      "loss": 0.219,
      "step": 31340
    },
    {
      "epoch": 2.22293919517816,
      "grad_norm": 1.1078933477401733,
      "learning_rate": 5.179407176287052e-05,
      "loss": 0.2456,
      "step": 31350
    },
    {
      "epoch": 2.223648289310406,
      "grad_norm": 2.2304115295410156,
      "learning_rate": 5.174679714461306e-05,
      "loss": 0.2761,
      "step": 31360
    },
    {
      "epoch": 2.224357383442652,
      "grad_norm": 1.402559518814087,
      "learning_rate": 5.1699522526355606e-05,
      "loss": 0.2516,
      "step": 31370
    },
    {
      "epoch": 2.225066477574898,
      "grad_norm": 1.3324857950210571,
      "learning_rate": 5.1652247908098136e-05,
      "loss": 0.2306,
      "step": 31380
    },
    {
      "epoch": 2.2257755717071444,
      "grad_norm": 0.7628041505813599,
      "learning_rate": 5.1604973289840686e-05,
      "loss": 0.249,
      "step": 31390
    },
    {
      "epoch": 2.22648466583939,
      "grad_norm": 0.8332725763320923,
      "learning_rate": 5.155769867158323e-05,
      "loss": 0.2553,
      "step": 31400
    },
    {
      "epoch": 2.227193759971636,
      "grad_norm": 1.3402172327041626,
      "learning_rate": 5.1510424053325765e-05,
      "loss": 0.2563,
      "step": 31410
    },
    {
      "epoch": 2.227902854103882,
      "grad_norm": 0.8660373687744141,
      "learning_rate": 5.1463149435068315e-05,
      "loss": 0.2499,
      "step": 31420
    },
    {
      "epoch": 2.228611948236128,
      "grad_norm": 1.9078788757324219,
      "learning_rate": 5.141587481681086e-05,
      "loss": 0.2321,
      "step": 31430
    },
    {
      "epoch": 2.2293210423683743,
      "grad_norm": 0.8797650337219238,
      "learning_rate": 5.1368600198553394e-05,
      "loss": 0.2525,
      "step": 31440
    },
    {
      "epoch": 2.2300301365006203,
      "grad_norm": 1.3998483419418335,
      "learning_rate": 5.1321325580295944e-05,
      "loss": 0.2218,
      "step": 31450
    },
    {
      "epoch": 2.2307392306328664,
      "grad_norm": 1.3142364025115967,
      "learning_rate": 5.127405096203849e-05,
      "loss": 0.2051,
      "step": 31460
    },
    {
      "epoch": 2.2314483247651125,
      "grad_norm": 1.015661358833313,
      "learning_rate": 5.1226776343781024e-05,
      "loss": 0.2466,
      "step": 31470
    },
    {
      "epoch": 2.2321574188973585,
      "grad_norm": 1.6565228700637817,
      "learning_rate": 5.1179501725523573e-05,
      "loss": 0.2262,
      "step": 31480
    },
    {
      "epoch": 2.2328665130296046,
      "grad_norm": 1.0161494016647339,
      "learning_rate": 5.11322271072661e-05,
      "loss": 0.2488,
      "step": 31490
    },
    {
      "epoch": 2.2335756071618507,
      "grad_norm": 1.557630181312561,
      "learning_rate": 5.108495248900865e-05,
      "loss": 0.2536,
      "step": 31500
    },
    {
      "epoch": 2.2342847012940967,
      "grad_norm": 1.1275029182434082,
      "learning_rate": 5.10376778707512e-05,
      "loss": 0.2455,
      "step": 31510
    },
    {
      "epoch": 2.234993795426343,
      "grad_norm": 1.319146990776062,
      "learning_rate": 5.099040325249373e-05,
      "loss": 0.245,
      "step": 31520
    },
    {
      "epoch": 2.235702889558589,
      "grad_norm": 0.7691044807434082,
      "learning_rate": 5.094312863423628e-05,
      "loss": 0.2701,
      "step": 31530
    },
    {
      "epoch": 2.236411983690835,
      "grad_norm": 1.0361264944076538,
      "learning_rate": 5.0895854015978825e-05,
      "loss": 0.2241,
      "step": 31540
    },
    {
      "epoch": 2.237121077823081,
      "grad_norm": 1.4187285900115967,
      "learning_rate": 5.084857939772136e-05,
      "loss": 0.2553,
      "step": 31550
    },
    {
      "epoch": 2.237830171955327,
      "grad_norm": 0.9735483527183533,
      "learning_rate": 5.080130477946391e-05,
      "loss": 0.2065,
      "step": 31560
    },
    {
      "epoch": 2.238539266087573,
      "grad_norm": 1.8136917352676392,
      "learning_rate": 5.0754030161206454e-05,
      "loss": 0.2075,
      "step": 31570
    },
    {
      "epoch": 2.239248360219819,
      "grad_norm": 1.0557727813720703,
      "learning_rate": 5.070675554294899e-05,
      "loss": 0.2294,
      "step": 31580
    },
    {
      "epoch": 2.2399574543520653,
      "grad_norm": 2.2292962074279785,
      "learning_rate": 5.065948092469154e-05,
      "loss": 0.1925,
      "step": 31590
    },
    {
      "epoch": 2.2406665484843113,
      "grad_norm": 1.45541512966156,
      "learning_rate": 5.061220630643408e-05,
      "loss": 0.2738,
      "step": 31600
    },
    {
      "epoch": 2.2413756426165574,
      "grad_norm": 0.9502015113830566,
      "learning_rate": 5.056493168817662e-05,
      "loss": 0.2284,
      "step": 31610
    },
    {
      "epoch": 2.2420847367488035,
      "grad_norm": 0.9617035984992981,
      "learning_rate": 5.051765706991917e-05,
      "loss": 0.2433,
      "step": 31620
    },
    {
      "epoch": 2.2427938308810496,
      "grad_norm": 0.8654603958129883,
      "learning_rate": 5.04703824516617e-05,
      "loss": 0.2119,
      "step": 31630
    },
    {
      "epoch": 2.2435029250132956,
      "grad_norm": 1.0470287799835205,
      "learning_rate": 5.042310783340425e-05,
      "loss": 0.2305,
      "step": 31640
    },
    {
      "epoch": 2.2442120191455417,
      "grad_norm": 1.0869718790054321,
      "learning_rate": 5.037583321514679e-05,
      "loss": 0.2867,
      "step": 31650
    },
    {
      "epoch": 2.2449211132777878,
      "grad_norm": 1.461545705795288,
      "learning_rate": 5.032855859688933e-05,
      "loss": 0.2348,
      "step": 31660
    },
    {
      "epoch": 2.245630207410034,
      "grad_norm": 0.9799725413322449,
      "learning_rate": 5.028128397863188e-05,
      "loss": 0.262,
      "step": 31670
    },
    {
      "epoch": 2.24633930154228,
      "grad_norm": 1.472764253616333,
      "learning_rate": 5.0234009360374415e-05,
      "loss": 0.263,
      "step": 31680
    },
    {
      "epoch": 2.247048395674526,
      "grad_norm": 1.7922885417938232,
      "learning_rate": 5.018673474211696e-05,
      "loss": 0.2037,
      "step": 31690
    },
    {
      "epoch": 2.247757489806772,
      "grad_norm": 1.0306668281555176,
      "learning_rate": 5.013946012385951e-05,
      "loss": 0.2361,
      "step": 31700
    },
    {
      "epoch": 2.248466583939018,
      "grad_norm": 0.7598497867584229,
      "learning_rate": 5.0092185505602044e-05,
      "loss": 0.2384,
      "step": 31710
    },
    {
      "epoch": 2.249175678071264,
      "grad_norm": 1.0917553901672363,
      "learning_rate": 5.004491088734459e-05,
      "loss": 0.2125,
      "step": 31720
    },
    {
      "epoch": 2.24988477220351,
      "grad_norm": 1.1017682552337646,
      "learning_rate": 4.999763626908713e-05,
      "loss": 0.242,
      "step": 31730
    },
    {
      "epoch": 2.2505938663357563,
      "grad_norm": 2.8373470306396484,
      "learning_rate": 4.995036165082967e-05,
      "loss": 0.2678,
      "step": 31740
    },
    {
      "epoch": 2.251302960468002,
      "grad_norm": 1.2830421924591064,
      "learning_rate": 4.9903087032572216e-05,
      "loss": 0.2213,
      "step": 31750
    },
    {
      "epoch": 2.252012054600248,
      "grad_norm": 1.3160498142242432,
      "learning_rate": 4.985581241431476e-05,
      "loss": 0.226,
      "step": 31760
    },
    {
      "epoch": 2.252721148732494,
      "grad_norm": 1.5593432188034058,
      "learning_rate": 4.9808537796057295e-05,
      "loss": 0.2506,
      "step": 31770
    },
    {
      "epoch": 2.25343024286474,
      "grad_norm": 0.9815822839736938,
      "learning_rate": 4.9761263177799845e-05,
      "loss": 0.2344,
      "step": 31780
    },
    {
      "epoch": 2.254139336996986,
      "grad_norm": 1.0683084726333618,
      "learning_rate": 4.971398855954238e-05,
      "loss": 0.2414,
      "step": 31790
    },
    {
      "epoch": 2.2548484311292323,
      "grad_norm": 0.993118941783905,
      "learning_rate": 4.9666713941284925e-05,
      "loss": 0.261,
      "step": 31800
    },
    {
      "epoch": 2.2555575252614783,
      "grad_norm": 1.7995625734329224,
      "learning_rate": 4.961943932302747e-05,
      "loss": 0.2411,
      "step": 31810
    },
    {
      "epoch": 2.2562666193937244,
      "grad_norm": 0.9544017314910889,
      "learning_rate": 4.957216470477001e-05,
      "loss": 0.2108,
      "step": 31820
    },
    {
      "epoch": 2.2569757135259705,
      "grad_norm": 0.8167558908462524,
      "learning_rate": 4.9524890086512554e-05,
      "loss": 0.238,
      "step": 31830
    },
    {
      "epoch": 2.2576848076582166,
      "grad_norm": 0.663866400718689,
      "learning_rate": 4.94776154682551e-05,
      "loss": 0.2015,
      "step": 31840
    },
    {
      "epoch": 2.2583939017904626,
      "grad_norm": 0.9933221936225891,
      "learning_rate": 4.943034084999763e-05,
      "loss": 0.2308,
      "step": 31850
    },
    {
      "epoch": 2.2591029959227087,
      "grad_norm": 1.1094937324523926,
      "learning_rate": 4.938306623174018e-05,
      "loss": 0.264,
      "step": 31860
    },
    {
      "epoch": 2.2598120900549548,
      "grad_norm": 1.2398443222045898,
      "learning_rate": 4.9335791613482726e-05,
      "loss": 0.2281,
      "step": 31870
    },
    {
      "epoch": 2.260521184187201,
      "grad_norm": 0.9256218671798706,
      "learning_rate": 4.928851699522526e-05,
      "loss": 0.2363,
      "step": 31880
    },
    {
      "epoch": 2.261230278319447,
      "grad_norm": 0.9993144273757935,
      "learning_rate": 4.924124237696781e-05,
      "loss": 0.2491,
      "step": 31890
    },
    {
      "epoch": 2.261939372451693,
      "grad_norm": 1.1489111185073853,
      "learning_rate": 4.9193967758710355e-05,
      "loss": 0.2442,
      "step": 31900
    },
    {
      "epoch": 2.262648466583939,
      "grad_norm": 1.1536577939987183,
      "learning_rate": 4.914669314045289e-05,
      "loss": 0.2891,
      "step": 31910
    },
    {
      "epoch": 2.263357560716185,
      "grad_norm": 1.2915550470352173,
      "learning_rate": 4.9099418522195435e-05,
      "loss": 0.2585,
      "step": 31920
    },
    {
      "epoch": 2.264066654848431,
      "grad_norm": 2.257383108139038,
      "learning_rate": 4.905214390393798e-05,
      "loss": 0.2185,
      "step": 31930
    },
    {
      "epoch": 2.2647757489806772,
      "grad_norm": 1.433250069618225,
      "learning_rate": 4.900486928568052e-05,
      "loss": 0.2778,
      "step": 31940
    },
    {
      "epoch": 2.2654848431129233,
      "grad_norm": 1.037233829498291,
      "learning_rate": 4.8957594667423064e-05,
      "loss": 0.2351,
      "step": 31950
    },
    {
      "epoch": 2.2661939372451694,
      "grad_norm": 1.1575604677200317,
      "learning_rate": 4.89103200491656e-05,
      "loss": 0.2015,
      "step": 31960
    },
    {
      "epoch": 2.2669030313774154,
      "grad_norm": 3.478992462158203,
      "learning_rate": 4.886304543090815e-05,
      "loss": 0.2268,
      "step": 31970
    },
    {
      "epoch": 2.2676121255096615,
      "grad_norm": 1.2415595054626465,
      "learning_rate": 4.881577081265069e-05,
      "loss": 0.2479,
      "step": 31980
    },
    {
      "epoch": 2.2683212196419076,
      "grad_norm": 0.9905064702033997,
      "learning_rate": 4.876849619439323e-05,
      "loss": 0.2015,
      "step": 31990
    },
    {
      "epoch": 2.2690303137741537,
      "grad_norm": 0.8469210267066956,
      "learning_rate": 4.872122157613577e-05,
      "loss": 0.2392,
      "step": 32000
    },
    {
      "epoch": 2.2697394079063997,
      "grad_norm": 2.062560558319092,
      "learning_rate": 4.867394695787832e-05,
      "loss": 0.2386,
      "step": 32010
    },
    {
      "epoch": 2.270448502038646,
      "grad_norm": 1.052735447883606,
      "learning_rate": 4.862667233962086e-05,
      "loss": 0.2311,
      "step": 32020
    },
    {
      "epoch": 2.271157596170892,
      "grad_norm": 1.093367099761963,
      "learning_rate": 4.85793977213634e-05,
      "loss": 0.24,
      "step": 32030
    },
    {
      "epoch": 2.271866690303138,
      "grad_norm": 1.3988213539123535,
      "learning_rate": 4.8532123103105945e-05,
      "loss": 0.2413,
      "step": 32040
    },
    {
      "epoch": 2.272575784435384,
      "grad_norm": 0.8930617570877075,
      "learning_rate": 4.848484848484849e-05,
      "loss": 0.2522,
      "step": 32050
    },
    {
      "epoch": 2.2732848785676296,
      "grad_norm": 1.178305983543396,
      "learning_rate": 4.843757386659103e-05,
      "loss": 0.2653,
      "step": 32060
    },
    {
      "epoch": 2.273993972699876,
      "grad_norm": 0.9278902411460876,
      "learning_rate": 4.839029924833357e-05,
      "loss": 0.2608,
      "step": 32070
    },
    {
      "epoch": 2.2747030668321218,
      "grad_norm": 0.9782655239105225,
      "learning_rate": 4.834302463007612e-05,
      "loss": 0.2539,
      "step": 32080
    },
    {
      "epoch": 2.2754121609643683,
      "grad_norm": 2.820713520050049,
      "learning_rate": 4.829575001181866e-05,
      "loss": 0.2319,
      "step": 32090
    },
    {
      "epoch": 2.276121255096614,
      "grad_norm": 1.5695685148239136,
      "learning_rate": 4.8248475393561196e-05,
      "loss": 0.2272,
      "step": 32100
    },
    {
      "epoch": 2.27683034922886,
      "grad_norm": 1.169934630393982,
      "learning_rate": 4.820120077530374e-05,
      "loss": 0.2383,
      "step": 32110
    },
    {
      "epoch": 2.277539443361106,
      "grad_norm": 6.2715911865234375,
      "learning_rate": 4.815392615704629e-05,
      "loss": 0.234,
      "step": 32120
    },
    {
      "epoch": 2.278248537493352,
      "grad_norm": 1.306359052658081,
      "learning_rate": 4.8106651538788826e-05,
      "loss": 0.2293,
      "step": 32130
    },
    {
      "epoch": 2.278957631625598,
      "grad_norm": 0.7654742002487183,
      "learning_rate": 4.805937692053137e-05,
      "loss": 0.2514,
      "step": 32140
    },
    {
      "epoch": 2.2796667257578442,
      "grad_norm": 2.9646763801574707,
      "learning_rate": 4.801210230227391e-05,
      "loss": 0.2726,
      "step": 32150
    },
    {
      "epoch": 2.2803758198900903,
      "grad_norm": 0.8990118503570557,
      "learning_rate": 4.7964827684016455e-05,
      "loss": 0.2459,
      "step": 32160
    },
    {
      "epoch": 2.2810849140223364,
      "grad_norm": 1.7303619384765625,
      "learning_rate": 4.7917553065759e-05,
      "loss": 0.2754,
      "step": 32170
    },
    {
      "epoch": 2.2817940081545824,
      "grad_norm": 1.5716363191604614,
      "learning_rate": 4.787027844750154e-05,
      "loss": 0.2629,
      "step": 32180
    },
    {
      "epoch": 2.2825031022868285,
      "grad_norm": 1.3743499517440796,
      "learning_rate": 4.782300382924408e-05,
      "loss": 0.2524,
      "step": 32190
    },
    {
      "epoch": 2.2832121964190746,
      "grad_norm": 1.0463348627090454,
      "learning_rate": 4.777572921098663e-05,
      "loss": 0.2502,
      "step": 32200
    },
    {
      "epoch": 2.2839212905513206,
      "grad_norm": 1.051979422569275,
      "learning_rate": 4.772845459272916e-05,
      "loss": 0.2117,
      "step": 32210
    },
    {
      "epoch": 2.2846303846835667,
      "grad_norm": 1.1564806699752808,
      "learning_rate": 4.7681179974471706e-05,
      "loss": 0.2192,
      "step": 32220
    },
    {
      "epoch": 2.285339478815813,
      "grad_norm": 0.7413396239280701,
      "learning_rate": 4.7633905356214256e-05,
      "loss": 0.2215,
      "step": 32230
    },
    {
      "epoch": 2.286048572948059,
      "grad_norm": 0.9684166312217712,
      "learning_rate": 4.758663073795679e-05,
      "loss": 0.2567,
      "step": 32240
    },
    {
      "epoch": 2.286757667080305,
      "grad_norm": 1.455402135848999,
      "learning_rate": 4.7539356119699336e-05,
      "loss": 0.2278,
      "step": 32250
    },
    {
      "epoch": 2.287466761212551,
      "grad_norm": 1.0992029905319214,
      "learning_rate": 4.749208150144188e-05,
      "loss": 0.2544,
      "step": 32260
    },
    {
      "epoch": 2.288175855344797,
      "grad_norm": 0.9498759508132935,
      "learning_rate": 4.744480688318442e-05,
      "loss": 0.2427,
      "step": 32270
    },
    {
      "epoch": 2.288884949477043,
      "grad_norm": 2.3714425563812256,
      "learning_rate": 4.7397532264926965e-05,
      "loss": 0.2497,
      "step": 32280
    },
    {
      "epoch": 2.289594043609289,
      "grad_norm": 0.7964220643043518,
      "learning_rate": 4.735025764666951e-05,
      "loss": 0.2387,
      "step": 32290
    },
    {
      "epoch": 2.2903031377415353,
      "grad_norm": 0.7671242952346802,
      "learning_rate": 4.7302983028412044e-05,
      "loss": 0.2455,
      "step": 32300
    },
    {
      "epoch": 2.2910122318737813,
      "grad_norm": 1.0493147373199463,
      "learning_rate": 4.7255708410154594e-05,
      "loss": 0.2458,
      "step": 32310
    },
    {
      "epoch": 2.2917213260060274,
      "grad_norm": 0.9805482029914856,
      "learning_rate": 4.720843379189713e-05,
      "loss": 0.2412,
      "step": 32320
    },
    {
      "epoch": 2.2924304201382735,
      "grad_norm": 1.5106455087661743,
      "learning_rate": 4.7161159173639673e-05,
      "loss": 0.2561,
      "step": 32330
    },
    {
      "epoch": 2.2931395142705195,
      "grad_norm": 1.188882827758789,
      "learning_rate": 4.7113884555382216e-05,
      "loss": 0.2521,
      "step": 32340
    },
    {
      "epoch": 2.2938486084027656,
      "grad_norm": 2.1170926094055176,
      "learning_rate": 4.706660993712476e-05,
      "loss": 0.2337,
      "step": 32350
    },
    {
      "epoch": 2.2945577025350117,
      "grad_norm": 2.156647205352783,
      "learning_rate": 4.70193353188673e-05,
      "loss": 0.2423,
      "step": 32360
    },
    {
      "epoch": 2.2952667966672577,
      "grad_norm": 1.328957200050354,
      "learning_rate": 4.6972060700609846e-05,
      "loss": 0.2785,
      "step": 32370
    },
    {
      "epoch": 2.295975890799504,
      "grad_norm": 0.9415785670280457,
      "learning_rate": 4.692478608235239e-05,
      "loss": 0.2542,
      "step": 32380
    },
    {
      "epoch": 2.2966849849317494,
      "grad_norm": 1.3072870969772339,
      "learning_rate": 4.687751146409493e-05,
      "loss": 0.2338,
      "step": 32390
    },
    {
      "epoch": 2.297394079063996,
      "grad_norm": 2.5852737426757812,
      "learning_rate": 4.6830236845837475e-05,
      "loss": 0.2394,
      "step": 32400
    },
    {
      "epoch": 2.2981031731962416,
      "grad_norm": 1.7440550327301025,
      "learning_rate": 4.678296222758001e-05,
      "loss": 0.2372,
      "step": 32410
    },
    {
      "epoch": 2.298812267328488,
      "grad_norm": 1.3327250480651855,
      "learning_rate": 4.673568760932256e-05,
      "loss": 0.2275,
      "step": 32420
    },
    {
      "epoch": 2.2995213614607337,
      "grad_norm": 1.1490720510482788,
      "learning_rate": 4.66884129910651e-05,
      "loss": 0.2147,
      "step": 32430
    },
    {
      "epoch": 2.30023045559298,
      "grad_norm": 1.8492467403411865,
      "learning_rate": 4.664113837280764e-05,
      "loss": 0.2366,
      "step": 32440
    },
    {
      "epoch": 2.300939549725226,
      "grad_norm": 0.8757996559143066,
      "learning_rate": 4.6593863754550183e-05,
      "loss": 0.2438,
      "step": 32450
    },
    {
      "epoch": 2.301648643857472,
      "grad_norm": 1.4210736751556396,
      "learning_rate": 4.6546589136292727e-05,
      "loss": 0.2251,
      "step": 32460
    },
    {
      "epoch": 2.302357737989718,
      "grad_norm": 1.0283198356628418,
      "learning_rate": 4.649931451803527e-05,
      "loss": 0.2333,
      "step": 32470
    },
    {
      "epoch": 2.303066832121964,
      "grad_norm": 0.9109870791435242,
      "learning_rate": 4.645203989977781e-05,
      "loss": 0.2088,
      "step": 32480
    },
    {
      "epoch": 2.30377592625421,
      "grad_norm": 1.1476994752883911,
      "learning_rate": 4.640476528152035e-05,
      "loss": 0.2362,
      "step": 32490
    },
    {
      "epoch": 2.304485020386456,
      "grad_norm": 1.2486987113952637,
      "learning_rate": 4.63574906632629e-05,
      "loss": 0.2193,
      "step": 32500
    },
    {
      "epoch": 2.3051941145187023,
      "grad_norm": 0.8269695043563843,
      "learning_rate": 4.631021604500544e-05,
      "loss": 0.2263,
      "step": 32510
    },
    {
      "epoch": 2.3059032086509483,
      "grad_norm": 2.219538688659668,
      "learning_rate": 4.626294142674798e-05,
      "loss": 0.255,
      "step": 32520
    },
    {
      "epoch": 2.3066123027831944,
      "grad_norm": 0.9752736687660217,
      "learning_rate": 4.621566680849052e-05,
      "loss": 0.2003,
      "step": 32530
    },
    {
      "epoch": 2.3073213969154405,
      "grad_norm": 0.9423645734786987,
      "learning_rate": 4.6168392190233064e-05,
      "loss": 0.2651,
      "step": 32540
    },
    {
      "epoch": 2.3080304910476865,
      "grad_norm": 2.025730609893799,
      "learning_rate": 4.612111757197561e-05,
      "loss": 0.2414,
      "step": 32550
    },
    {
      "epoch": 2.3087395851799326,
      "grad_norm": 0.9451714754104614,
      "learning_rate": 4.607384295371815e-05,
      "loss": 0.2458,
      "step": 32560
    },
    {
      "epoch": 2.3094486793121787,
      "grad_norm": 0.981733500957489,
      "learning_rate": 4.6026568335460694e-05,
      "loss": 0.2148,
      "step": 32570
    },
    {
      "epoch": 2.3101577734444247,
      "grad_norm": 1.539760947227478,
      "learning_rate": 4.5979293717203237e-05,
      "loss": 0.2543,
      "step": 32580
    },
    {
      "epoch": 2.310866867576671,
      "grad_norm": 0.9559736847877502,
      "learning_rate": 4.593201909894578e-05,
      "loss": 0.2906,
      "step": 32590
    },
    {
      "epoch": 2.311575961708917,
      "grad_norm": 1.347956657409668,
      "learning_rate": 4.5884744480688316e-05,
      "loss": 0.2262,
      "step": 32600
    },
    {
      "epoch": 2.312285055841163,
      "grad_norm": 1.0474050045013428,
      "learning_rate": 4.5837469862430866e-05,
      "loss": 0.2259,
      "step": 32610
    },
    {
      "epoch": 2.312994149973409,
      "grad_norm": 1.046555995941162,
      "learning_rate": 4.579019524417341e-05,
      "loss": 0.2355,
      "step": 32620
    },
    {
      "epoch": 2.313703244105655,
      "grad_norm": 0.7039837837219238,
      "learning_rate": 4.5742920625915945e-05,
      "loss": 0.2304,
      "step": 32630
    },
    {
      "epoch": 2.314412338237901,
      "grad_norm": 0.9926034808158875,
      "learning_rate": 4.569564600765849e-05,
      "loss": 0.2307,
      "step": 32640
    },
    {
      "epoch": 2.3151214323701472,
      "grad_norm": 1.3656362295150757,
      "learning_rate": 4.564837138940103e-05,
      "loss": 0.2756,
      "step": 32650
    },
    {
      "epoch": 2.3158305265023933,
      "grad_norm": 0.9869436621665955,
      "learning_rate": 4.5601096771143574e-05,
      "loss": 0.233,
      "step": 32660
    },
    {
      "epoch": 2.3165396206346394,
      "grad_norm": 1.3480674028396606,
      "learning_rate": 4.555382215288612e-05,
      "loss": 0.2492,
      "step": 32670
    },
    {
      "epoch": 2.3172487147668854,
      "grad_norm": 1.4006010293960571,
      "learning_rate": 4.550654753462866e-05,
      "loss": 0.2412,
      "step": 32680
    },
    {
      "epoch": 2.3179578088991315,
      "grad_norm": 1.236264705657959,
      "learning_rate": 4.5459272916371204e-05,
      "loss": 0.2478,
      "step": 32690
    },
    {
      "epoch": 2.3186669030313776,
      "grad_norm": 1.6379495859146118,
      "learning_rate": 4.541199829811375e-05,
      "loss": 0.2401,
      "step": 32700
    },
    {
      "epoch": 2.3193759971636236,
      "grad_norm": 1.1454299688339233,
      "learning_rate": 4.536472367985628e-05,
      "loss": 0.2327,
      "step": 32710
    },
    {
      "epoch": 2.3200850912958697,
      "grad_norm": 1.5016783475875854,
      "learning_rate": 4.531744906159883e-05,
      "loss": 0.2712,
      "step": 32720
    },
    {
      "epoch": 2.3207941854281158,
      "grad_norm": 1.2086609601974487,
      "learning_rate": 4.5270174443341376e-05,
      "loss": 0.2771,
      "step": 32730
    },
    {
      "epoch": 2.3215032795603614,
      "grad_norm": 0.7154223918914795,
      "learning_rate": 4.522289982508391e-05,
      "loss": 0.218,
      "step": 32740
    },
    {
      "epoch": 2.322212373692608,
      "grad_norm": 0.9939476251602173,
      "learning_rate": 4.5175625206826455e-05,
      "loss": 0.2595,
      "step": 32750
    },
    {
      "epoch": 2.3229214678248535,
      "grad_norm": 0.9005662202835083,
      "learning_rate": 4.5128350588569e-05,
      "loss": 0.2383,
      "step": 32760
    },
    {
      "epoch": 2.3236305619571,
      "grad_norm": 1.121872067451477,
      "learning_rate": 4.508107597031154e-05,
      "loss": 0.2361,
      "step": 32770
    },
    {
      "epoch": 2.3243396560893457,
      "grad_norm": 0.9971063733100891,
      "learning_rate": 4.5033801352054084e-05,
      "loss": 0.2627,
      "step": 32780
    },
    {
      "epoch": 2.3250487502215917,
      "grad_norm": 1.0252605676651,
      "learning_rate": 4.498652673379663e-05,
      "loss": 0.2135,
      "step": 32790
    },
    {
      "epoch": 2.325757844353838,
      "grad_norm": 2.6642229557037354,
      "learning_rate": 4.493925211553917e-05,
      "loss": 0.2382,
      "step": 32800
    },
    {
      "epoch": 2.326466938486084,
      "grad_norm": 2.495891571044922,
      "learning_rate": 4.4891977497281714e-05,
      "loss": 0.2203,
      "step": 32810
    },
    {
      "epoch": 2.32717603261833,
      "grad_norm": 1.7135827541351318,
      "learning_rate": 4.484470287902425e-05,
      "loss": 0.265,
      "step": 32820
    },
    {
      "epoch": 2.327885126750576,
      "grad_norm": 1.0613489151000977,
      "learning_rate": 4.479742826076679e-05,
      "loss": 0.2418,
      "step": 32830
    },
    {
      "epoch": 2.328594220882822,
      "grad_norm": 2.191760540008545,
      "learning_rate": 4.475015364250934e-05,
      "loss": 0.2449,
      "step": 32840
    },
    {
      "epoch": 2.329303315015068,
      "grad_norm": 0.9616583585739136,
      "learning_rate": 4.470287902425188e-05,
      "loss": 0.2579,
      "step": 32850
    },
    {
      "epoch": 2.3300124091473142,
      "grad_norm": 1.1014364957809448,
      "learning_rate": 4.465560440599442e-05,
      "loss": 0.2355,
      "step": 32860
    },
    {
      "epoch": 2.3307215032795603,
      "grad_norm": 1.4208875894546509,
      "learning_rate": 4.460832978773697e-05,
      "loss": 0.243,
      "step": 32870
    },
    {
      "epoch": 2.3314305974118064,
      "grad_norm": 0.8891164660453796,
      "learning_rate": 4.456105516947951e-05,
      "loss": 0.2397,
      "step": 32880
    },
    {
      "epoch": 2.3321396915440524,
      "grad_norm": 1.9554730653762817,
      "learning_rate": 4.451378055122205e-05,
      "loss": 0.2438,
      "step": 32890
    },
    {
      "epoch": 2.3328487856762985,
      "grad_norm": 1.9673818349838257,
      "learning_rate": 4.4466505932964594e-05,
      "loss": 0.2281,
      "step": 32900
    },
    {
      "epoch": 2.3335578798085446,
      "grad_norm": 2.3060290813446045,
      "learning_rate": 4.441923131470714e-05,
      "loss": 0.2668,
      "step": 32910
    },
    {
      "epoch": 2.3342669739407906,
      "grad_norm": 1.99485182762146,
      "learning_rate": 4.437195669644968e-05,
      "loss": 0.2636,
      "step": 32920
    },
    {
      "epoch": 2.3349760680730367,
      "grad_norm": 0.8290008902549744,
      "learning_rate": 4.432468207819222e-05,
      "loss": 0.2231,
      "step": 32930
    },
    {
      "epoch": 2.3356851622052828,
      "grad_norm": 1.315596580505371,
      "learning_rate": 4.427740745993476e-05,
      "loss": 0.2534,
      "step": 32940
    },
    {
      "epoch": 2.336394256337529,
      "grad_norm": 1.6445640325546265,
      "learning_rate": 4.423013284167731e-05,
      "loss": 0.2275,
      "step": 32950
    },
    {
      "epoch": 2.337103350469775,
      "grad_norm": 0.9280088543891907,
      "learning_rate": 4.4182858223419846e-05,
      "loss": 0.2353,
      "step": 32960
    },
    {
      "epoch": 2.337812444602021,
      "grad_norm": 1.142052412033081,
      "learning_rate": 4.413558360516239e-05,
      "loss": 0.245,
      "step": 32970
    },
    {
      "epoch": 2.338521538734267,
      "grad_norm": 0.9487666487693787,
      "learning_rate": 4.408830898690493e-05,
      "loss": 0.2412,
      "step": 32980
    },
    {
      "epoch": 2.339230632866513,
      "grad_norm": 1.1168111562728882,
      "learning_rate": 4.4041034368647475e-05,
      "loss": 0.2296,
      "step": 32990
    },
    {
      "epoch": 2.339939726998759,
      "grad_norm": 0.9079869389533997,
      "learning_rate": 4.399375975039002e-05,
      "loss": 0.2592,
      "step": 33000
    },
    {
      "epoch": 2.3406488211310053,
      "grad_norm": 0.7491257786750793,
      "learning_rate": 4.394648513213256e-05,
      "loss": 0.2316,
      "step": 33010
    },
    {
      "epoch": 2.3413579152632513,
      "grad_norm": 1.0812040567398071,
      "learning_rate": 4.3899210513875105e-05,
      "loss": 0.2424,
      "step": 33020
    },
    {
      "epoch": 2.3420670093954974,
      "grad_norm": 1.1516059637069702,
      "learning_rate": 4.385193589561765e-05,
      "loss": 0.2293,
      "step": 33030
    },
    {
      "epoch": 2.3427761035277435,
      "grad_norm": 0.8894214630126953,
      "learning_rate": 4.3804661277360184e-05,
      "loss": 0.2188,
      "step": 33040
    },
    {
      "epoch": 2.3434851976599895,
      "grad_norm": 1.261181354522705,
      "learning_rate": 4.375738665910273e-05,
      "loss": 0.2346,
      "step": 33050
    },
    {
      "epoch": 2.3441942917922356,
      "grad_norm": 0.7387579083442688,
      "learning_rate": 4.371011204084528e-05,
      "loss": 0.2464,
      "step": 33060
    },
    {
      "epoch": 2.3449033859244817,
      "grad_norm": 1.2954721450805664,
      "learning_rate": 4.366283742258781e-05,
      "loss": 0.2817,
      "step": 33070
    },
    {
      "epoch": 2.3456124800567277,
      "grad_norm": 0.8715137839317322,
      "learning_rate": 4.3615562804330356e-05,
      "loss": 0.229,
      "step": 33080
    },
    {
      "epoch": 2.3463215741889734,
      "grad_norm": 1.112563967704773,
      "learning_rate": 4.35682881860729e-05,
      "loss": 0.2242,
      "step": 33090
    },
    {
      "epoch": 2.34703066832122,
      "grad_norm": 0.9858121275901794,
      "learning_rate": 4.352101356781544e-05,
      "loss": 0.2345,
      "step": 33100
    },
    {
      "epoch": 2.3477397624534655,
      "grad_norm": 0.9311198592185974,
      "learning_rate": 4.3473738949557985e-05,
      "loss": 0.2561,
      "step": 33110
    },
    {
      "epoch": 2.3484488565857116,
      "grad_norm": 1.0705842971801758,
      "learning_rate": 4.342646433130053e-05,
      "loss": 0.233,
      "step": 33120
    },
    {
      "epoch": 2.3491579507179576,
      "grad_norm": 0.9649897217750549,
      "learning_rate": 4.3379189713043065e-05,
      "loss": 0.2627,
      "step": 33130
    },
    {
      "epoch": 2.3498670448502037,
      "grad_norm": 1.3600696325302124,
      "learning_rate": 4.3331915094785615e-05,
      "loss": 0.2554,
      "step": 33140
    },
    {
      "epoch": 2.3505761389824498,
      "grad_norm": 0.925636351108551,
      "learning_rate": 4.328464047652816e-05,
      "loss": 0.2287,
      "step": 33150
    },
    {
      "epoch": 2.351285233114696,
      "grad_norm": 1.107313632965088,
      "learning_rate": 4.3237365858270694e-05,
      "loss": 0.272,
      "step": 33160
    },
    {
      "epoch": 2.351994327246942,
      "grad_norm": 1.0856965780258179,
      "learning_rate": 4.319009124001324e-05,
      "loss": 0.2541,
      "step": 33170
    },
    {
      "epoch": 2.352703421379188,
      "grad_norm": 0.9879359006881714,
      "learning_rate": 4.314281662175578e-05,
      "loss": 0.1948,
      "step": 33180
    },
    {
      "epoch": 2.353412515511434,
      "grad_norm": 1.3946912288665771,
      "learning_rate": 4.309554200349832e-05,
      "loss": 0.2222,
      "step": 33190
    },
    {
      "epoch": 2.35412160964368,
      "grad_norm": 2.318406343460083,
      "learning_rate": 4.3048267385240866e-05,
      "loss": 0.2229,
      "step": 33200
    },
    {
      "epoch": 2.354830703775926,
      "grad_norm": 0.8999476432800293,
      "learning_rate": 4.300099276698341e-05,
      "loss": 0.2303,
      "step": 33210
    },
    {
      "epoch": 2.3555397979081723,
      "grad_norm": 1.7399190664291382,
      "learning_rate": 4.295371814872595e-05,
      "loss": 0.2592,
      "step": 33220
    },
    {
      "epoch": 2.3562488920404183,
      "grad_norm": 1.2722206115722656,
      "learning_rate": 4.2906443530468495e-05,
      "loss": 0.2516,
      "step": 33230
    },
    {
      "epoch": 2.3569579861726644,
      "grad_norm": 1.0283582210540771,
      "learning_rate": 4.285916891221103e-05,
      "loss": 0.2594,
      "step": 33240
    },
    {
      "epoch": 2.3576670803049105,
      "grad_norm": 1.5974414348602295,
      "learning_rate": 4.281189429395358e-05,
      "loss": 0.2415,
      "step": 33250
    },
    {
      "epoch": 2.3583761744371565,
      "grad_norm": 1.238968849182129,
      "learning_rate": 4.2764619675696125e-05,
      "loss": 0.2211,
      "step": 33260
    },
    {
      "epoch": 2.3590852685694026,
      "grad_norm": 2.6524782180786133,
      "learning_rate": 4.271734505743866e-05,
      "loss": 0.2828,
      "step": 33270
    },
    {
      "epoch": 2.3597943627016487,
      "grad_norm": 1.051918625831604,
      "learning_rate": 4.2670070439181204e-05,
      "loss": 0.2414,
      "step": 33280
    },
    {
      "epoch": 2.3605034568338947,
      "grad_norm": 1.0110526084899902,
      "learning_rate": 4.262279582092375e-05,
      "loss": 0.2211,
      "step": 33290
    },
    {
      "epoch": 2.361212550966141,
      "grad_norm": 0.7249163389205933,
      "learning_rate": 4.257552120266629e-05,
      "loss": 0.2445,
      "step": 33300
    },
    {
      "epoch": 2.361921645098387,
      "grad_norm": 1.1072889566421509,
      "learning_rate": 4.252824658440883e-05,
      "loss": 0.2461,
      "step": 33310
    },
    {
      "epoch": 2.362630739230633,
      "grad_norm": 1.0601005554199219,
      "learning_rate": 4.248097196615137e-05,
      "loss": 0.2297,
      "step": 33320
    },
    {
      "epoch": 2.363339833362879,
      "grad_norm": 1.0913125276565552,
      "learning_rate": 4.243369734789392e-05,
      "loss": 0.2574,
      "step": 33330
    },
    {
      "epoch": 2.364048927495125,
      "grad_norm": 0.9195890426635742,
      "learning_rate": 4.238642272963646e-05,
      "loss": 0.2397,
      "step": 33340
    },
    {
      "epoch": 2.364758021627371,
      "grad_norm": 0.973588228225708,
      "learning_rate": 4.2339148111379e-05,
      "loss": 0.2449,
      "step": 33350
    },
    {
      "epoch": 2.365467115759617,
      "grad_norm": 1.0085480213165283,
      "learning_rate": 4.229187349312155e-05,
      "loss": 0.2317,
      "step": 33360
    },
    {
      "epoch": 2.3661762098918633,
      "grad_norm": 1.8470563888549805,
      "learning_rate": 4.224459887486409e-05,
      "loss": 0.2239,
      "step": 33370
    },
    {
      "epoch": 2.3668853040241093,
      "grad_norm": 1.3115330934524536,
      "learning_rate": 4.219732425660663e-05,
      "loss": 0.2309,
      "step": 33380
    },
    {
      "epoch": 2.3675943981563554,
      "grad_norm": 1.3808673620224,
      "learning_rate": 4.215004963834917e-05,
      "loss": 0.2632,
      "step": 33390
    },
    {
      "epoch": 2.3683034922886015,
      "grad_norm": 1.3077309131622314,
      "learning_rate": 4.2102775020091714e-05,
      "loss": 0.2602,
      "step": 33400
    },
    {
      "epoch": 2.3690125864208476,
      "grad_norm": 0.903313934803009,
      "learning_rate": 4.205550040183426e-05,
      "loss": 0.2444,
      "step": 33410
    },
    {
      "epoch": 2.369721680553093,
      "grad_norm": 1.4922995567321777,
      "learning_rate": 4.20082257835768e-05,
      "loss": 0.2433,
      "step": 33420
    },
    {
      "epoch": 2.3704307746853397,
      "grad_norm": 2.0483274459838867,
      "learning_rate": 4.1960951165319337e-05,
      "loss": 0.2366,
      "step": 33430
    },
    {
      "epoch": 2.3711398688175853,
      "grad_norm": 0.866584837436676,
      "learning_rate": 4.1913676547061886e-05,
      "loss": 0.2548,
      "step": 33440
    },
    {
      "epoch": 2.371848962949832,
      "grad_norm": 0.9759684205055237,
      "learning_rate": 4.186640192880443e-05,
      "loss": 0.2633,
      "step": 33450
    },
    {
      "epoch": 2.3725580570820775,
      "grad_norm": 2.238286256790161,
      "learning_rate": 4.1819127310546966e-05,
      "loss": 0.2548,
      "step": 33460
    },
    {
      "epoch": 2.3732671512143235,
      "grad_norm": 1.258069634437561,
      "learning_rate": 4.177185269228951e-05,
      "loss": 0.2454,
      "step": 33470
    },
    {
      "epoch": 2.3739762453465696,
      "grad_norm": 1.2151683568954468,
      "learning_rate": 4.172457807403206e-05,
      "loss": 0.2412,
      "step": 33480
    },
    {
      "epoch": 2.3746853394788157,
      "grad_norm": 1.2963589429855347,
      "learning_rate": 4.1677303455774595e-05,
      "loss": 0.2754,
      "step": 33490
    },
    {
      "epoch": 2.3753944336110617,
      "grad_norm": 0.8710679411888123,
      "learning_rate": 4.163002883751714e-05,
      "loss": 0.2504,
      "step": 33500
    },
    {
      "epoch": 2.376103527743308,
      "grad_norm": 1.6307545900344849,
      "learning_rate": 4.158275421925968e-05,
      "loss": 0.2483,
      "step": 33510
    },
    {
      "epoch": 2.376812621875554,
      "grad_norm": 1.0544307231903076,
      "learning_rate": 4.1535479601002224e-05,
      "loss": 0.2112,
      "step": 33520
    },
    {
      "epoch": 2.3775217160078,
      "grad_norm": 0.6794379949569702,
      "learning_rate": 4.148820498274477e-05,
      "loss": 0.2474,
      "step": 33530
    },
    {
      "epoch": 2.378230810140046,
      "grad_norm": 0.9822885990142822,
      "learning_rate": 4.144093036448731e-05,
      "loss": 0.2565,
      "step": 33540
    },
    {
      "epoch": 2.378939904272292,
      "grad_norm": 1.475672721862793,
      "learning_rate": 4.139365574622985e-05,
      "loss": 0.2685,
      "step": 33550
    },
    {
      "epoch": 2.379648998404538,
      "grad_norm": 0.6850171089172363,
      "learning_rate": 4.1346381127972396e-05,
      "loss": 0.2153,
      "step": 33560
    },
    {
      "epoch": 2.380358092536784,
      "grad_norm": 1.5347516536712646,
      "learning_rate": 4.129910650971493e-05,
      "loss": 0.2781,
      "step": 33570
    },
    {
      "epoch": 2.3810671866690303,
      "grad_norm": 1.000860333442688,
      "learning_rate": 4.1251831891457476e-05,
      "loss": 0.2641,
      "step": 33580
    },
    {
      "epoch": 2.3817762808012763,
      "grad_norm": 0.7428032755851746,
      "learning_rate": 4.1204557273200026e-05,
      "loss": 0.2674,
      "step": 33590
    },
    {
      "epoch": 2.3824853749335224,
      "grad_norm": 1.084511637687683,
      "learning_rate": 4.115728265494256e-05,
      "loss": 0.2607,
      "step": 33600
    },
    {
      "epoch": 2.3831944690657685,
      "grad_norm": 1.941593885421753,
      "learning_rate": 4.1110008036685105e-05,
      "loss": 0.2292,
      "step": 33610
    },
    {
      "epoch": 2.3839035631980146,
      "grad_norm": 1.1765356063842773,
      "learning_rate": 4.106273341842765e-05,
      "loss": 0.2218,
      "step": 33620
    },
    {
      "epoch": 2.3846126573302606,
      "grad_norm": 1.1269845962524414,
      "learning_rate": 4.101545880017019e-05,
      "loss": 0.2232,
      "step": 33630
    },
    {
      "epoch": 2.3853217514625067,
      "grad_norm": 1.4695558547973633,
      "learning_rate": 4.0968184181912734e-05,
      "loss": 0.2427,
      "step": 33640
    },
    {
      "epoch": 2.3860308455947528,
      "grad_norm": 0.9195742607116699,
      "learning_rate": 4.092090956365528e-05,
      "loss": 0.2278,
      "step": 33650
    },
    {
      "epoch": 2.386739939726999,
      "grad_norm": 0.9966973662376404,
      "learning_rate": 4.0873634945397814e-05,
      "loss": 0.2294,
      "step": 33660
    },
    {
      "epoch": 2.387449033859245,
      "grad_norm": 0.9174795150756836,
      "learning_rate": 4.0826360327140363e-05,
      "loss": 0.2236,
      "step": 33670
    },
    {
      "epoch": 2.388158127991491,
      "grad_norm": 0.7608031034469604,
      "learning_rate": 4.07790857088829e-05,
      "loss": 0.237,
      "step": 33680
    },
    {
      "epoch": 2.388867222123737,
      "grad_norm": 1.8181626796722412,
      "learning_rate": 4.073181109062544e-05,
      "loss": 0.2466,
      "step": 33690
    },
    {
      "epoch": 2.389576316255983,
      "grad_norm": 0.89584881067276,
      "learning_rate": 4.068453647236799e-05,
      "loss": 0.2233,
      "step": 33700
    },
    {
      "epoch": 2.390285410388229,
      "grad_norm": 0.996997594833374,
      "learning_rate": 4.063726185411053e-05,
      "loss": 0.2464,
      "step": 33710
    },
    {
      "epoch": 2.3909945045204752,
      "grad_norm": 1.463077187538147,
      "learning_rate": 4.058998723585307e-05,
      "loss": 0.2298,
      "step": 33720
    },
    {
      "epoch": 2.3917035986527213,
      "grad_norm": 1.196520447731018,
      "learning_rate": 4.0542712617595615e-05,
      "loss": 0.2438,
      "step": 33730
    },
    {
      "epoch": 2.3924126927849674,
      "grad_norm": 1.0800468921661377,
      "learning_rate": 4.049543799933816e-05,
      "loss": 0.2682,
      "step": 33740
    },
    {
      "epoch": 2.3931217869172134,
      "grad_norm": 1.0032505989074707,
      "learning_rate": 4.04481633810807e-05,
      "loss": 0.2303,
      "step": 33750
    },
    {
      "epoch": 2.3938308810494595,
      "grad_norm": 2.1177115440368652,
      "learning_rate": 4.0400888762823244e-05,
      "loss": 0.2597,
      "step": 33760
    },
    {
      "epoch": 2.394539975181705,
      "grad_norm": 0.9091771841049194,
      "learning_rate": 4.035361414456578e-05,
      "loss": 0.2538,
      "step": 33770
    },
    {
      "epoch": 2.3952490693139517,
      "grad_norm": 0.7560423016548157,
      "learning_rate": 4.030633952630833e-05,
      "loss": 0.2611,
      "step": 33780
    },
    {
      "epoch": 2.3959581634461973,
      "grad_norm": 1.3057894706726074,
      "learning_rate": 4.025906490805087e-05,
      "loss": 0.2319,
      "step": 33790
    },
    {
      "epoch": 2.3966672575784433,
      "grad_norm": 1.0763064622879028,
      "learning_rate": 4.021179028979341e-05,
      "loss": 0.2579,
      "step": 33800
    },
    {
      "epoch": 2.3973763517106894,
      "grad_norm": 1.2938475608825684,
      "learning_rate": 4.016451567153595e-05,
      "loss": 0.2773,
      "step": 33810
    },
    {
      "epoch": 2.3980854458429355,
      "grad_norm": 2.1325526237487793,
      "learning_rate": 4.0117241053278496e-05,
      "loss": 0.2362,
      "step": 33820
    },
    {
      "epoch": 2.3987945399751816,
      "grad_norm": 3.264747381210327,
      "learning_rate": 4.006996643502104e-05,
      "loss": 0.2497,
      "step": 33830
    },
    {
      "epoch": 2.3995036341074276,
      "grad_norm": 0.9318044781684875,
      "learning_rate": 4.002269181676358e-05,
      "loss": 0.2366,
      "step": 33840
    },
    {
      "epoch": 2.4002127282396737,
      "grad_norm": 1.9819731712341309,
      "learning_rate": 3.9975417198506125e-05,
      "loss": 0.237,
      "step": 33850
    },
    {
      "epoch": 2.4009218223719198,
      "grad_norm": 0.9189184904098511,
      "learning_rate": 3.992814258024867e-05,
      "loss": 0.2506,
      "step": 33860
    },
    {
      "epoch": 2.401630916504166,
      "grad_norm": 0.865382969379425,
      "learning_rate": 3.988086796199121e-05,
      "loss": 0.2387,
      "step": 33870
    },
    {
      "epoch": 2.402340010636412,
      "grad_norm": 0.8736760020256042,
      "learning_rate": 3.983359334373375e-05,
      "loss": 0.2227,
      "step": 33880
    },
    {
      "epoch": 2.403049104768658,
      "grad_norm": 1.2782504558563232,
      "learning_rate": 3.97863187254763e-05,
      "loss": 0.2346,
      "step": 33890
    },
    {
      "epoch": 2.403758198900904,
      "grad_norm": 1.9038636684417725,
      "learning_rate": 3.9739044107218834e-05,
      "loss": 0.2389,
      "step": 33900
    },
    {
      "epoch": 2.40446729303315,
      "grad_norm": 1.760882019996643,
      "learning_rate": 3.969176948896138e-05,
      "loss": 0.257,
      "step": 33910
    },
    {
      "epoch": 2.405176387165396,
      "grad_norm": 1.1983753442764282,
      "learning_rate": 3.964449487070392e-05,
      "loss": 0.2384,
      "step": 33920
    },
    {
      "epoch": 2.4058854812976422,
      "grad_norm": 0.8923978209495544,
      "learning_rate": 3.959722025244646e-05,
      "loss": 0.2281,
      "step": 33930
    },
    {
      "epoch": 2.4065945754298883,
      "grad_norm": 1.016451120376587,
      "learning_rate": 3.9549945634189006e-05,
      "loss": 0.2624,
      "step": 33940
    },
    {
      "epoch": 2.4073036695621344,
      "grad_norm": 1.0879970788955688,
      "learning_rate": 3.950267101593155e-05,
      "loss": 0.2337,
      "step": 33950
    },
    {
      "epoch": 2.4080127636943804,
      "grad_norm": 1.3973402976989746,
      "learning_rate": 3.9455396397674085e-05,
      "loss": 0.2198,
      "step": 33960
    },
    {
      "epoch": 2.4087218578266265,
      "grad_norm": 1.3325231075286865,
      "learning_rate": 3.9408121779416635e-05,
      "loss": 0.2569,
      "step": 33970
    },
    {
      "epoch": 2.4094309519588726,
      "grad_norm": 0.9423863291740417,
      "learning_rate": 3.936084716115918e-05,
      "loss": 0.2436,
      "step": 33980
    },
    {
      "epoch": 2.4101400460911186,
      "grad_norm": 1.3319237232208252,
      "learning_rate": 3.9313572542901715e-05,
      "loss": 0.2437,
      "step": 33990
    },
    {
      "epoch": 2.4108491402233647,
      "grad_norm": 0.9456413388252258,
      "learning_rate": 3.9266297924644264e-05,
      "loss": 0.2086,
      "step": 34000
    },
    {
      "epoch": 2.411558234355611,
      "grad_norm": 1.1540220975875854,
      "learning_rate": 3.92190233063868e-05,
      "loss": 0.2247,
      "step": 34010
    },
    {
      "epoch": 2.412267328487857,
      "grad_norm": 0.8518804907798767,
      "learning_rate": 3.9171748688129344e-05,
      "loss": 0.244,
      "step": 34020
    },
    {
      "epoch": 2.412976422620103,
      "grad_norm": 1.5215997695922852,
      "learning_rate": 3.912447406987189e-05,
      "loss": 0.2405,
      "step": 34030
    },
    {
      "epoch": 2.413685516752349,
      "grad_norm": 1.34040367603302,
      "learning_rate": 3.907719945161443e-05,
      "loss": 0.2537,
      "step": 34040
    },
    {
      "epoch": 2.414394610884595,
      "grad_norm": 2.6112844944000244,
      "learning_rate": 3.902992483335697e-05,
      "loss": 0.209,
      "step": 34050
    },
    {
      "epoch": 2.415103705016841,
      "grad_norm": 0.9156516194343567,
      "learning_rate": 3.8982650215099516e-05,
      "loss": 0.2554,
      "step": 34060
    },
    {
      "epoch": 2.415812799149087,
      "grad_norm": 1.015027642250061,
      "learning_rate": 3.893537559684205e-05,
      "loss": 0.2421,
      "step": 34070
    },
    {
      "epoch": 2.4165218932813333,
      "grad_norm": 1.4439057111740112,
      "learning_rate": 3.88881009785846e-05,
      "loss": 0.2467,
      "step": 34080
    },
    {
      "epoch": 2.4172309874135793,
      "grad_norm": 1.179259181022644,
      "learning_rate": 3.8840826360327145e-05,
      "loss": 0.2009,
      "step": 34090
    },
    {
      "epoch": 2.417940081545825,
      "grad_norm": 1.1680095195770264,
      "learning_rate": 3.879355174206968e-05,
      "loss": 0.233,
      "step": 34100
    },
    {
      "epoch": 2.4186491756780715,
      "grad_norm": 1.5765031576156616,
      "learning_rate": 3.8746277123812225e-05,
      "loss": 0.2202,
      "step": 34110
    },
    {
      "epoch": 2.419358269810317,
      "grad_norm": 1.1325443983078003,
      "learning_rate": 3.8699002505554774e-05,
      "loss": 0.2524,
      "step": 34120
    },
    {
      "epoch": 2.4200673639425636,
      "grad_norm": 1.122069001197815,
      "learning_rate": 3.865172788729731e-05,
      "loss": 0.2331,
      "step": 34130
    },
    {
      "epoch": 2.4207764580748092,
      "grad_norm": 0.8308477997779846,
      "learning_rate": 3.8604453269039854e-05,
      "loss": 0.2336,
      "step": 34140
    },
    {
      "epoch": 2.4214855522070553,
      "grad_norm": 1.127915859222412,
      "learning_rate": 3.85571786507824e-05,
      "loss": 0.2354,
      "step": 34150
    },
    {
      "epoch": 2.4221946463393014,
      "grad_norm": 1.2244490385055542,
      "learning_rate": 3.850990403252494e-05,
      "loss": 0.2315,
      "step": 34160
    },
    {
      "epoch": 2.4229037404715474,
      "grad_norm": 1.2075068950653076,
      "learning_rate": 3.846262941426748e-05,
      "loss": 0.2149,
      "step": 34170
    },
    {
      "epoch": 2.4236128346037935,
      "grad_norm": 3.5836901664733887,
      "learning_rate": 3.841535479601002e-05,
      "loss": 0.2711,
      "step": 34180
    },
    {
      "epoch": 2.4243219287360396,
      "grad_norm": 0.8975628614425659,
      "learning_rate": 3.836808017775257e-05,
      "loss": 0.2276,
      "step": 34190
    },
    {
      "epoch": 2.4250310228682856,
      "grad_norm": 1.0783727169036865,
      "learning_rate": 3.832080555949511e-05,
      "loss": 0.2556,
      "step": 34200
    },
    {
      "epoch": 2.4257401170005317,
      "grad_norm": 1.289241075515747,
      "learning_rate": 3.827353094123765e-05,
      "loss": 0.2432,
      "step": 34210
    },
    {
      "epoch": 2.426449211132778,
      "grad_norm": 0.9211382269859314,
      "learning_rate": 3.822625632298019e-05,
      "loss": 0.2436,
      "step": 34220
    },
    {
      "epoch": 2.427158305265024,
      "grad_norm": 1.2139103412628174,
      "learning_rate": 3.817898170472274e-05,
      "loss": 0.2645,
      "step": 34230
    },
    {
      "epoch": 2.42786739939727,
      "grad_norm": 1.450616478919983,
      "learning_rate": 3.813170708646528e-05,
      "loss": 0.2547,
      "step": 34240
    },
    {
      "epoch": 2.428576493529516,
      "grad_norm": 1.560706615447998,
      "learning_rate": 3.808443246820782e-05,
      "loss": 0.2362,
      "step": 34250
    },
    {
      "epoch": 2.429285587661762,
      "grad_norm": 1.013326644897461,
      "learning_rate": 3.8037157849950364e-05,
      "loss": 0.2301,
      "step": 34260
    },
    {
      "epoch": 2.429994681794008,
      "grad_norm": 2.8355040550231934,
      "learning_rate": 3.798988323169291e-05,
      "loss": 0.2547,
      "step": 34270
    },
    {
      "epoch": 2.430703775926254,
      "grad_norm": 1.0254405736923218,
      "learning_rate": 3.794260861343545e-05,
      "loss": 0.2381,
      "step": 34280
    },
    {
      "epoch": 2.4314128700585003,
      "grad_norm": 0.7069016098976135,
      "learning_rate": 3.7895333995177986e-05,
      "loss": 0.2127,
      "step": 34290
    },
    {
      "epoch": 2.4321219641907463,
      "grad_norm": 1.4121224880218506,
      "learning_rate": 3.784805937692053e-05,
      "loss": 0.255,
      "step": 34300
    },
    {
      "epoch": 2.4328310583229924,
      "grad_norm": 1.1848808526992798,
      "learning_rate": 3.780078475866308e-05,
      "loss": 0.2445,
      "step": 34310
    },
    {
      "epoch": 2.4335401524552385,
      "grad_norm": 1.0375739336013794,
      "learning_rate": 3.7753510140405616e-05,
      "loss": 0.2274,
      "step": 34320
    },
    {
      "epoch": 2.4342492465874845,
      "grad_norm": 1.1496351957321167,
      "learning_rate": 3.770623552214816e-05,
      "loss": 0.2444,
      "step": 34330
    },
    {
      "epoch": 2.4349583407197306,
      "grad_norm": 1.0489095449447632,
      "learning_rate": 3.765896090389071e-05,
      "loss": 0.213,
      "step": 34340
    },
    {
      "epoch": 2.4356674348519767,
      "grad_norm": 2.25669002532959,
      "learning_rate": 3.7611686285633245e-05,
      "loss": 0.2608,
      "step": 34350
    },
    {
      "epoch": 2.4363765289842227,
      "grad_norm": 1.0330638885498047,
      "learning_rate": 3.756441166737579e-05,
      "loss": 0.2359,
      "step": 34360
    },
    {
      "epoch": 2.437085623116469,
      "grad_norm": 0.844519317150116,
      "learning_rate": 3.751713704911833e-05,
      "loss": 0.2399,
      "step": 34370
    },
    {
      "epoch": 2.437794717248715,
      "grad_norm": 1.597652792930603,
      "learning_rate": 3.7469862430860874e-05,
      "loss": 0.2109,
      "step": 34380
    },
    {
      "epoch": 2.438503811380961,
      "grad_norm": 1.0303212404251099,
      "learning_rate": 3.742258781260342e-05,
      "loss": 0.223,
      "step": 34390
    },
    {
      "epoch": 2.439212905513207,
      "grad_norm": 0.7834652066230774,
      "learning_rate": 3.737531319434595e-05,
      "loss": 0.2336,
      "step": 34400
    },
    {
      "epoch": 2.439921999645453,
      "grad_norm": 2.110309600830078,
      "learning_rate": 3.7328038576088496e-05,
      "loss": 0.225,
      "step": 34410
    },
    {
      "epoch": 2.440631093777699,
      "grad_norm": 0.9804618954658508,
      "learning_rate": 3.7280763957831046e-05,
      "loss": 0.2641,
      "step": 34420
    },
    {
      "epoch": 2.4413401879099452,
      "grad_norm": 1.1782073974609375,
      "learning_rate": 3.723348933957358e-05,
      "loss": 0.2386,
      "step": 34430
    },
    {
      "epoch": 2.4420492820421913,
      "grad_norm": 1.3636265993118286,
      "learning_rate": 3.7186214721316126e-05,
      "loss": 0.241,
      "step": 34440
    },
    {
      "epoch": 2.442758376174437,
      "grad_norm": 1.6976993083953857,
      "learning_rate": 3.713894010305867e-05,
      "loss": 0.2405,
      "step": 34450
    },
    {
      "epoch": 2.4434674703066834,
      "grad_norm": 2.3997244834899902,
      "learning_rate": 3.709166548480121e-05,
      "loss": 0.237,
      "step": 34460
    },
    {
      "epoch": 2.444176564438929,
      "grad_norm": 1.2088119983673096,
      "learning_rate": 3.7044390866543755e-05,
      "loss": 0.2676,
      "step": 34470
    },
    {
      "epoch": 2.4448856585711756,
      "grad_norm": 1.5305315256118774,
      "learning_rate": 3.69971162482863e-05,
      "loss": 0.2351,
      "step": 34480
    },
    {
      "epoch": 2.445594752703421,
      "grad_norm": 0.9522581100463867,
      "learning_rate": 3.694984163002884e-05,
      "loss": 0.2268,
      "step": 34490
    },
    {
      "epoch": 2.4463038468356673,
      "grad_norm": 1.014925241470337,
      "learning_rate": 3.6902567011771384e-05,
      "loss": 0.2322,
      "step": 34500
    },
    {
      "epoch": 2.4470129409679133,
      "grad_norm": 1.3840484619140625,
      "learning_rate": 3.685529239351393e-05,
      "loss": 0.2546,
      "step": 34510
    },
    {
      "epoch": 2.4477220351001594,
      "grad_norm": 1.7403751611709595,
      "learning_rate": 3.680801777525646e-05,
      "loss": 0.2412,
      "step": 34520
    },
    {
      "epoch": 2.4484311292324055,
      "grad_norm": 1.0919201374053955,
      "learning_rate": 3.676074315699901e-05,
      "loss": 0.2222,
      "step": 34530
    },
    {
      "epoch": 2.4491402233646515,
      "grad_norm": 1.1546273231506348,
      "learning_rate": 3.671346853874155e-05,
      "loss": 0.2529,
      "step": 34540
    },
    {
      "epoch": 2.4498493174968976,
      "grad_norm": 1.4702006578445435,
      "learning_rate": 3.666619392048409e-05,
      "loss": 0.2743,
      "step": 34550
    },
    {
      "epoch": 2.4505584116291437,
      "grad_norm": 1.0003689527511597,
      "learning_rate": 3.6618919302226636e-05,
      "loss": 0.2432,
      "step": 34560
    },
    {
      "epoch": 2.4512675057613897,
      "grad_norm": 1.723386287689209,
      "learning_rate": 3.657164468396918e-05,
      "loss": 0.2273,
      "step": 34570
    },
    {
      "epoch": 2.451976599893636,
      "grad_norm": 0.8741451501846313,
      "learning_rate": 3.652437006571172e-05,
      "loss": 0.2275,
      "step": 34580
    },
    {
      "epoch": 2.452685694025882,
      "grad_norm": 1.3135491609573364,
      "learning_rate": 3.6477095447454265e-05,
      "loss": 0.2867,
      "step": 34590
    },
    {
      "epoch": 2.453394788158128,
      "grad_norm": 1.129492163658142,
      "learning_rate": 3.64298208291968e-05,
      "loss": 0.2514,
      "step": 34600
    },
    {
      "epoch": 2.454103882290374,
      "grad_norm": 2.9639055728912354,
      "learning_rate": 3.638254621093935e-05,
      "loss": 0.2316,
      "step": 34610
    },
    {
      "epoch": 2.45481297642262,
      "grad_norm": 1.2131531238555908,
      "learning_rate": 3.6335271592681894e-05,
      "loss": 0.2544,
      "step": 34620
    },
    {
      "epoch": 2.455522070554866,
      "grad_norm": 0.8277497887611389,
      "learning_rate": 3.628799697442443e-05,
      "loss": 0.2633,
      "step": 34630
    },
    {
      "epoch": 2.4562311646871122,
      "grad_norm": 0.9839473366737366,
      "learning_rate": 3.624072235616698e-05,
      "loss": 0.2598,
      "step": 34640
    },
    {
      "epoch": 2.4569402588193583,
      "grad_norm": 1.0018390417099,
      "learning_rate": 3.6193447737909516e-05,
      "loss": 0.1966,
      "step": 34650
    },
    {
      "epoch": 2.4576493529516044,
      "grad_norm": 1.5663337707519531,
      "learning_rate": 3.614617311965206e-05,
      "loss": 0.2564,
      "step": 34660
    },
    {
      "epoch": 2.4583584470838504,
      "grad_norm": 1.1447302103042603,
      "learning_rate": 3.60988985013946e-05,
      "loss": 0.2251,
      "step": 34670
    },
    {
      "epoch": 2.4590675412160965,
      "grad_norm": 0.9303680658340454,
      "learning_rate": 3.6051623883137146e-05,
      "loss": 0.2557,
      "step": 34680
    },
    {
      "epoch": 2.4597766353483426,
      "grad_norm": 1.5789645910263062,
      "learning_rate": 3.600434926487969e-05,
      "loss": 0.2259,
      "step": 34690
    },
    {
      "epoch": 2.4604857294805886,
      "grad_norm": 1.3789993524551392,
      "learning_rate": 3.595707464662223e-05,
      "loss": 0.259,
      "step": 34700
    },
    {
      "epoch": 2.4611948236128347,
      "grad_norm": 1.1339538097381592,
      "learning_rate": 3.590980002836477e-05,
      "loss": 0.251,
      "step": 34710
    },
    {
      "epoch": 2.4619039177450808,
      "grad_norm": 1.034064769744873,
      "learning_rate": 3.586252541010732e-05,
      "loss": 0.246,
      "step": 34720
    },
    {
      "epoch": 2.462613011877327,
      "grad_norm": 1.0385183095932007,
      "learning_rate": 3.581525079184986e-05,
      "loss": 0.2044,
      "step": 34730
    },
    {
      "epoch": 2.463322106009573,
      "grad_norm": 1.1086293458938599,
      "learning_rate": 3.57679761735924e-05,
      "loss": 0.2181,
      "step": 34740
    },
    {
      "epoch": 2.464031200141819,
      "grad_norm": 0.8614596128463745,
      "learning_rate": 3.572070155533494e-05,
      "loss": 0.2258,
      "step": 34750
    },
    {
      "epoch": 2.464740294274065,
      "grad_norm": 0.9757917523384094,
      "learning_rate": 3.5673426937077483e-05,
      "loss": 0.2623,
      "step": 34760
    },
    {
      "epoch": 2.465449388406311,
      "grad_norm": 2.2561984062194824,
      "learning_rate": 3.5626152318820027e-05,
      "loss": 0.2524,
      "step": 34770
    },
    {
      "epoch": 2.466158482538557,
      "grad_norm": 1.3865047693252563,
      "learning_rate": 3.557887770056257e-05,
      "loss": 0.2196,
      "step": 34780
    },
    {
      "epoch": 2.4668675766708033,
      "grad_norm": 2.373443841934204,
      "learning_rate": 3.553160308230511e-05,
      "loss": 0.2681,
      "step": 34790
    },
    {
      "epoch": 2.467576670803049,
      "grad_norm": 1.003510594367981,
      "learning_rate": 3.5484328464047656e-05,
      "loss": 0.2588,
      "step": 34800
    },
    {
      "epoch": 2.4682857649352954,
      "grad_norm": 1.5054917335510254,
      "learning_rate": 3.54370538457902e-05,
      "loss": 0.2829,
      "step": 34810
    },
    {
      "epoch": 2.468994859067541,
      "grad_norm": 1.1149635314941406,
      "learning_rate": 3.5389779227532735e-05,
      "loss": 0.2219,
      "step": 34820
    },
    {
      "epoch": 2.469703953199787,
      "grad_norm": 1.0833231210708618,
      "learning_rate": 3.5342504609275285e-05,
      "loss": 0.2526,
      "step": 34830
    },
    {
      "epoch": 2.470413047332033,
      "grad_norm": 3.180783271789551,
      "learning_rate": 3.529522999101783e-05,
      "loss": 0.2642,
      "step": 34840
    },
    {
      "epoch": 2.471122141464279,
      "grad_norm": 2.894249439239502,
      "learning_rate": 3.5247955372760364e-05,
      "loss": 0.2422,
      "step": 34850
    },
    {
      "epoch": 2.4718312355965253,
      "grad_norm": 0.8759032487869263,
      "learning_rate": 3.520068075450291e-05,
      "loss": 0.2086,
      "step": 34860
    },
    {
      "epoch": 2.4725403297287714,
      "grad_norm": 2.6422975063323975,
      "learning_rate": 3.515340613624545e-05,
      "loss": 0.2435,
      "step": 34870
    },
    {
      "epoch": 2.4732494238610174,
      "grad_norm": 0.9836653470993042,
      "learning_rate": 3.5106131517987994e-05,
      "loss": 0.2515,
      "step": 34880
    },
    {
      "epoch": 2.4739585179932635,
      "grad_norm": 1.0464155673980713,
      "learning_rate": 3.5058856899730537e-05,
      "loss": 0.2409,
      "step": 34890
    },
    {
      "epoch": 2.4746676121255096,
      "grad_norm": 1.1864370107650757,
      "learning_rate": 3.501158228147308e-05,
      "loss": 0.2514,
      "step": 34900
    },
    {
      "epoch": 2.4753767062577556,
      "grad_norm": 2.3167927265167236,
      "learning_rate": 3.496430766321562e-05,
      "loss": 0.2324,
      "step": 34910
    },
    {
      "epoch": 2.4760858003900017,
      "grad_norm": 1.5771183967590332,
      "learning_rate": 3.4917033044958166e-05,
      "loss": 0.2336,
      "step": 34920
    },
    {
      "epoch": 2.4767948945222478,
      "grad_norm": 1.6652724742889404,
      "learning_rate": 3.48697584267007e-05,
      "loss": 0.2636,
      "step": 34930
    },
    {
      "epoch": 2.477503988654494,
      "grad_norm": 2.7108285427093506,
      "learning_rate": 3.4822483808443245e-05,
      "loss": 0.2109,
      "step": 34940
    },
    {
      "epoch": 2.47821308278674,
      "grad_norm": 1.1549795866012573,
      "learning_rate": 3.4775209190185795e-05,
      "loss": 0.2398,
      "step": 34950
    },
    {
      "epoch": 2.478922176918986,
      "grad_norm": 1.0307164192199707,
      "learning_rate": 3.472793457192833e-05,
      "loss": 0.2628,
      "step": 34960
    },
    {
      "epoch": 2.479631271051232,
      "grad_norm": 1.2774382829666138,
      "learning_rate": 3.4680659953670874e-05,
      "loss": 0.2466,
      "step": 34970
    },
    {
      "epoch": 2.480340365183478,
      "grad_norm": 1.5702812671661377,
      "learning_rate": 3.463338533541342e-05,
      "loss": 0.2261,
      "step": 34980
    },
    {
      "epoch": 2.481049459315724,
      "grad_norm": 1.1797497272491455,
      "learning_rate": 3.458611071715596e-05,
      "loss": 0.2935,
      "step": 34990
    },
    {
      "epoch": 2.4817585534479703,
      "grad_norm": 0.9143548607826233,
      "learning_rate": 3.4538836098898504e-05,
      "loss": 0.2331,
      "step": 35000
    },
    {
      "epoch": 2.4824676475802163,
      "grad_norm": 1.0405237674713135,
      "learning_rate": 3.449156148064105e-05,
      "loss": 0.2445,
      "step": 35010
    },
    {
      "epoch": 2.4831767417124624,
      "grad_norm": 1.158038854598999,
      "learning_rate": 3.444428686238359e-05,
      "loss": 0.2223,
      "step": 35020
    },
    {
      "epoch": 2.4838858358447085,
      "grad_norm": 1.9628081321716309,
      "learning_rate": 3.439701224412613e-05,
      "loss": 0.2205,
      "step": 35030
    },
    {
      "epoch": 2.4845949299769545,
      "grad_norm": 1.2178826332092285,
      "learning_rate": 3.434973762586867e-05,
      "loss": 0.2762,
      "step": 35040
    },
    {
      "epoch": 2.4853040241092006,
      "grad_norm": 1.372582197189331,
      "learning_rate": 3.430246300761121e-05,
      "loss": 0.2434,
      "step": 35050
    },
    {
      "epoch": 2.4860131182414467,
      "grad_norm": 1.0213767290115356,
      "learning_rate": 3.425518838935376e-05,
      "loss": 0.2356,
      "step": 35060
    },
    {
      "epoch": 2.4867222123736927,
      "grad_norm": 1.306494116783142,
      "learning_rate": 3.42079137710963e-05,
      "loss": 0.259,
      "step": 35070
    },
    {
      "epoch": 2.487431306505939,
      "grad_norm": 0.7362866401672363,
      "learning_rate": 3.416063915283884e-05,
      "loss": 0.238,
      "step": 35080
    },
    {
      "epoch": 2.488140400638185,
      "grad_norm": 1.1465518474578857,
      "learning_rate": 3.4113364534581384e-05,
      "loss": 0.239,
      "step": 35090
    },
    {
      "epoch": 2.488849494770431,
      "grad_norm": 1.552029013633728,
      "learning_rate": 3.406608991632393e-05,
      "loss": 0.2637,
      "step": 35100
    },
    {
      "epoch": 2.489558588902677,
      "grad_norm": 1.1664718389511108,
      "learning_rate": 3.401881529806647e-05,
      "loss": 0.2333,
      "step": 35110
    },
    {
      "epoch": 2.490267683034923,
      "grad_norm": 1.1351004838943481,
      "learning_rate": 3.3971540679809014e-05,
      "loss": 0.2049,
      "step": 35120
    },
    {
      "epoch": 2.4909767771671687,
      "grad_norm": 1.0038816928863525,
      "learning_rate": 3.392426606155156e-05,
      "loss": 0.2427,
      "step": 35130
    },
    {
      "epoch": 2.491685871299415,
      "grad_norm": 1.3080308437347412,
      "learning_rate": 3.38769914432941e-05,
      "loss": 0.2459,
      "step": 35140
    },
    {
      "epoch": 2.492394965431661,
      "grad_norm": 1.517531156539917,
      "learning_rate": 3.3829716825036636e-05,
      "loss": 0.2564,
      "step": 35150
    },
    {
      "epoch": 2.4931040595639073,
      "grad_norm": 1.2384216785430908,
      "learning_rate": 3.378244220677918e-05,
      "loss": 0.2809,
      "step": 35160
    },
    {
      "epoch": 2.493813153696153,
      "grad_norm": 1.3071411848068237,
      "learning_rate": 3.373516758852173e-05,
      "loss": 0.2088,
      "step": 35170
    },
    {
      "epoch": 2.494522247828399,
      "grad_norm": 0.9546182751655579,
      "learning_rate": 3.3687892970264265e-05,
      "loss": 0.2478,
      "step": 35180
    },
    {
      "epoch": 2.495231341960645,
      "grad_norm": 1.2178089618682861,
      "learning_rate": 3.364061835200681e-05,
      "loss": 0.2188,
      "step": 35190
    },
    {
      "epoch": 2.495940436092891,
      "grad_norm": 1.1019563674926758,
      "learning_rate": 3.359334373374935e-05,
      "loss": 0.2324,
      "step": 35200
    },
    {
      "epoch": 2.4966495302251372,
      "grad_norm": 1.4320412874221802,
      "learning_rate": 3.3546069115491895e-05,
      "loss": 0.2404,
      "step": 35210
    },
    {
      "epoch": 2.4973586243573833,
      "grad_norm": 1.4293437004089355,
      "learning_rate": 3.349879449723444e-05,
      "loss": 0.2495,
      "step": 35220
    },
    {
      "epoch": 2.4980677184896294,
      "grad_norm": 0.9105784296989441,
      "learning_rate": 3.345151987897698e-05,
      "loss": 0.2088,
      "step": 35230
    },
    {
      "epoch": 2.4987768126218755,
      "grad_norm": 1.3200100660324097,
      "learning_rate": 3.340424526071952e-05,
      "loss": 0.254,
      "step": 35240
    },
    {
      "epoch": 2.4994859067541215,
      "grad_norm": 0.9473557472229004,
      "learning_rate": 3.335697064246207e-05,
      "loss": 0.2181,
      "step": 35250
    },
    {
      "epoch": 2.5001950008863676,
      "grad_norm": 1.009146809577942,
      "learning_rate": 3.33096960242046e-05,
      "loss": 0.2444,
      "step": 35260
    },
    {
      "epoch": 2.5009040950186137,
      "grad_norm": 0.9692777395248413,
      "learning_rate": 3.3262421405947146e-05,
      "loss": 0.2251,
      "step": 35270
    },
    {
      "epoch": 2.5016131891508597,
      "grad_norm": 1.0793870687484741,
      "learning_rate": 3.321514678768969e-05,
      "loss": 0.2561,
      "step": 35280
    },
    {
      "epoch": 2.502322283283106,
      "grad_norm": 1.1217536926269531,
      "learning_rate": 3.316787216943223e-05,
      "loss": 0.2288,
      "step": 35290
    },
    {
      "epoch": 2.503031377415352,
      "grad_norm": 1.2832257747650146,
      "learning_rate": 3.3120597551174775e-05,
      "loss": 0.2371,
      "step": 35300
    },
    {
      "epoch": 2.503740471547598,
      "grad_norm": 1.254569172859192,
      "learning_rate": 3.307332293291732e-05,
      "loss": 0.2224,
      "step": 35310
    },
    {
      "epoch": 2.504449565679844,
      "grad_norm": 0.9074589610099792,
      "learning_rate": 3.302604831465986e-05,
      "loss": 0.2677,
      "step": 35320
    },
    {
      "epoch": 2.50515865981209,
      "grad_norm": 1.2731528282165527,
      "learning_rate": 3.2978773696402405e-05,
      "loss": 0.2697,
      "step": 35330
    },
    {
      "epoch": 2.505867753944336,
      "grad_norm": 0.8692008256912231,
      "learning_rate": 3.293149907814495e-05,
      "loss": 0.2592,
      "step": 35340
    },
    {
      "epoch": 2.506576848076582,
      "grad_norm": 0.7643836140632629,
      "learning_rate": 3.2884224459887484e-05,
      "loss": 0.2419,
      "step": 35350
    },
    {
      "epoch": 2.5072859422088283,
      "grad_norm": 1.0546902418136597,
      "learning_rate": 3.2836949841630034e-05,
      "loss": 0.2634,
      "step": 35360
    },
    {
      "epoch": 2.5079950363410743,
      "grad_norm": 0.8083577752113342,
      "learning_rate": 3.278967522337258e-05,
      "loss": 0.1967,
      "step": 35370
    },
    {
      "epoch": 2.5087041304733204,
      "grad_norm": 1.1029962301254272,
      "learning_rate": 3.274240060511511e-05,
      "loss": 0.2551,
      "step": 35380
    },
    {
      "epoch": 2.5094132246055665,
      "grad_norm": 1.2067619562149048,
      "learning_rate": 3.2695125986857656e-05,
      "loss": 0.2466,
      "step": 35390
    },
    {
      "epoch": 2.5101223187378126,
      "grad_norm": 0.9582014083862305,
      "learning_rate": 3.26478513686002e-05,
      "loss": 0.2231,
      "step": 35400
    },
    {
      "epoch": 2.5108314128700586,
      "grad_norm": 2.250619649887085,
      "learning_rate": 3.260057675034274e-05,
      "loss": 0.2845,
      "step": 35410
    },
    {
      "epoch": 2.5115405070023047,
      "grad_norm": 1.0218697786331177,
      "learning_rate": 3.2553302132085285e-05,
      "loss": 0.2439,
      "step": 35420
    },
    {
      "epoch": 2.5122496011345508,
      "grad_norm": 1.0765612125396729,
      "learning_rate": 3.250602751382782e-05,
      "loss": 0.2311,
      "step": 35430
    },
    {
      "epoch": 2.512958695266797,
      "grad_norm": 0.9164279699325562,
      "learning_rate": 3.245875289557037e-05,
      "loss": 0.2066,
      "step": 35440
    },
    {
      "epoch": 2.513667789399043,
      "grad_norm": 1.4698269367218018,
      "learning_rate": 3.2411478277312915e-05,
      "loss": 0.2287,
      "step": 35450
    },
    {
      "epoch": 2.5143768835312885,
      "grad_norm": 1.4732990264892578,
      "learning_rate": 3.236420365905545e-05,
      "loss": 0.2417,
      "step": 35460
    },
    {
      "epoch": 2.515085977663535,
      "grad_norm": 1.070420503616333,
      "learning_rate": 3.2316929040798e-05,
      "loss": 0.2359,
      "step": 35470
    },
    {
      "epoch": 2.5157950717957807,
      "grad_norm": 1.0927659273147583,
      "learning_rate": 3.2269654422540544e-05,
      "loss": 0.2553,
      "step": 35480
    },
    {
      "epoch": 2.516504165928027,
      "grad_norm": 3.0126543045043945,
      "learning_rate": 3.222237980428308e-05,
      "loss": 0.2111,
      "step": 35490
    },
    {
      "epoch": 2.517213260060273,
      "grad_norm": 0.9977256059646606,
      "learning_rate": 3.217510518602562e-05,
      "loss": 0.2226,
      "step": 35500
    },
    {
      "epoch": 2.5179223541925193,
      "grad_norm": 0.8409512639045715,
      "learning_rate": 3.2127830567768166e-05,
      "loss": 0.2178,
      "step": 35510
    },
    {
      "epoch": 2.518631448324765,
      "grad_norm": 0.7551063299179077,
      "learning_rate": 3.208055594951071e-05,
      "loss": 0.205,
      "step": 35520
    },
    {
      "epoch": 2.5193405424570114,
      "grad_norm": 4.030965805053711,
      "learning_rate": 3.203328133125325e-05,
      "loss": 0.233,
      "step": 35530
    },
    {
      "epoch": 2.520049636589257,
      "grad_norm": 1.4610663652420044,
      "learning_rate": 3.198600671299579e-05,
      "loss": 0.2246,
      "step": 35540
    },
    {
      "epoch": 2.520758730721503,
      "grad_norm": 1.8465203046798706,
      "learning_rate": 3.193873209473834e-05,
      "loss": 0.215,
      "step": 35550
    },
    {
      "epoch": 2.521467824853749,
      "grad_norm": 1.5094834566116333,
      "learning_rate": 3.189145747648088e-05,
      "loss": 0.2327,
      "step": 35560
    },
    {
      "epoch": 2.5221769189859953,
      "grad_norm": 0.8773999810218811,
      "learning_rate": 3.184418285822342e-05,
      "loss": 0.2244,
      "step": 35570
    },
    {
      "epoch": 2.5228860131182413,
      "grad_norm": 1.0838501453399658,
      "learning_rate": 3.179690823996596e-05,
      "loss": 0.2582,
      "step": 35580
    },
    {
      "epoch": 2.5235951072504874,
      "grad_norm": 0.7552492022514343,
      "learning_rate": 3.174963362170851e-05,
      "loss": 0.2379,
      "step": 35590
    },
    {
      "epoch": 2.5243042013827335,
      "grad_norm": 1.0363229513168335,
      "learning_rate": 3.170235900345105e-05,
      "loss": 0.229,
      "step": 35600
    },
    {
      "epoch": 2.5250132955149795,
      "grad_norm": 1.0563675165176392,
      "learning_rate": 3.165508438519359e-05,
      "loss": 0.2412,
      "step": 35610
    },
    {
      "epoch": 2.5257223896472256,
      "grad_norm": 1.4183777570724487,
      "learning_rate": 3.160780976693613e-05,
      "loss": 0.2429,
      "step": 35620
    },
    {
      "epoch": 2.5264314837794717,
      "grad_norm": 0.6413614153862,
      "learning_rate": 3.1560535148678676e-05,
      "loss": 0.2261,
      "step": 35630
    },
    {
      "epoch": 2.5271405779117178,
      "grad_norm": 0.7782555818557739,
      "learning_rate": 3.151326053042122e-05,
      "loss": 0.2021,
      "step": 35640
    },
    {
      "epoch": 2.527849672043964,
      "grad_norm": 0.9548876881599426,
      "learning_rate": 3.1465985912163756e-05,
      "loss": 0.2277,
      "step": 35650
    },
    {
      "epoch": 2.52855876617621,
      "grad_norm": 1.075323224067688,
      "learning_rate": 3.1418711293906306e-05,
      "loss": 0.2013,
      "step": 35660
    },
    {
      "epoch": 2.529267860308456,
      "grad_norm": 0.8755591511726379,
      "learning_rate": 3.137143667564885e-05,
      "loss": 0.2201,
      "step": 35670
    },
    {
      "epoch": 2.529976954440702,
      "grad_norm": 1.0893081426620483,
      "learning_rate": 3.1324162057391385e-05,
      "loss": 0.228,
      "step": 35680
    },
    {
      "epoch": 2.530686048572948,
      "grad_norm": 1.3357179164886475,
      "learning_rate": 3.127688743913393e-05,
      "loss": 0.2421,
      "step": 35690
    },
    {
      "epoch": 2.531395142705194,
      "grad_norm": 1.1394630670547485,
      "learning_rate": 3.122961282087648e-05,
      "loss": 0.2389,
      "step": 35700
    },
    {
      "epoch": 2.5321042368374402,
      "grad_norm": 1.7625678777694702,
      "learning_rate": 3.1182338202619014e-05,
      "loss": 0.208,
      "step": 35710
    },
    {
      "epoch": 2.5328133309696863,
      "grad_norm": 1.143409013748169,
      "learning_rate": 3.113506358436156e-05,
      "loss": 0.2415,
      "step": 35720
    },
    {
      "epoch": 2.5335224251019324,
      "grad_norm": 2.855776786804199,
      "learning_rate": 3.10877889661041e-05,
      "loss": 0.2433,
      "step": 35730
    },
    {
      "epoch": 2.5342315192341784,
      "grad_norm": 1.1620460748672485,
      "learning_rate": 3.104051434784664e-05,
      "loss": 0.2054,
      "step": 35740
    },
    {
      "epoch": 2.5349406133664245,
      "grad_norm": 0.9400559663772583,
      "learning_rate": 3.0993239729589186e-05,
      "loss": 0.2343,
      "step": 35750
    },
    {
      "epoch": 2.5356497074986706,
      "grad_norm": 0.9211326837539673,
      "learning_rate": 3.094596511133173e-05,
      "loss": 0.2321,
      "step": 35760
    },
    {
      "epoch": 2.5363588016309166,
      "grad_norm": 1.066881775856018,
      "learning_rate": 3.089869049307427e-05,
      "loss": 0.2962,
      "step": 35770
    },
    {
      "epoch": 2.5370678957631627,
      "grad_norm": 1.2037732601165771,
      "learning_rate": 3.0851415874816816e-05,
      "loss": 0.2455,
      "step": 35780
    },
    {
      "epoch": 2.5377769898954083,
      "grad_norm": 1.1220461130142212,
      "learning_rate": 3.080414125655935e-05,
      "loss": 0.2318,
      "step": 35790
    },
    {
      "epoch": 2.538486084027655,
      "grad_norm": 1.1132463216781616,
      "learning_rate": 3.0756866638301895e-05,
      "loss": 0.235,
      "step": 35800
    },
    {
      "epoch": 2.5391951781599005,
      "grad_norm": 1.624279499053955,
      "learning_rate": 3.0709592020044445e-05,
      "loss": 0.2308,
      "step": 35810
    },
    {
      "epoch": 2.539904272292147,
      "grad_norm": 1.2196869850158691,
      "learning_rate": 3.066231740178698e-05,
      "loss": 0.2575,
      "step": 35820
    },
    {
      "epoch": 2.5406133664243926,
      "grad_norm": 1.2455379962921143,
      "learning_rate": 3.0615042783529524e-05,
      "loss": 0.2654,
      "step": 35830
    },
    {
      "epoch": 2.541322460556639,
      "grad_norm": 1.1056365966796875,
      "learning_rate": 3.056776816527207e-05,
      "loss": 0.2302,
      "step": 35840
    },
    {
      "epoch": 2.5420315546888848,
      "grad_norm": 0.8714109063148499,
      "learning_rate": 3.052049354701461e-05,
      "loss": 0.2534,
      "step": 35850
    },
    {
      "epoch": 2.5427406488211313,
      "grad_norm": 1.7110289335250854,
      "learning_rate": 3.0473218928757153e-05,
      "loss": 0.2152,
      "step": 35860
    },
    {
      "epoch": 2.543449742953377,
      "grad_norm": 1.4076919555664062,
      "learning_rate": 3.0425944310499693e-05,
      "loss": 0.2476,
      "step": 35870
    },
    {
      "epoch": 2.544158837085623,
      "grad_norm": 1.2246191501617432,
      "learning_rate": 3.0378669692242233e-05,
      "loss": 0.2172,
      "step": 35880
    },
    {
      "epoch": 2.544867931217869,
      "grad_norm": 1.517754316329956,
      "learning_rate": 3.033139507398478e-05,
      "loss": 0.2577,
      "step": 35890
    },
    {
      "epoch": 2.545577025350115,
      "grad_norm": 1.103682279586792,
      "learning_rate": 3.0284120455727322e-05,
      "loss": 0.2443,
      "step": 35900
    },
    {
      "epoch": 2.546286119482361,
      "grad_norm": 1.0572515726089478,
      "learning_rate": 3.0236845837469862e-05,
      "loss": 0.2306,
      "step": 35910
    },
    {
      "epoch": 2.5469952136146072,
      "grad_norm": 0.9614297747612,
      "learning_rate": 3.0189571219212405e-05,
      "loss": 0.2521,
      "step": 35920
    },
    {
      "epoch": 2.5477043077468533,
      "grad_norm": 0.7338804006576538,
      "learning_rate": 3.014229660095495e-05,
      "loss": 0.2216,
      "step": 35930
    },
    {
      "epoch": 2.5484134018790994,
      "grad_norm": 1.3269553184509277,
      "learning_rate": 3.009502198269749e-05,
      "loss": 0.2555,
      "step": 35940
    },
    {
      "epoch": 2.5491224960113454,
      "grad_norm": 2.008822202682495,
      "learning_rate": 3.004774736444003e-05,
      "loss": 0.2631,
      "step": 35950
    },
    {
      "epoch": 2.5498315901435915,
      "grad_norm": 1.395121693611145,
      "learning_rate": 3.0000472746182577e-05,
      "loss": 0.2341,
      "step": 35960
    },
    {
      "epoch": 2.5505406842758376,
      "grad_norm": 1.5285677909851074,
      "learning_rate": 2.995319812792512e-05,
      "loss": 0.2762,
      "step": 35970
    },
    {
      "epoch": 2.5512497784080836,
      "grad_norm": 2.204728126525879,
      "learning_rate": 2.990592350966766e-05,
      "loss": 0.2225,
      "step": 35980
    },
    {
      "epoch": 2.5519588725403297,
      "grad_norm": 1.3032246828079224,
      "learning_rate": 2.98586488914102e-05,
      "loss": 0.2139,
      "step": 35990
    },
    {
      "epoch": 2.552667966672576,
      "grad_norm": 1.4006084203720093,
      "learning_rate": 2.981137427315275e-05,
      "loss": 0.2645,
      "step": 36000
    },
    {
      "epoch": 2.553377060804822,
      "grad_norm": 0.9570492506027222,
      "learning_rate": 2.976409965489529e-05,
      "loss": 0.2528,
      "step": 36010
    },
    {
      "epoch": 2.554086154937068,
      "grad_norm": 0.9047584533691406,
      "learning_rate": 2.971682503663783e-05,
      "loss": 0.2488,
      "step": 36020
    },
    {
      "epoch": 2.554795249069314,
      "grad_norm": 1.340360403060913,
      "learning_rate": 2.9669550418380372e-05,
      "loss": 0.2474,
      "step": 36030
    },
    {
      "epoch": 2.55550434320156,
      "grad_norm": 1.1336959600448608,
      "learning_rate": 2.962227580012292e-05,
      "loss": 0.2308,
      "step": 36040
    },
    {
      "epoch": 2.556213437333806,
      "grad_norm": 1.46051824092865,
      "learning_rate": 2.9575001181865458e-05,
      "loss": 0.2621,
      "step": 36050
    },
    {
      "epoch": 2.556922531466052,
      "grad_norm": 1.6095143556594849,
      "learning_rate": 2.9527726563607998e-05,
      "loss": 0.2636,
      "step": 36060
    },
    {
      "epoch": 2.5576316255982983,
      "grad_norm": 1.5985469818115234,
      "learning_rate": 2.948045194535054e-05,
      "loss": 0.2562,
      "step": 36070
    },
    {
      "epoch": 2.5583407197305443,
      "grad_norm": 0.8337804079055786,
      "learning_rate": 2.9433177327093087e-05,
      "loss": 0.2408,
      "step": 36080
    },
    {
      "epoch": 2.5590498138627904,
      "grad_norm": 1.0601617097854614,
      "learning_rate": 2.9385902708835627e-05,
      "loss": 0.2509,
      "step": 36090
    },
    {
      "epoch": 2.5597589079950365,
      "grad_norm": 1.0721921920776367,
      "learning_rate": 2.933862809057817e-05,
      "loss": 0.2213,
      "step": 36100
    },
    {
      "epoch": 2.5604680021272825,
      "grad_norm": 1.0350319147109985,
      "learning_rate": 2.9291353472320717e-05,
      "loss": 0.2315,
      "step": 36110
    },
    {
      "epoch": 2.5611770962595286,
      "grad_norm": 1.880924940109253,
      "learning_rate": 2.9244078854063256e-05,
      "loss": 0.2618,
      "step": 36120
    },
    {
      "epoch": 2.5618861903917747,
      "grad_norm": 0.9197983145713806,
      "learning_rate": 2.9196804235805796e-05,
      "loss": 0.2365,
      "step": 36130
    },
    {
      "epoch": 2.5625952845240203,
      "grad_norm": 1.799363613128662,
      "learning_rate": 2.914952961754834e-05,
      "loss": 0.2174,
      "step": 36140
    },
    {
      "epoch": 2.563304378656267,
      "grad_norm": 0.9920843839645386,
      "learning_rate": 2.9102254999290885e-05,
      "loss": 0.2678,
      "step": 36150
    },
    {
      "epoch": 2.5640134727885124,
      "grad_norm": 1.155124545097351,
      "learning_rate": 2.9054980381033425e-05,
      "loss": 0.2536,
      "step": 36160
    },
    {
      "epoch": 2.564722566920759,
      "grad_norm": 0.8790344595909119,
      "learning_rate": 2.9007705762775965e-05,
      "loss": 0.2277,
      "step": 36170
    },
    {
      "epoch": 2.5654316610530046,
      "grad_norm": 1.2355036735534668,
      "learning_rate": 2.8960431144518508e-05,
      "loss": 0.2266,
      "step": 36180
    },
    {
      "epoch": 2.566140755185251,
      "grad_norm": 1.1447334289550781,
      "learning_rate": 2.8913156526261054e-05,
      "loss": 0.22,
      "step": 36190
    },
    {
      "epoch": 2.5668498493174967,
      "grad_norm": 1.0233588218688965,
      "learning_rate": 2.8865881908003594e-05,
      "loss": 0.2642,
      "step": 36200
    },
    {
      "epoch": 2.5675589434497432,
      "grad_norm": 1.1431492567062378,
      "learning_rate": 2.8818607289746137e-05,
      "loss": 0.2232,
      "step": 36210
    },
    {
      "epoch": 2.568268037581989,
      "grad_norm": 0.821040689945221,
      "learning_rate": 2.8771332671488677e-05,
      "loss": 0.2261,
      "step": 36220
    },
    {
      "epoch": 2.568977131714235,
      "grad_norm": 1.206044316291809,
      "learning_rate": 2.8724058053231223e-05,
      "loss": 0.2389,
      "step": 36230
    },
    {
      "epoch": 2.569686225846481,
      "grad_norm": 1.0656466484069824,
      "learning_rate": 2.8676783434973763e-05,
      "loss": 0.2,
      "step": 36240
    },
    {
      "epoch": 2.570395319978727,
      "grad_norm": 3.294947862625122,
      "learning_rate": 2.8629508816716306e-05,
      "loss": 0.2445,
      "step": 36250
    },
    {
      "epoch": 2.571104414110973,
      "grad_norm": 1.3853026628494263,
      "learning_rate": 2.8582234198458852e-05,
      "loss": 0.1886,
      "step": 36260
    },
    {
      "epoch": 2.571813508243219,
      "grad_norm": 1.4118987321853638,
      "learning_rate": 2.8534959580201392e-05,
      "loss": 0.2789,
      "step": 36270
    },
    {
      "epoch": 2.5725226023754653,
      "grad_norm": 1.0882371664047241,
      "learning_rate": 2.8487684961943932e-05,
      "loss": 0.2336,
      "step": 36280
    },
    {
      "epoch": 2.5732316965077113,
      "grad_norm": 1.050181269645691,
      "learning_rate": 2.8440410343686475e-05,
      "loss": 0.2498,
      "step": 36290
    },
    {
      "epoch": 2.5739407906399574,
      "grad_norm": 0.7653147578239441,
      "learning_rate": 2.839313572542902e-05,
      "loss": 0.2819,
      "step": 36300
    },
    {
      "epoch": 2.5746498847722035,
      "grad_norm": 1.4926904439926147,
      "learning_rate": 2.834586110717156e-05,
      "loss": 0.2259,
      "step": 36310
    },
    {
      "epoch": 2.5753589789044495,
      "grad_norm": 1.4502332210540771,
      "learning_rate": 2.8298586488914104e-05,
      "loss": 0.2525,
      "step": 36320
    },
    {
      "epoch": 2.5760680730366956,
      "grad_norm": 1.3646501302719116,
      "learning_rate": 2.8251311870656644e-05,
      "loss": 0.2777,
      "step": 36330
    },
    {
      "epoch": 2.5767771671689417,
      "grad_norm": 1.179735779762268,
      "learning_rate": 2.820403725239919e-05,
      "loss": 0.2316,
      "step": 36340
    },
    {
      "epoch": 2.5774862613011877,
      "grad_norm": 1.5185308456420898,
      "learning_rate": 2.815676263414173e-05,
      "loss": 0.2609,
      "step": 36350
    },
    {
      "epoch": 2.578195355433434,
      "grad_norm": 0.8845122456550598,
      "learning_rate": 2.8109488015884273e-05,
      "loss": 0.2413,
      "step": 36360
    },
    {
      "epoch": 2.57890444956568,
      "grad_norm": 2.8812055587768555,
      "learning_rate": 2.8062213397626813e-05,
      "loss": 0.2329,
      "step": 36370
    },
    {
      "epoch": 2.579613543697926,
      "grad_norm": 1.7054717540740967,
      "learning_rate": 2.801493877936936e-05,
      "loss": 0.2209,
      "step": 36380
    },
    {
      "epoch": 2.580322637830172,
      "grad_norm": 1.671900987625122,
      "learning_rate": 2.7967664161111902e-05,
      "loss": 0.2275,
      "step": 36390
    },
    {
      "epoch": 2.581031731962418,
      "grad_norm": 1.3026542663574219,
      "learning_rate": 2.7920389542854442e-05,
      "loss": 0.2324,
      "step": 36400
    },
    {
      "epoch": 2.581740826094664,
      "grad_norm": 2.0711123943328857,
      "learning_rate": 2.787311492459698e-05,
      "loss": 0.2568,
      "step": 36410
    },
    {
      "epoch": 2.5824499202269102,
      "grad_norm": 1.3486884832382202,
      "learning_rate": 2.7825840306339528e-05,
      "loss": 0.2318,
      "step": 36420
    },
    {
      "epoch": 2.5831590143591563,
      "grad_norm": 0.896913468837738,
      "learning_rate": 2.777856568808207e-05,
      "loss": 0.2337,
      "step": 36430
    },
    {
      "epoch": 2.5838681084914024,
      "grad_norm": 1.7500813007354736,
      "learning_rate": 2.773129106982461e-05,
      "loss": 0.2621,
      "step": 36440
    },
    {
      "epoch": 2.5845772026236484,
      "grad_norm": 1.1703482866287231,
      "learning_rate": 2.7684016451567157e-05,
      "loss": 0.2653,
      "step": 36450
    },
    {
      "epoch": 2.5852862967558945,
      "grad_norm": 1.0031591653823853,
      "learning_rate": 2.7636741833309697e-05,
      "loss": 0.2195,
      "step": 36460
    },
    {
      "epoch": 2.5859953908881406,
      "grad_norm": 1.2803521156311035,
      "learning_rate": 2.758946721505224e-05,
      "loss": 0.2241,
      "step": 36470
    },
    {
      "epoch": 2.5867044850203866,
      "grad_norm": 1.0679948329925537,
      "learning_rate": 2.754219259679478e-05,
      "loss": 0.2683,
      "step": 36480
    },
    {
      "epoch": 2.5874135791526323,
      "grad_norm": 1.0636495351791382,
      "learning_rate": 2.7494917978537326e-05,
      "loss": 0.2353,
      "step": 36490
    },
    {
      "epoch": 2.5881226732848788,
      "grad_norm": 1.777162790298462,
      "learning_rate": 2.744764336027987e-05,
      "loss": 0.2268,
      "step": 36500
    },
    {
      "epoch": 2.5888317674171244,
      "grad_norm": 1.0194931030273438,
      "learning_rate": 2.740036874202241e-05,
      "loss": 0.2512,
      "step": 36510
    },
    {
      "epoch": 2.589540861549371,
      "grad_norm": 1.0088552236557007,
      "learning_rate": 2.735309412376495e-05,
      "loss": 0.2155,
      "step": 36520
    },
    {
      "epoch": 2.5902499556816165,
      "grad_norm": 0.7862381935119629,
      "learning_rate": 2.7305819505507495e-05,
      "loss": 0.2045,
      "step": 36530
    },
    {
      "epoch": 2.590959049813863,
      "grad_norm": 1.620465636253357,
      "learning_rate": 2.7258544887250038e-05,
      "loss": 0.223,
      "step": 36540
    },
    {
      "epoch": 2.5916681439461087,
      "grad_norm": 1.0907034873962402,
      "learning_rate": 2.7211270268992578e-05,
      "loss": 0.2749,
      "step": 36550
    },
    {
      "epoch": 2.5923772380783547,
      "grad_norm": 1.1359361410140991,
      "learning_rate": 2.7163995650735117e-05,
      "loss": 0.2406,
      "step": 36560
    },
    {
      "epoch": 2.593086332210601,
      "grad_norm": 1.1075444221496582,
      "learning_rate": 2.7116721032477664e-05,
      "loss": 0.2235,
      "step": 36570
    },
    {
      "epoch": 2.593795426342847,
      "grad_norm": 2.832611083984375,
      "learning_rate": 2.7069446414220207e-05,
      "loss": 0.2352,
      "step": 36580
    },
    {
      "epoch": 2.594504520475093,
      "grad_norm": 1.7947286367416382,
      "learning_rate": 2.7022171795962747e-05,
      "loss": 0.2149,
      "step": 36590
    },
    {
      "epoch": 2.595213614607339,
      "grad_norm": 1.2885620594024658,
      "learning_rate": 2.6974897177705293e-05,
      "loss": 0.2381,
      "step": 36600
    },
    {
      "epoch": 2.595922708739585,
      "grad_norm": 1.3084051609039307,
      "learning_rate": 2.6927622559447836e-05,
      "loss": 0.2458,
      "step": 36610
    },
    {
      "epoch": 2.596631802871831,
      "grad_norm": 1.152403712272644,
      "learning_rate": 2.6880347941190376e-05,
      "loss": 0.2497,
      "step": 36620
    },
    {
      "epoch": 2.597340897004077,
      "grad_norm": 2.4581634998321533,
      "learning_rate": 2.6833073322932916e-05,
      "loss": 0.244,
      "step": 36630
    },
    {
      "epoch": 2.5980499911363233,
      "grad_norm": 0.837393045425415,
      "learning_rate": 2.6785798704675462e-05,
      "loss": 0.2149,
      "step": 36640
    },
    {
      "epoch": 2.5987590852685694,
      "grad_norm": 1.2077631950378418,
      "learning_rate": 2.6738524086418005e-05,
      "loss": 0.2629,
      "step": 36650
    },
    {
      "epoch": 2.5994681794008154,
      "grad_norm": 1.29924476146698,
      "learning_rate": 2.6691249468160545e-05,
      "loss": 0.2304,
      "step": 36660
    },
    {
      "epoch": 2.6001772735330615,
      "grad_norm": 1.1999584436416626,
      "learning_rate": 2.6643974849903088e-05,
      "loss": 0.2357,
      "step": 36670
    },
    {
      "epoch": 2.6008863676653076,
      "grad_norm": 0.9744841456413269,
      "learning_rate": 2.6596700231645634e-05,
      "loss": 0.2539,
      "step": 36680
    },
    {
      "epoch": 2.6015954617975536,
      "grad_norm": 2.5135552883148193,
      "learning_rate": 2.6549425613388174e-05,
      "loss": 0.2502,
      "step": 36690
    },
    {
      "epoch": 2.6023045559297997,
      "grad_norm": 1.222520351409912,
      "learning_rate": 2.6502150995130714e-05,
      "loss": 0.2403,
      "step": 36700
    },
    {
      "epoch": 2.6030136500620458,
      "grad_norm": 1.693837285041809,
      "learning_rate": 2.6454876376873257e-05,
      "loss": 0.2332,
      "step": 36710
    },
    {
      "epoch": 2.603722744194292,
      "grad_norm": 1.596632957458496,
      "learning_rate": 2.6407601758615803e-05,
      "loss": 0.2318,
      "step": 36720
    },
    {
      "epoch": 2.604431838326538,
      "grad_norm": 1.3544639348983765,
      "learning_rate": 2.6360327140358343e-05,
      "loss": 0.2272,
      "step": 36730
    },
    {
      "epoch": 2.605140932458784,
      "grad_norm": 0.9418292045593262,
      "learning_rate": 2.6313052522100883e-05,
      "loss": 0.2074,
      "step": 36740
    },
    {
      "epoch": 2.60585002659103,
      "grad_norm": 1.1453219652175903,
      "learning_rate": 2.626577790384343e-05,
      "loss": 0.2341,
      "step": 36750
    },
    {
      "epoch": 2.606559120723276,
      "grad_norm": 1.5983586311340332,
      "learning_rate": 2.6218503285585972e-05,
      "loss": 0.2266,
      "step": 36760
    },
    {
      "epoch": 2.607268214855522,
      "grad_norm": 2.4211549758911133,
      "learning_rate": 2.6171228667328512e-05,
      "loss": 0.2431,
      "step": 36770
    },
    {
      "epoch": 2.6079773089877683,
      "grad_norm": 0.9015202522277832,
      "learning_rate": 2.6123954049071055e-05,
      "loss": 0.2265,
      "step": 36780
    },
    {
      "epoch": 2.6086864031200143,
      "grad_norm": 1.006671667098999,
      "learning_rate": 2.60766794308136e-05,
      "loss": 0.2348,
      "step": 36790
    },
    {
      "epoch": 2.6093954972522604,
      "grad_norm": 1.5389926433563232,
      "learning_rate": 2.602940481255614e-05,
      "loss": 0.2561,
      "step": 36800
    },
    {
      "epoch": 2.6101045913845065,
      "grad_norm": 1.2909457683563232,
      "learning_rate": 2.598213019429868e-05,
      "loss": 0.2455,
      "step": 36810
    },
    {
      "epoch": 2.610813685516752,
      "grad_norm": 0.9576348066329956,
      "learning_rate": 2.5934855576041224e-05,
      "loss": 0.237,
      "step": 36820
    },
    {
      "epoch": 2.6115227796489986,
      "grad_norm": 1.2474570274353027,
      "learning_rate": 2.588758095778377e-05,
      "loss": 0.1938,
      "step": 36830
    },
    {
      "epoch": 2.612231873781244,
      "grad_norm": 1.0625914335250854,
      "learning_rate": 2.584030633952631e-05,
      "loss": 0.2273,
      "step": 36840
    },
    {
      "epoch": 2.6129409679134907,
      "grad_norm": 1.2291488647460938,
      "learning_rate": 2.579303172126885e-05,
      "loss": 0.2338,
      "step": 36850
    },
    {
      "epoch": 2.6136500620457364,
      "grad_norm": 0.9369129538536072,
      "learning_rate": 2.5745757103011393e-05,
      "loss": 0.2453,
      "step": 36860
    },
    {
      "epoch": 2.614359156177983,
      "grad_norm": 1.2131121158599854,
      "learning_rate": 2.569848248475394e-05,
      "loss": 0.2523,
      "step": 36870
    },
    {
      "epoch": 2.6150682503102285,
      "grad_norm": 0.9454299807548523,
      "learning_rate": 2.565120786649648e-05,
      "loss": 0.2463,
      "step": 36880
    },
    {
      "epoch": 2.615777344442475,
      "grad_norm": 0.8799285292625427,
      "learning_rate": 2.5603933248239022e-05,
      "loss": 0.2442,
      "step": 36890
    },
    {
      "epoch": 2.6164864385747206,
      "grad_norm": 4.714387893676758,
      "learning_rate": 2.555665862998156e-05,
      "loss": 0.228,
      "step": 36900
    },
    {
      "epoch": 2.6171955327069667,
      "grad_norm": 1.2351151704788208,
      "learning_rate": 2.5509384011724108e-05,
      "loss": 0.2285,
      "step": 36910
    },
    {
      "epoch": 2.6179046268392128,
      "grad_norm": 1.3130894899368286,
      "learning_rate": 2.5462109393466648e-05,
      "loss": 0.2327,
      "step": 36920
    },
    {
      "epoch": 2.618613720971459,
      "grad_norm": 1.4136581420898438,
      "learning_rate": 2.541483477520919e-05,
      "loss": 0.2192,
      "step": 36930
    },
    {
      "epoch": 2.619322815103705,
      "grad_norm": 1.3576908111572266,
      "learning_rate": 2.5367560156951737e-05,
      "loss": 0.2512,
      "step": 36940
    },
    {
      "epoch": 2.620031909235951,
      "grad_norm": 1.1156386137008667,
      "learning_rate": 2.5320285538694277e-05,
      "loss": 0.262,
      "step": 36950
    },
    {
      "epoch": 2.620741003368197,
      "grad_norm": 0.8999633193016052,
      "learning_rate": 2.5273010920436816e-05,
      "loss": 0.2253,
      "step": 36960
    },
    {
      "epoch": 2.621450097500443,
      "grad_norm": 1.4215115308761597,
      "learning_rate": 2.522573630217936e-05,
      "loss": 0.2887,
      "step": 36970
    },
    {
      "epoch": 2.622159191632689,
      "grad_norm": 1.715043067932129,
      "learning_rate": 2.5178461683921906e-05,
      "loss": 0.2772,
      "step": 36980
    },
    {
      "epoch": 2.6228682857649352,
      "grad_norm": 2.9452006816864014,
      "learning_rate": 2.5131187065664446e-05,
      "loss": 0.2457,
      "step": 36990
    },
    {
      "epoch": 2.6235773798971813,
      "grad_norm": 1.031592845916748,
      "learning_rate": 2.508391244740699e-05,
      "loss": 0.2657,
      "step": 37000
    },
    {
      "epoch": 2.6242864740294274,
      "grad_norm": 0.9543763399124146,
      "learning_rate": 2.503663782914953e-05,
      "loss": 0.1955,
      "step": 37010
    },
    {
      "epoch": 2.6249955681616735,
      "grad_norm": 1.1948193311691284,
      "learning_rate": 2.498936321089207e-05,
      "loss": 0.2379,
      "step": 37020
    },
    {
      "epoch": 2.6257046622939195,
      "grad_norm": 0.9542125463485718,
      "learning_rate": 2.4942088592634615e-05,
      "loss": 0.222,
      "step": 37030
    },
    {
      "epoch": 2.6264137564261656,
      "grad_norm": 1.0540720224380493,
      "learning_rate": 2.4894813974377158e-05,
      "loss": 0.2508,
      "step": 37040
    },
    {
      "epoch": 2.6271228505584117,
      "grad_norm": 0.8935338258743286,
      "learning_rate": 2.48475393561197e-05,
      "loss": 0.248,
      "step": 37050
    },
    {
      "epoch": 2.6278319446906577,
      "grad_norm": 1.4003205299377441,
      "learning_rate": 2.480026473786224e-05,
      "loss": 0.2745,
      "step": 37060
    },
    {
      "epoch": 2.628541038822904,
      "grad_norm": 1.3511879444122314,
      "learning_rate": 2.4752990119604787e-05,
      "loss": 0.2542,
      "step": 37070
    },
    {
      "epoch": 2.62925013295515,
      "grad_norm": 1.1100364923477173,
      "learning_rate": 2.470571550134733e-05,
      "loss": 0.2256,
      "step": 37080
    },
    {
      "epoch": 2.629959227087396,
      "grad_norm": 2.5076324939727783,
      "learning_rate": 2.465844088308987e-05,
      "loss": 0.2675,
      "step": 37090
    },
    {
      "epoch": 2.630668321219642,
      "grad_norm": 1.1480841636657715,
      "learning_rate": 2.4611166264832413e-05,
      "loss": 0.2471,
      "step": 37100
    },
    {
      "epoch": 2.631377415351888,
      "grad_norm": 1.4491655826568604,
      "learning_rate": 2.4563891646574956e-05,
      "loss": 0.2281,
      "step": 37110
    },
    {
      "epoch": 2.632086509484134,
      "grad_norm": 0.9430562853813171,
      "learning_rate": 2.45166170283175e-05,
      "loss": 0.249,
      "step": 37120
    },
    {
      "epoch": 2.63279560361638,
      "grad_norm": 1.7224057912826538,
      "learning_rate": 2.446934241006004e-05,
      "loss": 0.2589,
      "step": 37130
    },
    {
      "epoch": 2.6335046977486263,
      "grad_norm": 0.9655771851539612,
      "learning_rate": 2.442206779180258e-05,
      "loss": 0.2396,
      "step": 37140
    },
    {
      "epoch": 2.6342137918808723,
      "grad_norm": 0.9165507555007935,
      "learning_rate": 2.4374793173545125e-05,
      "loss": 0.2667,
      "step": 37150
    },
    {
      "epoch": 2.6349228860131184,
      "grad_norm": 1.60805344581604,
      "learning_rate": 2.4327518555287668e-05,
      "loss": 0.2409,
      "step": 37160
    },
    {
      "epoch": 2.635631980145364,
      "grad_norm": 0.8260314464569092,
      "learning_rate": 2.4280243937030207e-05,
      "loss": 0.2459,
      "step": 37170
    },
    {
      "epoch": 2.6363410742776106,
      "grad_norm": 0.8212453126907349,
      "learning_rate": 2.4232969318772754e-05,
      "loss": 0.2267,
      "step": 37180
    },
    {
      "epoch": 2.637050168409856,
      "grad_norm": 1.3818778991699219,
      "learning_rate": 2.4185694700515294e-05,
      "loss": 0.2447,
      "step": 37190
    },
    {
      "epoch": 2.6377592625421027,
      "grad_norm": 1.1461234092712402,
      "learning_rate": 2.4138420082257837e-05,
      "loss": 0.2691,
      "step": 37200
    },
    {
      "epoch": 2.6384683566743483,
      "grad_norm": 0.8017842173576355,
      "learning_rate": 2.409114546400038e-05,
      "loss": 0.2467,
      "step": 37210
    },
    {
      "epoch": 2.639177450806595,
      "grad_norm": 0.9566860198974609,
      "learning_rate": 2.4043870845742923e-05,
      "loss": 0.2416,
      "step": 37220
    },
    {
      "epoch": 2.6398865449388405,
      "grad_norm": 1.0915526151657104,
      "learning_rate": 2.3996596227485466e-05,
      "loss": 0.2482,
      "step": 37230
    },
    {
      "epoch": 2.640595639071087,
      "grad_norm": 1.1275137662887573,
      "learning_rate": 2.3949321609228006e-05,
      "loss": 0.2385,
      "step": 37240
    },
    {
      "epoch": 2.6413047332033326,
      "grad_norm": 0.8996636867523193,
      "learning_rate": 2.390204699097055e-05,
      "loss": 0.238,
      "step": 37250
    },
    {
      "epoch": 2.6420138273355787,
      "grad_norm": 0.9261469841003418,
      "learning_rate": 2.385477237271309e-05,
      "loss": 0.2551,
      "step": 37260
    },
    {
      "epoch": 2.6427229214678247,
      "grad_norm": 0.9352648854255676,
      "learning_rate": 2.3807497754455635e-05,
      "loss": 0.2428,
      "step": 37270
    },
    {
      "epoch": 2.643432015600071,
      "grad_norm": 1.0669549703598022,
      "learning_rate": 2.3760223136198174e-05,
      "loss": 0.2512,
      "step": 37280
    },
    {
      "epoch": 2.644141109732317,
      "grad_norm": 0.9248042702674866,
      "learning_rate": 2.371294851794072e-05,
      "loss": 0.222,
      "step": 37290
    },
    {
      "epoch": 2.644850203864563,
      "grad_norm": 1.2682459354400635,
      "learning_rate": 2.366567389968326e-05,
      "loss": 0.2004,
      "step": 37300
    },
    {
      "epoch": 2.645559297996809,
      "grad_norm": 1.8408002853393555,
      "learning_rate": 2.3618399281425804e-05,
      "loss": 0.2452,
      "step": 37310
    },
    {
      "epoch": 2.646268392129055,
      "grad_norm": 1.2095003128051758,
      "learning_rate": 2.3571124663168347e-05,
      "loss": 0.2574,
      "step": 37320
    },
    {
      "epoch": 2.646977486261301,
      "grad_norm": 1.943424940109253,
      "learning_rate": 2.352385004491089e-05,
      "loss": 0.2498,
      "step": 37330
    },
    {
      "epoch": 2.647686580393547,
      "grad_norm": 1.202785849571228,
      "learning_rate": 2.347657542665343e-05,
      "loss": 0.227,
      "step": 37340
    },
    {
      "epoch": 2.6483956745257933,
      "grad_norm": 1.77749764919281,
      "learning_rate": 2.3429300808395972e-05,
      "loss": 0.2516,
      "step": 37350
    },
    {
      "epoch": 2.6491047686580393,
      "grad_norm": 1.1542448997497559,
      "learning_rate": 2.3382026190138516e-05,
      "loss": 0.2425,
      "step": 37360
    },
    {
      "epoch": 2.6498138627902854,
      "grad_norm": 0.9737889766693115,
      "learning_rate": 2.333475157188106e-05,
      "loss": 0.2342,
      "step": 37370
    },
    {
      "epoch": 2.6505229569225315,
      "grad_norm": 0.9568897485733032,
      "learning_rate": 2.32874769536236e-05,
      "loss": 0.2397,
      "step": 37380
    },
    {
      "epoch": 2.6512320510547775,
      "grad_norm": 1.2110828161239624,
      "learning_rate": 2.324020233536614e-05,
      "loss": 0.2517,
      "step": 37390
    },
    {
      "epoch": 2.6519411451870236,
      "grad_norm": 1.095579743385315,
      "learning_rate": 2.3192927717108688e-05,
      "loss": 0.2373,
      "step": 37400
    },
    {
      "epoch": 2.6526502393192697,
      "grad_norm": 1.6018216609954834,
      "learning_rate": 2.3145653098851228e-05,
      "loss": 0.2375,
      "step": 37410
    },
    {
      "epoch": 2.6533593334515158,
      "grad_norm": 0.8031683564186096,
      "learning_rate": 2.309837848059377e-05,
      "loss": 0.2379,
      "step": 37420
    },
    {
      "epoch": 2.654068427583762,
      "grad_norm": 1.995640754699707,
      "learning_rate": 2.3051103862336314e-05,
      "loss": 0.2281,
      "step": 37430
    },
    {
      "epoch": 2.654777521716008,
      "grad_norm": 1.154884934425354,
      "learning_rate": 2.3003829244078857e-05,
      "loss": 0.2356,
      "step": 37440
    },
    {
      "epoch": 2.655486615848254,
      "grad_norm": 1.2235770225524902,
      "learning_rate": 2.2956554625821396e-05,
      "loss": 0.2351,
      "step": 37450
    },
    {
      "epoch": 2.6561957099805,
      "grad_norm": 2.905320644378662,
      "learning_rate": 2.290928000756394e-05,
      "loss": 0.2428,
      "step": 37460
    },
    {
      "epoch": 2.656904804112746,
      "grad_norm": 2.0596415996551514,
      "learning_rate": 2.2862005389306483e-05,
      "loss": 0.236,
      "step": 37470
    },
    {
      "epoch": 2.657613898244992,
      "grad_norm": 1.0212461948394775,
      "learning_rate": 2.2814730771049026e-05,
      "loss": 0.2501,
      "step": 37480
    },
    {
      "epoch": 2.6583229923772382,
      "grad_norm": 1.1285755634307861,
      "learning_rate": 2.2767456152791565e-05,
      "loss": 0.2523,
      "step": 37490
    },
    {
      "epoch": 2.659032086509484,
      "grad_norm": 1.4409581422805786,
      "learning_rate": 2.2720181534534112e-05,
      "loss": 0.2091,
      "step": 37500
    },
    {
      "epoch": 2.6597411806417304,
      "grad_norm": 1.1458944082260132,
      "learning_rate": 2.267290691627665e-05,
      "loss": 0.2227,
      "step": 37510
    },
    {
      "epoch": 2.660450274773976,
      "grad_norm": 0.9119452238082886,
      "learning_rate": 2.2625632298019195e-05,
      "loss": 0.2549,
      "step": 37520
    },
    {
      "epoch": 2.6611593689062225,
      "grad_norm": 1.4499746561050415,
      "learning_rate": 2.2578357679761734e-05,
      "loss": 0.2458,
      "step": 37530
    },
    {
      "epoch": 2.661868463038468,
      "grad_norm": 1.0490057468414307,
      "learning_rate": 2.253108306150428e-05,
      "loss": 0.2477,
      "step": 37540
    },
    {
      "epoch": 2.6625775571707146,
      "grad_norm": 0.9215757250785828,
      "learning_rate": 2.2483808443246824e-05,
      "loss": 0.2283,
      "step": 37550
    },
    {
      "epoch": 2.6632866513029603,
      "grad_norm": 1.3149217367172241,
      "learning_rate": 2.2436533824989363e-05,
      "loss": 0.2526,
      "step": 37560
    },
    {
      "epoch": 2.663995745435207,
      "grad_norm": 1.4386985301971436,
      "learning_rate": 2.2389259206731906e-05,
      "loss": 0.1955,
      "step": 37570
    },
    {
      "epoch": 2.6647048395674524,
      "grad_norm": 3.1255226135253906,
      "learning_rate": 2.234198458847445e-05,
      "loss": 0.1847,
      "step": 37580
    },
    {
      "epoch": 2.6654139336996985,
      "grad_norm": 1.3001437187194824,
      "learning_rate": 2.2294709970216993e-05,
      "loss": 0.2453,
      "step": 37590
    },
    {
      "epoch": 2.6661230278319445,
      "grad_norm": 0.7512652277946472,
      "learning_rate": 2.2247435351959532e-05,
      "loss": 0.2528,
      "step": 37600
    },
    {
      "epoch": 2.6668321219641906,
      "grad_norm": 0.8458905220031738,
      "learning_rate": 2.220016073370208e-05,
      "loss": 0.2112,
      "step": 37610
    },
    {
      "epoch": 2.6675412160964367,
      "grad_norm": 0.9158129096031189,
      "learning_rate": 2.215288611544462e-05,
      "loss": 0.2394,
      "step": 37620
    },
    {
      "epoch": 2.6682503102286828,
      "grad_norm": 1.2252978086471558,
      "learning_rate": 2.210561149718716e-05,
      "loss": 0.2301,
      "step": 37630
    },
    {
      "epoch": 2.668959404360929,
      "grad_norm": 1.9542192220687866,
      "learning_rate": 2.2058336878929705e-05,
      "loss": 0.2281,
      "step": 37640
    },
    {
      "epoch": 2.669668498493175,
      "grad_norm": 0.9955037236213684,
      "learning_rate": 2.2011062260672248e-05,
      "loss": 0.2333,
      "step": 37650
    },
    {
      "epoch": 2.670377592625421,
      "grad_norm": 0.9392558932304382,
      "learning_rate": 2.1963787642414787e-05,
      "loss": 0.2191,
      "step": 37660
    },
    {
      "epoch": 2.671086686757667,
      "grad_norm": 0.9076080918312073,
      "learning_rate": 2.191651302415733e-05,
      "loss": 0.2353,
      "step": 37670
    },
    {
      "epoch": 2.671795780889913,
      "grad_norm": 1.0270133018493652,
      "learning_rate": 2.1869238405899873e-05,
      "loss": 0.2277,
      "step": 37680
    },
    {
      "epoch": 2.672504875022159,
      "grad_norm": 1.353434443473816,
      "learning_rate": 2.1821963787642417e-05,
      "loss": 0.2504,
      "step": 37690
    },
    {
      "epoch": 2.6732139691544052,
      "grad_norm": 1.0865520238876343,
      "learning_rate": 2.1774689169384956e-05,
      "loss": 0.244,
      "step": 37700
    },
    {
      "epoch": 2.6739230632866513,
      "grad_norm": 1.0994608402252197,
      "learning_rate": 2.17274145511275e-05,
      "loss": 0.2503,
      "step": 37710
    },
    {
      "epoch": 2.6746321574188974,
      "grad_norm": 0.8678391575813293,
      "learning_rate": 2.1680139932870046e-05,
      "loss": 0.2191,
      "step": 37720
    },
    {
      "epoch": 2.6753412515511434,
      "grad_norm": 0.6481241583824158,
      "learning_rate": 2.1632865314612585e-05,
      "loss": 0.2441,
      "step": 37730
    },
    {
      "epoch": 2.6760503456833895,
      "grad_norm": 1.0812181234359741,
      "learning_rate": 2.158559069635513e-05,
      "loss": 0.2475,
      "step": 37740
    },
    {
      "epoch": 2.6767594398156356,
      "grad_norm": 0.9230226278305054,
      "learning_rate": 2.153831607809767e-05,
      "loss": 0.2248,
      "step": 37750
    },
    {
      "epoch": 2.6774685339478816,
      "grad_norm": 1.2133828401565552,
      "learning_rate": 2.1491041459840215e-05,
      "loss": 0.2393,
      "step": 37760
    },
    {
      "epoch": 2.6781776280801277,
      "grad_norm": 1.2735835313796997,
      "learning_rate": 2.1443766841582754e-05,
      "loss": 0.252,
      "step": 37770
    },
    {
      "epoch": 2.678886722212374,
      "grad_norm": 0.7813161611557007,
      "learning_rate": 2.1396492223325297e-05,
      "loss": 0.2576,
      "step": 37780
    },
    {
      "epoch": 2.67959581634462,
      "grad_norm": 0.9590486288070679,
      "learning_rate": 2.134921760506784e-05,
      "loss": 0.2376,
      "step": 37790
    },
    {
      "epoch": 2.680304910476866,
      "grad_norm": 1.6642849445343018,
      "learning_rate": 2.1301942986810384e-05,
      "loss": 0.2497,
      "step": 37800
    },
    {
      "epoch": 2.681014004609112,
      "grad_norm": 0.9185329675674438,
      "learning_rate": 2.1254668368552923e-05,
      "loss": 0.2506,
      "step": 37810
    },
    {
      "epoch": 2.681723098741358,
      "grad_norm": 1.0194778442382812,
      "learning_rate": 2.1207393750295466e-05,
      "loss": 0.2226,
      "step": 37820
    },
    {
      "epoch": 2.682432192873604,
      "grad_norm": 1.3896738290786743,
      "learning_rate": 2.116011913203801e-05,
      "loss": 0.2625,
      "step": 37830
    },
    {
      "epoch": 2.68314128700585,
      "grad_norm": 0.9223114252090454,
      "learning_rate": 2.1112844513780552e-05,
      "loss": 0.2428,
      "step": 37840
    },
    {
      "epoch": 2.683850381138096,
      "grad_norm": 1.034219741821289,
      "learning_rate": 2.1065569895523092e-05,
      "loss": 0.2378,
      "step": 37850
    },
    {
      "epoch": 2.6845594752703423,
      "grad_norm": 0.9125880002975464,
      "learning_rate": 2.101829527726564e-05,
      "loss": 0.1884,
      "step": 37860
    },
    {
      "epoch": 2.685268569402588,
      "grad_norm": 1.2564674615859985,
      "learning_rate": 2.0971020659008178e-05,
      "loss": 0.239,
      "step": 37870
    },
    {
      "epoch": 2.6859776635348345,
      "grad_norm": 1.1152499914169312,
      "learning_rate": 2.092374604075072e-05,
      "loss": 0.2394,
      "step": 37880
    },
    {
      "epoch": 2.68668675766708,
      "grad_norm": 1.1371124982833862,
      "learning_rate": 2.0876471422493264e-05,
      "loss": 0.2328,
      "step": 37890
    },
    {
      "epoch": 2.6873958517993266,
      "grad_norm": 1.3449351787567139,
      "learning_rate": 2.0829196804235807e-05,
      "loss": 0.2713,
      "step": 37900
    },
    {
      "epoch": 2.6881049459315722,
      "grad_norm": 1.3807274103164673,
      "learning_rate": 2.078192218597835e-05,
      "loss": 0.2728,
      "step": 37910
    },
    {
      "epoch": 2.6888140400638187,
      "grad_norm": 1.3640811443328857,
      "learning_rate": 2.073464756772089e-05,
      "loss": 0.2465,
      "step": 37920
    },
    {
      "epoch": 2.6895231341960644,
      "grad_norm": 1.2892154455184937,
      "learning_rate": 2.0687372949463437e-05,
      "loss": 0.2271,
      "step": 37930
    },
    {
      "epoch": 2.6902322283283104,
      "grad_norm": 0.9196566939353943,
      "learning_rate": 2.0640098331205976e-05,
      "loss": 0.2328,
      "step": 37940
    },
    {
      "epoch": 2.6909413224605565,
      "grad_norm": 1.288406491279602,
      "learning_rate": 2.059282371294852e-05,
      "loss": 0.2318,
      "step": 37950
    },
    {
      "epoch": 2.6916504165928026,
      "grad_norm": 0.9240307211875916,
      "learning_rate": 2.054554909469106e-05,
      "loss": 0.2436,
      "step": 37960
    },
    {
      "epoch": 2.6923595107250486,
      "grad_norm": 1.6117206811904907,
      "learning_rate": 2.0498274476433606e-05,
      "loss": 0.2274,
      "step": 37970
    },
    {
      "epoch": 2.6930686048572947,
      "grad_norm": 1.261902928352356,
      "learning_rate": 2.0450999858176145e-05,
      "loss": 0.2456,
      "step": 37980
    },
    {
      "epoch": 2.693777698989541,
      "grad_norm": 0.866412341594696,
      "learning_rate": 2.0403725239918688e-05,
      "loss": 0.251,
      "step": 37990
    },
    {
      "epoch": 2.694486793121787,
      "grad_norm": 1.3619089126586914,
      "learning_rate": 2.035645062166123e-05,
      "loss": 0.2397,
      "step": 38000
    },
    {
      "epoch": 2.695195887254033,
      "grad_norm": 3.1521623134613037,
      "learning_rate": 2.0309176003403774e-05,
      "loss": 0.2222,
      "step": 38010
    },
    {
      "epoch": 2.695904981386279,
      "grad_norm": 1.0118757486343384,
      "learning_rate": 2.0261901385146314e-05,
      "loss": 0.245,
      "step": 38020
    },
    {
      "epoch": 2.696614075518525,
      "grad_norm": 1.2169800996780396,
      "learning_rate": 2.0214626766888857e-05,
      "loss": 0.2455,
      "step": 38030
    },
    {
      "epoch": 2.697323169650771,
      "grad_norm": 0.920723557472229,
      "learning_rate": 2.0167352148631404e-05,
      "loss": 0.2477,
      "step": 38040
    },
    {
      "epoch": 2.698032263783017,
      "grad_norm": 1.2141145467758179,
      "learning_rate": 2.0120077530373943e-05,
      "loss": 0.2723,
      "step": 38050
    },
    {
      "epoch": 2.6987413579152633,
      "grad_norm": 0.8709750771522522,
      "learning_rate": 2.0072802912116486e-05,
      "loss": 0.254,
      "step": 38060
    },
    {
      "epoch": 2.6994504520475093,
      "grad_norm": 1.2347657680511475,
      "learning_rate": 2.0025528293859026e-05,
      "loss": 0.2207,
      "step": 38070
    },
    {
      "epoch": 2.7001595461797554,
      "grad_norm": 1.2670501470565796,
      "learning_rate": 1.9978253675601573e-05,
      "loss": 0.2214,
      "step": 38080
    },
    {
      "epoch": 2.7008686403120015,
      "grad_norm": 1.1435718536376953,
      "learning_rate": 1.9930979057344112e-05,
      "loss": 0.2362,
      "step": 38090
    },
    {
      "epoch": 2.7015777344442475,
      "grad_norm": 0.9042963981628418,
      "learning_rate": 1.9883704439086655e-05,
      "loss": 0.2584,
      "step": 38100
    },
    {
      "epoch": 2.7022868285764936,
      "grad_norm": 0.9683585166931152,
      "learning_rate": 1.98364298208292e-05,
      "loss": 0.265,
      "step": 38110
    },
    {
      "epoch": 2.7029959227087397,
      "grad_norm": 1.135496973991394,
      "learning_rate": 1.978915520257174e-05,
      "loss": 0.2449,
      "step": 38120
    },
    {
      "epoch": 2.7037050168409857,
      "grad_norm": 1.1953880786895752,
      "learning_rate": 1.974188058431428e-05,
      "loss": 0.224,
      "step": 38130
    },
    {
      "epoch": 2.704414110973232,
      "grad_norm": 1.0858044624328613,
      "learning_rate": 1.9694605966056824e-05,
      "loss": 0.241,
      "step": 38140
    },
    {
      "epoch": 2.705123205105478,
      "grad_norm": 0.9545758366584778,
      "learning_rate": 1.9647331347799367e-05,
      "loss": 0.2558,
      "step": 38150
    },
    {
      "epoch": 2.705832299237724,
      "grad_norm": 1.0582488775253296,
      "learning_rate": 1.960005672954191e-05,
      "loss": 0.2231,
      "step": 38160
    },
    {
      "epoch": 2.70654139336997,
      "grad_norm": 1.388226866722107,
      "learning_rate": 1.955278211128445e-05,
      "loss": 0.2752,
      "step": 38170
    },
    {
      "epoch": 2.707250487502216,
      "grad_norm": 0.8953976035118103,
      "learning_rate": 1.9505507493026996e-05,
      "loss": 0.2616,
      "step": 38180
    },
    {
      "epoch": 2.707959581634462,
      "grad_norm": 1.0823699235916138,
      "learning_rate": 1.9458232874769536e-05,
      "loss": 0.2304,
      "step": 38190
    },
    {
      "epoch": 2.708668675766708,
      "grad_norm": 1.0621438026428223,
      "learning_rate": 1.941095825651208e-05,
      "loss": 0.2261,
      "step": 38200
    },
    {
      "epoch": 2.7093777698989543,
      "grad_norm": 1.4955811500549316,
      "learning_rate": 1.9363683638254622e-05,
      "loss": 0.2852,
      "step": 38210
    },
    {
      "epoch": 2.7100868640312,
      "grad_norm": 1.275828242301941,
      "learning_rate": 1.9316409019997165e-05,
      "loss": 0.2663,
      "step": 38220
    },
    {
      "epoch": 2.7107959581634464,
      "grad_norm": 1.0840213298797607,
      "learning_rate": 1.926913440173971e-05,
      "loss": 0.2294,
      "step": 38230
    },
    {
      "epoch": 2.711505052295692,
      "grad_norm": 1.845412015914917,
      "learning_rate": 1.9221859783482248e-05,
      "loss": 0.227,
      "step": 38240
    },
    {
      "epoch": 2.7122141464279386,
      "grad_norm": 1.1339671611785889,
      "learning_rate": 1.917458516522479e-05,
      "loss": 0.2394,
      "step": 38250
    },
    {
      "epoch": 2.712923240560184,
      "grad_norm": 1.2018229961395264,
      "learning_rate": 1.9127310546967334e-05,
      "loss": 0.2341,
      "step": 38260
    },
    {
      "epoch": 2.7136323346924303,
      "grad_norm": 1.2600210905075073,
      "learning_rate": 1.9080035928709877e-05,
      "loss": 0.2212,
      "step": 38270
    },
    {
      "epoch": 2.7143414288246763,
      "grad_norm": 1.6316311359405518,
      "learning_rate": 1.9032761310452417e-05,
      "loss": 0.2343,
      "step": 38280
    },
    {
      "epoch": 2.7150505229569224,
      "grad_norm": 1.0431931018829346,
      "learning_rate": 1.8985486692194963e-05,
      "loss": 0.2319,
      "step": 38290
    },
    {
      "epoch": 2.7157596170891685,
      "grad_norm": 1.3674774169921875,
      "learning_rate": 1.8938212073937503e-05,
      "loss": 0.2813,
      "step": 38300
    },
    {
      "epoch": 2.7164687112214145,
      "grad_norm": 1.2118936777114868,
      "learning_rate": 1.8890937455680046e-05,
      "loss": 0.233,
      "step": 38310
    },
    {
      "epoch": 2.7171778053536606,
      "grad_norm": 1.1095428466796875,
      "learning_rate": 1.884366283742259e-05,
      "loss": 0.2336,
      "step": 38320
    },
    {
      "epoch": 2.7178868994859067,
      "grad_norm": 1.0585713386535645,
      "learning_rate": 1.8796388219165132e-05,
      "loss": 0.1931,
      "step": 38330
    },
    {
      "epoch": 2.7185959936181527,
      "grad_norm": 1.9979701042175293,
      "learning_rate": 1.8749113600907672e-05,
      "loss": 0.2477,
      "step": 38340
    },
    {
      "epoch": 2.719305087750399,
      "grad_norm": 0.8337684869766235,
      "learning_rate": 1.8701838982650215e-05,
      "loss": 0.2368,
      "step": 38350
    },
    {
      "epoch": 2.720014181882645,
      "grad_norm": 1.359447717666626,
      "learning_rate": 1.8654564364392758e-05,
      "loss": 0.2524,
      "step": 38360
    },
    {
      "epoch": 2.720723276014891,
      "grad_norm": 1.1584999561309814,
      "learning_rate": 1.86072897461353e-05,
      "loss": 0.227,
      "step": 38370
    },
    {
      "epoch": 2.721432370147137,
      "grad_norm": 1.7439392805099487,
      "learning_rate": 1.8560015127877844e-05,
      "loss": 0.2863,
      "step": 38380
    },
    {
      "epoch": 2.722141464279383,
      "grad_norm": 4.652795791625977,
      "learning_rate": 1.8512740509620384e-05,
      "loss": 0.2502,
      "step": 38390
    },
    {
      "epoch": 2.722850558411629,
      "grad_norm": 1.3752455711364746,
      "learning_rate": 1.846546589136293e-05,
      "loss": 0.2529,
      "step": 38400
    },
    {
      "epoch": 2.723559652543875,
      "grad_norm": 0.9576643705368042,
      "learning_rate": 1.841819127310547e-05,
      "loss": 0.2257,
      "step": 38410
    },
    {
      "epoch": 2.7242687466761213,
      "grad_norm": 1.3742399215698242,
      "learning_rate": 1.8370916654848013e-05,
      "loss": 0.2434,
      "step": 38420
    },
    {
      "epoch": 2.7249778408083674,
      "grad_norm": 1.0914238691329956,
      "learning_rate": 1.8323642036590556e-05,
      "loss": 0.2452,
      "step": 38430
    },
    {
      "epoch": 2.7256869349406134,
      "grad_norm": 1.1777982711791992,
      "learning_rate": 1.82763674183331e-05,
      "loss": 0.2252,
      "step": 38440
    },
    {
      "epoch": 2.7263960290728595,
      "grad_norm": 1.3040096759796143,
      "learning_rate": 1.822909280007564e-05,
      "loss": 0.2638,
      "step": 38450
    },
    {
      "epoch": 2.7271051232051056,
      "grad_norm": 1.0759027004241943,
      "learning_rate": 1.8181818181818182e-05,
      "loss": 0.2881,
      "step": 38460
    },
    {
      "epoch": 2.7278142173373516,
      "grad_norm": 0.8851248621940613,
      "learning_rate": 1.8134543563560725e-05,
      "loss": 0.2187,
      "step": 38470
    },
    {
      "epoch": 2.7285233114695977,
      "grad_norm": 1.1925979852676392,
      "learning_rate": 1.8087268945303268e-05,
      "loss": 0.2528,
      "step": 38480
    },
    {
      "epoch": 2.7292324056018438,
      "grad_norm": 1.1716198921203613,
      "learning_rate": 1.8039994327045808e-05,
      "loss": 0.2487,
      "step": 38490
    },
    {
      "epoch": 2.72994149973409,
      "grad_norm": 1.1883823871612549,
      "learning_rate": 1.799271970878835e-05,
      "loss": 0.2899,
      "step": 38500
    },
    {
      "epoch": 2.730650593866336,
      "grad_norm": 1.414595127105713,
      "learning_rate": 1.7945445090530894e-05,
      "loss": 0.2514,
      "step": 38510
    },
    {
      "epoch": 2.731359687998582,
      "grad_norm": 1.7636243104934692,
      "learning_rate": 1.7898170472273437e-05,
      "loss": 0.2303,
      "step": 38520
    },
    {
      "epoch": 2.7320687821308276,
      "grad_norm": 0.958457350730896,
      "learning_rate": 1.785089585401598e-05,
      "loss": 0.2466,
      "step": 38530
    },
    {
      "epoch": 2.732777876263074,
      "grad_norm": 1.7085721492767334,
      "learning_rate": 1.7803621235758523e-05,
      "loss": 0.2588,
      "step": 38540
    },
    {
      "epoch": 2.7334869703953197,
      "grad_norm": 1.7162599563598633,
      "learning_rate": 1.7756346617501066e-05,
      "loss": 0.2314,
      "step": 38550
    },
    {
      "epoch": 2.7341960645275663,
      "grad_norm": 1.1528066396713257,
      "learning_rate": 1.7709071999243606e-05,
      "loss": 0.2326,
      "step": 38560
    },
    {
      "epoch": 2.734905158659812,
      "grad_norm": 1.1944881677627563,
      "learning_rate": 1.766179738098615e-05,
      "loss": 0.2268,
      "step": 38570
    },
    {
      "epoch": 2.7356142527920584,
      "grad_norm": 1.6799982786178589,
      "learning_rate": 1.7614522762728692e-05,
      "loss": 0.2578,
      "step": 38580
    },
    {
      "epoch": 2.736323346924304,
      "grad_norm": 1.122796654701233,
      "learning_rate": 1.7567248144471235e-05,
      "loss": 0.2382,
      "step": 38590
    },
    {
      "epoch": 2.7370324410565505,
      "grad_norm": 1.3554075956344604,
      "learning_rate": 1.7519973526213775e-05,
      "loss": 0.2744,
      "step": 38600
    },
    {
      "epoch": 2.737741535188796,
      "grad_norm": 1.240394949913025,
      "learning_rate": 1.747269890795632e-05,
      "loss": 0.2491,
      "step": 38610
    },
    {
      "epoch": 2.738450629321042,
      "grad_norm": 0.8643202185630798,
      "learning_rate": 1.742542428969886e-05,
      "loss": 0.2306,
      "step": 38620
    },
    {
      "epoch": 2.7391597234532883,
      "grad_norm": 0.8170473575592041,
      "learning_rate": 1.7378149671441404e-05,
      "loss": 0.2326,
      "step": 38630
    },
    {
      "epoch": 2.7398688175855344,
      "grad_norm": 1.111043095588684,
      "learning_rate": 1.7330875053183944e-05,
      "loss": 0.266,
      "step": 38640
    },
    {
      "epoch": 2.7405779117177804,
      "grad_norm": 0.9798130393028259,
      "learning_rate": 1.728360043492649e-05,
      "loss": 0.2683,
      "step": 38650
    },
    {
      "epoch": 2.7412870058500265,
      "grad_norm": 0.9573352932929993,
      "learning_rate": 1.723632581666903e-05,
      "loss": 0.2209,
      "step": 38660
    },
    {
      "epoch": 2.7419960999822726,
      "grad_norm": 0.969712495803833,
      "learning_rate": 1.7189051198411573e-05,
      "loss": 0.2311,
      "step": 38670
    },
    {
      "epoch": 2.7427051941145186,
      "grad_norm": 0.8288928866386414,
      "learning_rate": 1.7141776580154116e-05,
      "loss": 0.2224,
      "step": 38680
    },
    {
      "epoch": 2.7434142882467647,
      "grad_norm": 1.436031699180603,
      "learning_rate": 1.709450196189666e-05,
      "loss": 0.2295,
      "step": 38690
    },
    {
      "epoch": 2.7441233823790108,
      "grad_norm": 1.0662949085235596,
      "learning_rate": 1.7047227343639202e-05,
      "loss": 0.2166,
      "step": 38700
    },
    {
      "epoch": 2.744832476511257,
      "grad_norm": 1.0606027841567993,
      "learning_rate": 1.6999952725381742e-05,
      "loss": 0.2902,
      "step": 38710
    },
    {
      "epoch": 2.745541570643503,
      "grad_norm": 1.2871202230453491,
      "learning_rate": 1.695267810712429e-05,
      "loss": 0.2434,
      "step": 38720
    },
    {
      "epoch": 2.746250664775749,
      "grad_norm": 1.0442715883255005,
      "learning_rate": 1.6905403488866828e-05,
      "loss": 0.2201,
      "step": 38730
    },
    {
      "epoch": 2.746959758907995,
      "grad_norm": 1.2012498378753662,
      "learning_rate": 1.685812887060937e-05,
      "loss": 0.2327,
      "step": 38740
    },
    {
      "epoch": 2.747668853040241,
      "grad_norm": 1.458108901977539,
      "learning_rate": 1.6810854252351914e-05,
      "loss": 0.2463,
      "step": 38750
    },
    {
      "epoch": 2.748377947172487,
      "grad_norm": 1.133830189704895,
      "learning_rate": 1.6763579634094457e-05,
      "loss": 0.2384,
      "step": 38760
    },
    {
      "epoch": 2.7490870413047332,
      "grad_norm": 1.0760490894317627,
      "learning_rate": 1.6716305015836997e-05,
      "loss": 0.2067,
      "step": 38770
    },
    {
      "epoch": 2.7497961354369793,
      "grad_norm": 1.4070053100585938,
      "learning_rate": 1.666903039757954e-05,
      "loss": 0.2319,
      "step": 38780
    },
    {
      "epoch": 2.7505052295692254,
      "grad_norm": 1.1566414833068848,
      "learning_rate": 1.6621755779322083e-05,
      "loss": 0.2011,
      "step": 38790
    },
    {
      "epoch": 2.7512143237014715,
      "grad_norm": 0.9563481211662292,
      "learning_rate": 1.6574481161064626e-05,
      "loss": 0.2245,
      "step": 38800
    },
    {
      "epoch": 2.7519234178337175,
      "grad_norm": 2.2291438579559326,
      "learning_rate": 1.6527206542807166e-05,
      "loss": 0.2627,
      "step": 38810
    },
    {
      "epoch": 2.7526325119659636,
      "grad_norm": 2.2806780338287354,
      "learning_rate": 1.647993192454971e-05,
      "loss": 0.2421,
      "step": 38820
    },
    {
      "epoch": 2.7533416060982097,
      "grad_norm": 1.1524854898452759,
      "learning_rate": 1.6432657306292252e-05,
      "loss": 0.2422,
      "step": 38830
    },
    {
      "epoch": 2.7540507002304557,
      "grad_norm": 1.2883837223052979,
      "learning_rate": 1.6385382688034795e-05,
      "loss": 0.2276,
      "step": 38840
    },
    {
      "epoch": 2.754759794362702,
      "grad_norm": 1.3198128938674927,
      "learning_rate": 1.6338108069777338e-05,
      "loss": 0.2275,
      "step": 38850
    },
    {
      "epoch": 2.755468888494948,
      "grad_norm": 0.9112423062324524,
      "learning_rate": 1.629083345151988e-05,
      "loss": 0.2393,
      "step": 38860
    },
    {
      "epoch": 2.756177982627194,
      "grad_norm": 0.9921044111251831,
      "learning_rate": 1.6243558833262424e-05,
      "loss": 0.2277,
      "step": 38870
    },
    {
      "epoch": 2.7568870767594396,
      "grad_norm": 1.19999098777771,
      "learning_rate": 1.6196284215004964e-05,
      "loss": 0.2443,
      "step": 38880
    },
    {
      "epoch": 2.757596170891686,
      "grad_norm": 1.0369207859039307,
      "learning_rate": 1.6149009596747507e-05,
      "loss": 0.2528,
      "step": 38890
    },
    {
      "epoch": 2.7583052650239317,
      "grad_norm": 1.1027144193649292,
      "learning_rate": 1.610173497849005e-05,
      "loss": 0.2413,
      "step": 38900
    },
    {
      "epoch": 2.759014359156178,
      "grad_norm": 1.0726591348648071,
      "learning_rate": 1.6054460360232593e-05,
      "loss": 0.2291,
      "step": 38910
    },
    {
      "epoch": 2.759723453288424,
      "grad_norm": 1.2850910425186157,
      "learning_rate": 1.6007185741975133e-05,
      "loss": 0.2383,
      "step": 38920
    },
    {
      "epoch": 2.7604325474206703,
      "grad_norm": 1.3044795989990234,
      "learning_rate": 1.5959911123717676e-05,
      "loss": 0.2368,
      "step": 38930
    },
    {
      "epoch": 2.761141641552916,
      "grad_norm": 1.0597198009490967,
      "learning_rate": 1.591263650546022e-05,
      "loss": 0.246,
      "step": 38940
    },
    {
      "epoch": 2.761850735685162,
      "grad_norm": 1.0108600854873657,
      "learning_rate": 1.5865361887202762e-05,
      "loss": 0.2326,
      "step": 38950
    },
    {
      "epoch": 2.762559829817408,
      "grad_norm": 1.224741816520691,
      "learning_rate": 1.58180872689453e-05,
      "loss": 0.2417,
      "step": 38960
    },
    {
      "epoch": 2.763268923949654,
      "grad_norm": 0.7324011325836182,
      "learning_rate": 1.5770812650687848e-05,
      "loss": 0.2332,
      "step": 38970
    },
    {
      "epoch": 2.7639780180819002,
      "grad_norm": 1.151838779449463,
      "learning_rate": 1.5723538032430388e-05,
      "loss": 0.2603,
      "step": 38980
    },
    {
      "epoch": 2.7646871122141463,
      "grad_norm": 1.1296544075012207,
      "learning_rate": 1.567626341417293e-05,
      "loss": 0.2181,
      "step": 38990
    },
    {
      "epoch": 2.7653962063463924,
      "grad_norm": 1.3179441690444946,
      "learning_rate": 1.5628988795915474e-05,
      "loss": 0.261,
      "step": 39000
    },
    {
      "epoch": 2.7661053004786385,
      "grad_norm": 1.076574683189392,
      "learning_rate": 1.5581714177658017e-05,
      "loss": 0.2322,
      "step": 39010
    },
    {
      "epoch": 2.7668143946108845,
      "grad_norm": 1.342233419418335,
      "learning_rate": 1.553443955940056e-05,
      "loss": 0.243,
      "step": 39020
    },
    {
      "epoch": 2.7675234887431306,
      "grad_norm": 2.211721420288086,
      "learning_rate": 1.54871649411431e-05,
      "loss": 0.2459,
      "step": 39030
    },
    {
      "epoch": 2.7682325828753767,
      "grad_norm": 1.346535086631775,
      "learning_rate": 1.5439890322885643e-05,
      "loss": 0.2161,
      "step": 39040
    },
    {
      "epoch": 2.7689416770076227,
      "grad_norm": 1.5715467929840088,
      "learning_rate": 1.5392615704628186e-05,
      "loss": 0.2553,
      "step": 39050
    },
    {
      "epoch": 2.769650771139869,
      "grad_norm": 2.9252395629882812,
      "learning_rate": 1.534534108637073e-05,
      "loss": 0.2274,
      "step": 39060
    },
    {
      "epoch": 2.770359865272115,
      "grad_norm": 1.8295936584472656,
      "learning_rate": 1.529806646811327e-05,
      "loss": 0.2375,
      "step": 39070
    },
    {
      "epoch": 2.771068959404361,
      "grad_norm": 0.9772696495056152,
      "learning_rate": 1.5250791849855813e-05,
      "loss": 0.2266,
      "step": 39080
    },
    {
      "epoch": 2.771778053536607,
      "grad_norm": 1.2076071500778198,
      "learning_rate": 1.5203517231598355e-05,
      "loss": 0.2186,
      "step": 39090
    },
    {
      "epoch": 2.772487147668853,
      "grad_norm": 1.2304550409317017,
      "learning_rate": 1.5156242613340898e-05,
      "loss": 0.2735,
      "step": 39100
    },
    {
      "epoch": 2.773196241801099,
      "grad_norm": 1.1881242990493774,
      "learning_rate": 1.510896799508344e-05,
      "loss": 0.2654,
      "step": 39110
    },
    {
      "epoch": 2.773905335933345,
      "grad_norm": 1.401431918144226,
      "learning_rate": 1.5061693376825984e-05,
      "loss": 0.229,
      "step": 39120
    },
    {
      "epoch": 2.7746144300655913,
      "grad_norm": 0.9671643376350403,
      "learning_rate": 1.5014418758568524e-05,
      "loss": 0.2746,
      "step": 39130
    },
    {
      "epoch": 2.7753235241978373,
      "grad_norm": 0.9792654514312744,
      "learning_rate": 1.4967144140311068e-05,
      "loss": 0.2305,
      "step": 39140
    },
    {
      "epoch": 2.7760326183300834,
      "grad_norm": 1.4391162395477295,
      "learning_rate": 1.4919869522053608e-05,
      "loss": 0.2516,
      "step": 39150
    },
    {
      "epoch": 2.7767417124623295,
      "grad_norm": 1.0540400743484497,
      "learning_rate": 1.4872594903796153e-05,
      "loss": 0.2259,
      "step": 39160
    },
    {
      "epoch": 2.7774508065945755,
      "grad_norm": 1.2050551176071167,
      "learning_rate": 1.4825320285538696e-05,
      "loss": 0.256,
      "step": 39170
    },
    {
      "epoch": 2.7781599007268216,
      "grad_norm": 1.8691143989562988,
      "learning_rate": 1.4778045667281237e-05,
      "loss": 0.2606,
      "step": 39180
    },
    {
      "epoch": 2.7788689948590677,
      "grad_norm": 2.1149048805236816,
      "learning_rate": 1.473077104902378e-05,
      "loss": 0.2385,
      "step": 39190
    },
    {
      "epoch": 2.7795780889913138,
      "grad_norm": 1.276931643486023,
      "learning_rate": 1.4683496430766322e-05,
      "loss": 0.2207,
      "step": 39200
    },
    {
      "epoch": 2.7802871831235594,
      "grad_norm": 0.9612320065498352,
      "learning_rate": 1.4636221812508865e-05,
      "loss": 0.2358,
      "step": 39210
    },
    {
      "epoch": 2.780996277255806,
      "grad_norm": 0.9559575915336609,
      "learning_rate": 1.4588947194251406e-05,
      "loss": 0.2397,
      "step": 39220
    },
    {
      "epoch": 2.7817053713880515,
      "grad_norm": 1.098531723022461,
      "learning_rate": 1.4541672575993951e-05,
      "loss": 0.2576,
      "step": 39230
    },
    {
      "epoch": 2.782414465520298,
      "grad_norm": 1.4359968900680542,
      "learning_rate": 1.449439795773649e-05,
      "loss": 0.237,
      "step": 39240
    },
    {
      "epoch": 2.7831235596525437,
      "grad_norm": 1.248620867729187,
      "learning_rate": 1.4447123339479035e-05,
      "loss": 0.2521,
      "step": 39250
    },
    {
      "epoch": 2.78383265378479,
      "grad_norm": 1.1088625192642212,
      "learning_rate": 1.4399848721221577e-05,
      "loss": 0.2448,
      "step": 39260
    },
    {
      "epoch": 2.784541747917036,
      "grad_norm": 1.514022707939148,
      "learning_rate": 1.435257410296412e-05,
      "loss": 0.2514,
      "step": 39270
    },
    {
      "epoch": 2.7852508420492823,
      "grad_norm": 1.0026037693023682,
      "learning_rate": 1.4305299484706661e-05,
      "loss": 0.229,
      "step": 39280
    },
    {
      "epoch": 2.785959936181528,
      "grad_norm": 1.1607643365859985,
      "learning_rate": 1.4258024866449204e-05,
      "loss": 0.2432,
      "step": 39290
    },
    {
      "epoch": 2.786669030313774,
      "grad_norm": 0.965223491191864,
      "learning_rate": 1.4210750248191746e-05,
      "loss": 0.2246,
      "step": 39300
    },
    {
      "epoch": 2.78737812444602,
      "grad_norm": 1.4045666456222534,
      "learning_rate": 1.4163475629934289e-05,
      "loss": 0.2227,
      "step": 39310
    },
    {
      "epoch": 2.788087218578266,
      "grad_norm": 1.2222893238067627,
      "learning_rate": 1.411620101167683e-05,
      "loss": 0.2291,
      "step": 39320
    },
    {
      "epoch": 2.788796312710512,
      "grad_norm": 1.2421678304672241,
      "learning_rate": 1.4068926393419373e-05,
      "loss": 0.2597,
      "step": 39330
    },
    {
      "epoch": 2.7895054068427583,
      "grad_norm": 1.3502918481826782,
      "learning_rate": 1.4021651775161918e-05,
      "loss": 0.2375,
      "step": 39340
    },
    {
      "epoch": 2.7902145009750043,
      "grad_norm": 0.9889366626739502,
      "learning_rate": 1.3974377156904458e-05,
      "loss": 0.2126,
      "step": 39350
    },
    {
      "epoch": 2.7909235951072504,
      "grad_norm": 1.124774694442749,
      "learning_rate": 1.3927102538647002e-05,
      "loss": 0.2345,
      "step": 39360
    },
    {
      "epoch": 2.7916326892394965,
      "grad_norm": 1.0267406702041626,
      "learning_rate": 1.3879827920389544e-05,
      "loss": 0.2918,
      "step": 39370
    },
    {
      "epoch": 2.7923417833717425,
      "grad_norm": 2.4154393672943115,
      "learning_rate": 1.3832553302132087e-05,
      "loss": 0.266,
      "step": 39380
    },
    {
      "epoch": 2.7930508775039886,
      "grad_norm": 0.9820238947868347,
      "learning_rate": 1.3785278683874628e-05,
      "loss": 0.2379,
      "step": 39390
    },
    {
      "epoch": 2.7937599716362347,
      "grad_norm": 1.8791142702102661,
      "learning_rate": 1.3738004065617171e-05,
      "loss": 0.2317,
      "step": 39400
    },
    {
      "epoch": 2.7944690657684808,
      "grad_norm": 1.72328519821167,
      "learning_rate": 1.3690729447359713e-05,
      "loss": 0.227,
      "step": 39410
    },
    {
      "epoch": 2.795178159900727,
      "grad_norm": 1.032732605934143,
      "learning_rate": 1.3643454829102256e-05,
      "loss": 0.2426,
      "step": 39420
    },
    {
      "epoch": 2.795887254032973,
      "grad_norm": 5.119006633758545,
      "learning_rate": 1.3596180210844797e-05,
      "loss": 0.2288,
      "step": 39430
    },
    {
      "epoch": 2.796596348165219,
      "grad_norm": 1.0681203603744507,
      "learning_rate": 1.354890559258734e-05,
      "loss": 0.2334,
      "step": 39440
    },
    {
      "epoch": 2.797305442297465,
      "grad_norm": 0.9785066843032837,
      "learning_rate": 1.3501630974329882e-05,
      "loss": 0.2226,
      "step": 39450
    },
    {
      "epoch": 2.798014536429711,
      "grad_norm": 1.1163362264633179,
      "learning_rate": 1.3454356356072426e-05,
      "loss": 0.2148,
      "step": 39460
    },
    {
      "epoch": 2.798723630561957,
      "grad_norm": 1.6435588598251343,
      "learning_rate": 1.3407081737814966e-05,
      "loss": 0.2484,
      "step": 39470
    },
    {
      "epoch": 2.7994327246942032,
      "grad_norm": 0.9990524649620056,
      "learning_rate": 1.335980711955751e-05,
      "loss": 0.249,
      "step": 39480
    },
    {
      "epoch": 2.8001418188264493,
      "grad_norm": 1.1118159294128418,
      "learning_rate": 1.3312532501300054e-05,
      "loss": 0.2441,
      "step": 39490
    },
    {
      "epoch": 2.8008509129586954,
      "grad_norm": 1.53793466091156,
      "learning_rate": 1.3265257883042595e-05,
      "loss": 0.2689,
      "step": 39500
    },
    {
      "epoch": 2.8015600070909414,
      "grad_norm": 1.2637410163879395,
      "learning_rate": 1.3217983264785138e-05,
      "loss": 0.2374,
      "step": 39510
    },
    {
      "epoch": 2.8022691012231875,
      "grad_norm": 1.02956223487854,
      "learning_rate": 1.317070864652768e-05,
      "loss": 0.2126,
      "step": 39520
    },
    {
      "epoch": 2.8029781953554336,
      "grad_norm": 1.5376802682876587,
      "learning_rate": 1.3123434028270223e-05,
      "loss": 0.2157,
      "step": 39530
    },
    {
      "epoch": 2.8036872894876796,
      "grad_norm": 1.5913312435150146,
      "learning_rate": 1.3076159410012764e-05,
      "loss": 0.2478,
      "step": 39540
    },
    {
      "epoch": 2.8043963836199257,
      "grad_norm": 1.350875735282898,
      "learning_rate": 1.3028884791755309e-05,
      "loss": 0.2321,
      "step": 39550
    },
    {
      "epoch": 2.8051054777521713,
      "grad_norm": 0.9665699005126953,
      "learning_rate": 1.2981610173497849e-05,
      "loss": 0.2377,
      "step": 39560
    },
    {
      "epoch": 2.805814571884418,
      "grad_norm": 1.068256139755249,
      "learning_rate": 1.2934335555240393e-05,
      "loss": 0.2328,
      "step": 39570
    },
    {
      "epoch": 2.8065236660166635,
      "grad_norm": 1.192975640296936,
      "learning_rate": 1.2887060936982933e-05,
      "loss": 0.2316,
      "step": 39580
    },
    {
      "epoch": 2.80723276014891,
      "grad_norm": 1.6531769037246704,
      "learning_rate": 1.2839786318725478e-05,
      "loss": 0.2647,
      "step": 39590
    },
    {
      "epoch": 2.8079418542811556,
      "grad_norm": 1.126813530921936,
      "learning_rate": 1.279251170046802e-05,
      "loss": 0.2441,
      "step": 39600
    },
    {
      "epoch": 2.808650948413402,
      "grad_norm": 1.3236688375473022,
      "learning_rate": 1.2745237082210562e-05,
      "loss": 0.2143,
      "step": 39610
    },
    {
      "epoch": 2.8093600425456478,
      "grad_norm": 1.2229331731796265,
      "learning_rate": 1.2697962463953104e-05,
      "loss": 0.243,
      "step": 39620
    },
    {
      "epoch": 2.8100691366778943,
      "grad_norm": 0.8436616063117981,
      "learning_rate": 1.2650687845695647e-05,
      "loss": 0.2944,
      "step": 39630
    },
    {
      "epoch": 2.81077823081014,
      "grad_norm": 1.39069402217865,
      "learning_rate": 1.2603413227438188e-05,
      "loss": 0.249,
      "step": 39640
    },
    {
      "epoch": 2.811487324942386,
      "grad_norm": 0.893751859664917,
      "learning_rate": 1.2556138609180731e-05,
      "loss": 0.2274,
      "step": 39650
    },
    {
      "epoch": 2.812196419074632,
      "grad_norm": 1.3140510320663452,
      "learning_rate": 1.2508863990923276e-05,
      "loss": 0.2128,
      "step": 39660
    },
    {
      "epoch": 2.812905513206878,
      "grad_norm": 0.8687760233879089,
      "learning_rate": 1.2461589372665816e-05,
      "loss": 0.2658,
      "step": 39670
    },
    {
      "epoch": 2.813614607339124,
      "grad_norm": 1.0689297914505005,
      "learning_rate": 1.2414314754408359e-05,
      "loss": 0.2308,
      "step": 39680
    },
    {
      "epoch": 2.8143237014713702,
      "grad_norm": 4.104773044586182,
      "learning_rate": 1.23670401361509e-05,
      "loss": 0.2355,
      "step": 39690
    },
    {
      "epoch": 2.8150327956036163,
      "grad_norm": 1.2592219114303589,
      "learning_rate": 1.2319765517893443e-05,
      "loss": 0.282,
      "step": 39700
    },
    {
      "epoch": 2.8157418897358624,
      "grad_norm": 1.1330379247665405,
      "learning_rate": 1.2272490899635986e-05,
      "loss": 0.2334,
      "step": 39710
    },
    {
      "epoch": 2.8164509838681084,
      "grad_norm": 1.3979483842849731,
      "learning_rate": 1.2225216281378528e-05,
      "loss": 0.2477,
      "step": 39720
    },
    {
      "epoch": 2.8171600780003545,
      "grad_norm": 1.104113221168518,
      "learning_rate": 1.217794166312107e-05,
      "loss": 0.3065,
      "step": 39730
    },
    {
      "epoch": 2.8178691721326006,
      "grad_norm": 0.8149908185005188,
      "learning_rate": 1.2130667044863614e-05,
      "loss": 0.2481,
      "step": 39740
    },
    {
      "epoch": 2.8185782662648466,
      "grad_norm": 1.1083087921142578,
      "learning_rate": 1.2083392426606157e-05,
      "loss": 0.2195,
      "step": 39750
    },
    {
      "epoch": 2.8192873603970927,
      "grad_norm": 1.0244288444519043,
      "learning_rate": 1.2036117808348698e-05,
      "loss": 0.228,
      "step": 39760
    },
    {
      "epoch": 2.819996454529339,
      "grad_norm": 0.9510757923126221,
      "learning_rate": 1.1988843190091241e-05,
      "loss": 0.2493,
      "step": 39770
    },
    {
      "epoch": 2.820705548661585,
      "grad_norm": 0.94503253698349,
      "learning_rate": 1.1941568571833783e-05,
      "loss": 0.2078,
      "step": 39780
    },
    {
      "epoch": 2.821414642793831,
      "grad_norm": 1.195827603340149,
      "learning_rate": 1.1894293953576326e-05,
      "loss": 0.2544,
      "step": 39790
    },
    {
      "epoch": 2.822123736926077,
      "grad_norm": 2.819072961807251,
      "learning_rate": 1.1847019335318869e-05,
      "loss": 0.2244,
      "step": 39800
    },
    {
      "epoch": 2.822832831058323,
      "grad_norm": 0.9264347553253174,
      "learning_rate": 1.179974471706141e-05,
      "loss": 0.219,
      "step": 39810
    },
    {
      "epoch": 2.823541925190569,
      "grad_norm": 1.055951714515686,
      "learning_rate": 1.1752470098803953e-05,
      "loss": 0.2659,
      "step": 39820
    },
    {
      "epoch": 2.824251019322815,
      "grad_norm": 0.9989161491394043,
      "learning_rate": 1.1705195480546495e-05,
      "loss": 0.2411,
      "step": 39830
    },
    {
      "epoch": 2.8249601134550613,
      "grad_norm": 1.1540800333023071,
      "learning_rate": 1.1657920862289038e-05,
      "loss": 0.2238,
      "step": 39840
    },
    {
      "epoch": 2.8256692075873073,
      "grad_norm": 1.3607879877090454,
      "learning_rate": 1.1610646244031579e-05,
      "loss": 0.2305,
      "step": 39850
    },
    {
      "epoch": 2.8263783017195534,
      "grad_norm": 0.7705298066139221,
      "learning_rate": 1.1563371625774122e-05,
      "loss": 0.2397,
      "step": 39860
    },
    {
      "epoch": 2.8270873958517995,
      "grad_norm": 1.8951096534729004,
      "learning_rate": 1.1516097007516665e-05,
      "loss": 0.2464,
      "step": 39870
    },
    {
      "epoch": 2.8277964899840455,
      "grad_norm": 0.8845863938331604,
      "learning_rate": 1.1468822389259206e-05,
      "loss": 0.1953,
      "step": 39880
    },
    {
      "epoch": 2.828505584116291,
      "grad_norm": 1.1041560173034668,
      "learning_rate": 1.142154777100175e-05,
      "loss": 0.2313,
      "step": 39890
    },
    {
      "epoch": 2.8292146782485377,
      "grad_norm": 1.146111249923706,
      "learning_rate": 1.1374273152744293e-05,
      "loss": 0.2544,
      "step": 39900
    },
    {
      "epoch": 2.8299237723807833,
      "grad_norm": 0.9895105957984924,
      "learning_rate": 1.1326998534486836e-05,
      "loss": 0.2238,
      "step": 39910
    },
    {
      "epoch": 2.83063286651303,
      "grad_norm": 0.9329178929328918,
      "learning_rate": 1.1279723916229377e-05,
      "loss": 0.2504,
      "step": 39920
    },
    {
      "epoch": 2.8313419606452754,
      "grad_norm": 1.0881808996200562,
      "learning_rate": 1.123244929797192e-05,
      "loss": 0.25,
      "step": 39930
    },
    {
      "epoch": 2.832051054777522,
      "grad_norm": 1.2824828624725342,
      "learning_rate": 1.1185174679714462e-05,
      "loss": 0.2214,
      "step": 39940
    },
    {
      "epoch": 2.8327601489097676,
      "grad_norm": 1.101851463317871,
      "learning_rate": 1.1137900061457005e-05,
      "loss": 0.2354,
      "step": 39950
    },
    {
      "epoch": 2.833469243042014,
      "grad_norm": 1.1537214517593384,
      "learning_rate": 1.1090625443199548e-05,
      "loss": 0.2299,
      "step": 39960
    },
    {
      "epoch": 2.8341783371742597,
      "grad_norm": 1.1958283185958862,
      "learning_rate": 1.1043350824942089e-05,
      "loss": 0.238,
      "step": 39970
    },
    {
      "epoch": 2.834887431306506,
      "grad_norm": 1.0277539491653442,
      "learning_rate": 1.0996076206684632e-05,
      "loss": 0.2888,
      "step": 39980
    },
    {
      "epoch": 2.835596525438752,
      "grad_norm": 1.224247694015503,
      "learning_rate": 1.0948801588427173e-05,
      "loss": 0.2494,
      "step": 39990
    },
    {
      "epoch": 2.836305619570998,
      "grad_norm": 1.2937811613082886,
      "learning_rate": 1.0901526970169717e-05,
      "loss": 0.212,
      "step": 40000
    },
    {
      "epoch": 2.837014713703244,
      "grad_norm": 1.0853484869003296,
      "learning_rate": 1.0854252351912258e-05,
      "loss": 0.2381,
      "step": 40010
    },
    {
      "epoch": 2.83772380783549,
      "grad_norm": 2.729794979095459,
      "learning_rate": 1.0806977733654801e-05,
      "loss": 0.2416,
      "step": 40020
    },
    {
      "epoch": 2.838432901967736,
      "grad_norm": 1.2801264524459839,
      "learning_rate": 1.0759703115397344e-05,
      "loss": 0.2227,
      "step": 40030
    },
    {
      "epoch": 2.839141996099982,
      "grad_norm": 1.038732647895813,
      "learning_rate": 1.0712428497139885e-05,
      "loss": 0.2434,
      "step": 40040
    },
    {
      "epoch": 2.8398510902322283,
      "grad_norm": 1.0196524858474731,
      "learning_rate": 1.0665153878882429e-05,
      "loss": 0.2324,
      "step": 40050
    },
    {
      "epoch": 2.8405601843644743,
      "grad_norm": 1.056188941001892,
      "learning_rate": 1.0617879260624972e-05,
      "loss": 0.2341,
      "step": 40060
    },
    {
      "epoch": 2.8412692784967204,
      "grad_norm": 1.4471964836120605,
      "learning_rate": 1.0570604642367515e-05,
      "loss": 0.2531,
      "step": 40070
    },
    {
      "epoch": 2.8419783726289665,
      "grad_norm": 1.102309226989746,
      "learning_rate": 1.0523330024110056e-05,
      "loss": 0.2138,
      "step": 40080
    },
    {
      "epoch": 2.8426874667612125,
      "grad_norm": 1.6153995990753174,
      "learning_rate": 1.0476055405852599e-05,
      "loss": 0.249,
      "step": 40090
    },
    {
      "epoch": 2.8433965608934586,
      "grad_norm": 1.3188095092773438,
      "learning_rate": 1.042878078759514e-05,
      "loss": 0.203,
      "step": 40100
    },
    {
      "epoch": 2.8441056550257047,
      "grad_norm": 1.8909789323806763,
      "learning_rate": 1.0381506169337684e-05,
      "loss": 0.2758,
      "step": 40110
    },
    {
      "epoch": 2.8448147491579507,
      "grad_norm": 2.3786730766296387,
      "learning_rate": 1.0334231551080225e-05,
      "loss": 0.2483,
      "step": 40120
    },
    {
      "epoch": 2.845523843290197,
      "grad_norm": 1.9515655040740967,
      "learning_rate": 1.0286956932822768e-05,
      "loss": 0.2396,
      "step": 40130
    },
    {
      "epoch": 2.846232937422443,
      "grad_norm": 1.4907360076904297,
      "learning_rate": 1.0239682314565311e-05,
      "loss": 0.2778,
      "step": 40140
    },
    {
      "epoch": 2.846942031554689,
      "grad_norm": 0.9354016780853271,
      "learning_rate": 1.0192407696307852e-05,
      "loss": 0.2471,
      "step": 40150
    },
    {
      "epoch": 2.847651125686935,
      "grad_norm": 1.3129600286483765,
      "learning_rate": 1.0145133078050395e-05,
      "loss": 0.2468,
      "step": 40160
    },
    {
      "epoch": 2.848360219819181,
      "grad_norm": 1.1710073947906494,
      "learning_rate": 1.0097858459792937e-05,
      "loss": 0.2039,
      "step": 40170
    },
    {
      "epoch": 2.849069313951427,
      "grad_norm": 1.7853299379348755,
      "learning_rate": 1.005058384153548e-05,
      "loss": 0.2404,
      "step": 40180
    },
    {
      "epoch": 2.849778408083673,
      "grad_norm": 1.8672043085098267,
      "learning_rate": 1.0003309223278021e-05,
      "loss": 0.1902,
      "step": 40190
    },
    {
      "epoch": 2.8504875022159193,
      "grad_norm": 0.790713906288147,
      "learning_rate": 9.956034605020564e-06,
      "loss": 0.2448,
      "step": 40200
    },
    {
      "epoch": 2.8511965963481654,
      "grad_norm": 1.0893614292144775,
      "learning_rate": 9.908759986763107e-06,
      "loss": 0.2368,
      "step": 40210
    },
    {
      "epoch": 2.8519056904804114,
      "grad_norm": 0.9964269399642944,
      "learning_rate": 9.86148536850565e-06,
      "loss": 0.2502,
      "step": 40220
    },
    {
      "epoch": 2.8526147846126575,
      "grad_norm": 1.0597578287124634,
      "learning_rate": 9.814210750248194e-06,
      "loss": 0.2349,
      "step": 40230
    },
    {
      "epoch": 2.853323878744903,
      "grad_norm": 1.0666451454162598,
      "learning_rate": 9.766936131990735e-06,
      "loss": 0.2374,
      "step": 40240
    },
    {
      "epoch": 2.8540329728771496,
      "grad_norm": 0.9483038187026978,
      "learning_rate": 9.719661513733278e-06,
      "loss": 0.2567,
      "step": 40250
    },
    {
      "epoch": 2.8547420670093953,
      "grad_norm": 2.1961188316345215,
      "learning_rate": 9.67238689547582e-06,
      "loss": 0.2552,
      "step": 40260
    },
    {
      "epoch": 2.8554511611416418,
      "grad_norm": 1.3148235082626343,
      "learning_rate": 9.625112277218362e-06,
      "loss": 0.2479,
      "step": 40270
    },
    {
      "epoch": 2.8561602552738874,
      "grad_norm": 0.956484317779541,
      "learning_rate": 9.577837658960904e-06,
      "loss": 0.2212,
      "step": 40280
    },
    {
      "epoch": 2.856869349406134,
      "grad_norm": 1.2307153940200806,
      "learning_rate": 9.530563040703447e-06,
      "loss": 0.2356,
      "step": 40290
    },
    {
      "epoch": 2.8575784435383795,
      "grad_norm": 4.285590171813965,
      "learning_rate": 9.48328842244599e-06,
      "loss": 0.2496,
      "step": 40300
    },
    {
      "epoch": 2.858287537670626,
      "grad_norm": 0.9369952082633972,
      "learning_rate": 9.436013804188531e-06,
      "loss": 0.2605,
      "step": 40310
    },
    {
      "epoch": 2.8589966318028717,
      "grad_norm": 1.5862101316452026,
      "learning_rate": 9.388739185931074e-06,
      "loss": 0.2494,
      "step": 40320
    },
    {
      "epoch": 2.8597057259351177,
      "grad_norm": 1.282914638519287,
      "learning_rate": 9.341464567673616e-06,
      "loss": 0.2425,
      "step": 40330
    },
    {
      "epoch": 2.860414820067364,
      "grad_norm": 2.0601320266723633,
      "learning_rate": 9.294189949416159e-06,
      "loss": 0.2296,
      "step": 40340
    },
    {
      "epoch": 2.86112391419961,
      "grad_norm": 1.2470297813415527,
      "learning_rate": 9.2469153311587e-06,
      "loss": 0.212,
      "step": 40350
    },
    {
      "epoch": 2.861833008331856,
      "grad_norm": 1.0476174354553223,
      "learning_rate": 9.199640712901243e-06,
      "loss": 0.2255,
      "step": 40360
    },
    {
      "epoch": 2.862542102464102,
      "grad_norm": 1.050109624862671,
      "learning_rate": 9.152366094643786e-06,
      "loss": 0.2871,
      "step": 40370
    },
    {
      "epoch": 2.863251196596348,
      "grad_norm": 2.604775905609131,
      "learning_rate": 9.105091476386328e-06,
      "loss": 0.203,
      "step": 40380
    },
    {
      "epoch": 2.863960290728594,
      "grad_norm": 1.399826169013977,
      "learning_rate": 9.05781685812887e-06,
      "loss": 0.2685,
      "step": 40390
    },
    {
      "epoch": 2.86466938486084,
      "grad_norm": 1.292401671409607,
      "learning_rate": 9.010542239871414e-06,
      "loss": 0.2413,
      "step": 40400
    },
    {
      "epoch": 2.8653784789930863,
      "grad_norm": 1.1407536268234253,
      "learning_rate": 8.963267621613957e-06,
      "loss": 0.2204,
      "step": 40410
    },
    {
      "epoch": 2.8660875731253324,
      "grad_norm": 0.9405627250671387,
      "learning_rate": 8.915993003356498e-06,
      "loss": 0.2051,
      "step": 40420
    },
    {
      "epoch": 2.8667966672575784,
      "grad_norm": 1.4924068450927734,
      "learning_rate": 8.868718385099041e-06,
      "loss": 0.2561,
      "step": 40430
    },
    {
      "epoch": 2.8675057613898245,
      "grad_norm": 1.1947433948516846,
      "learning_rate": 8.821443766841583e-06,
      "loss": 0.2545,
      "step": 40440
    },
    {
      "epoch": 2.8682148555220706,
      "grad_norm": 0.9855808019638062,
      "learning_rate": 8.774169148584126e-06,
      "loss": 0.2497,
      "step": 40450
    },
    {
      "epoch": 2.8689239496543166,
      "grad_norm": 0.9927337169647217,
      "learning_rate": 8.726894530326667e-06,
      "loss": 0.2247,
      "step": 40460
    },
    {
      "epoch": 2.8696330437865627,
      "grad_norm": 0.9524276852607727,
      "learning_rate": 8.67961991206921e-06,
      "loss": 0.2098,
      "step": 40470
    },
    {
      "epoch": 2.8703421379188088,
      "grad_norm": 0.8976592421531677,
      "learning_rate": 8.632345293811753e-06,
      "loss": 0.2347,
      "step": 40480
    },
    {
      "epoch": 2.871051232051055,
      "grad_norm": 1.4226527214050293,
      "learning_rate": 8.585070675554295e-06,
      "loss": 0.2552,
      "step": 40490
    },
    {
      "epoch": 2.871760326183301,
      "grad_norm": 2.2437565326690674,
      "learning_rate": 8.537796057296838e-06,
      "loss": 0.246,
      "step": 40500
    },
    {
      "epoch": 2.872469420315547,
      "grad_norm": 1.1517701148986816,
      "learning_rate": 8.49052143903938e-06,
      "loss": 0.2046,
      "step": 40510
    },
    {
      "epoch": 2.873178514447793,
      "grad_norm": 1.6482347249984741,
      "learning_rate": 8.443246820781922e-06,
      "loss": 0.2242,
      "step": 40520
    },
    {
      "epoch": 2.873887608580039,
      "grad_norm": 1.0563230514526367,
      "learning_rate": 8.395972202524464e-06,
      "loss": 0.2545,
      "step": 40530
    },
    {
      "epoch": 2.874596702712285,
      "grad_norm": 1.097847819328308,
      "learning_rate": 8.348697584267007e-06,
      "loss": 0.2618,
      "step": 40540
    },
    {
      "epoch": 2.8753057968445312,
      "grad_norm": 1.0315731763839722,
      "learning_rate": 8.30142296600955e-06,
      "loss": 0.2374,
      "step": 40550
    },
    {
      "epoch": 2.8760148909767773,
      "grad_norm": 2.132720470428467,
      "learning_rate": 8.254148347752093e-06,
      "loss": 0.2236,
      "step": 40560
    },
    {
      "epoch": 2.8767239851090234,
      "grad_norm": 1.0132708549499512,
      "learning_rate": 8.206873729494636e-06,
      "loss": 0.2223,
      "step": 40570
    },
    {
      "epoch": 2.8774330792412695,
      "grad_norm": 0.8224313855171204,
      "learning_rate": 8.159599111237177e-06,
      "loss": 0.2305,
      "step": 40580
    },
    {
      "epoch": 2.878142173373515,
      "grad_norm": 1.1808074712753296,
      "learning_rate": 8.11232449297972e-06,
      "loss": 0.2111,
      "step": 40590
    },
    {
      "epoch": 2.8788512675057616,
      "grad_norm": 0.9516061544418335,
      "learning_rate": 8.065049874722262e-06,
      "loss": 0.224,
      "step": 40600
    },
    {
      "epoch": 2.879560361638007,
      "grad_norm": 1.0660839080810547,
      "learning_rate": 8.017775256464805e-06,
      "loss": 0.2502,
      "step": 40610
    },
    {
      "epoch": 2.8802694557702537,
      "grad_norm": 1.5800306797027588,
      "learning_rate": 7.970500638207346e-06,
      "loss": 0.2257,
      "step": 40620
    },
    {
      "epoch": 2.8809785499024994,
      "grad_norm": 1.30568265914917,
      "learning_rate": 7.92322601994989e-06,
      "loss": 0.2683,
      "step": 40630
    },
    {
      "epoch": 2.881687644034746,
      "grad_norm": 0.8691309690475464,
      "learning_rate": 7.875951401692432e-06,
      "loss": 0.236,
      "step": 40640
    },
    {
      "epoch": 2.8823967381669915,
      "grad_norm": 1.8354822397232056,
      "learning_rate": 7.828676783434974e-06,
      "loss": 0.2439,
      "step": 40650
    },
    {
      "epoch": 2.8831058322992376,
      "grad_norm": 1.0115363597869873,
      "learning_rate": 7.781402165177517e-06,
      "loss": 0.2241,
      "step": 40660
    },
    {
      "epoch": 2.8838149264314836,
      "grad_norm": 1.3528152704238892,
      "learning_rate": 7.734127546920058e-06,
      "loss": 0.2203,
      "step": 40670
    },
    {
      "epoch": 2.8845240205637297,
      "grad_norm": 0.930509626865387,
      "learning_rate": 7.686852928662601e-06,
      "loss": 0.2407,
      "step": 40680
    },
    {
      "epoch": 2.8852331146959758,
      "grad_norm": 2.1298623085021973,
      "learning_rate": 7.639578310405143e-06,
      "loss": 0.1925,
      "step": 40690
    },
    {
      "epoch": 2.885942208828222,
      "grad_norm": 2.554778575897217,
      "learning_rate": 7.592303692147686e-06,
      "loss": 0.2328,
      "step": 40700
    },
    {
      "epoch": 2.886651302960468,
      "grad_norm": 1.1426043510437012,
      "learning_rate": 7.54502907389023e-06,
      "loss": 0.2213,
      "step": 40710
    },
    {
      "epoch": 2.887360397092714,
      "grad_norm": 1.5819202661514282,
      "learning_rate": 7.497754455632772e-06,
      "loss": 0.2798,
      "step": 40720
    },
    {
      "epoch": 2.88806949122496,
      "grad_norm": 1.0375406742095947,
      "learning_rate": 7.450479837375314e-06,
      "loss": 0.2578,
      "step": 40730
    },
    {
      "epoch": 2.888778585357206,
      "grad_norm": 1.2772154808044434,
      "learning_rate": 7.403205219117856e-06,
      "loss": 0.2254,
      "step": 40740
    },
    {
      "epoch": 2.889487679489452,
      "grad_norm": 1.3339803218841553,
      "learning_rate": 7.3559306008603985e-06,
      "loss": 0.1986,
      "step": 40750
    },
    {
      "epoch": 2.8901967736216982,
      "grad_norm": 0.9395516514778137,
      "learning_rate": 7.308655982602941e-06,
      "loss": 0.2349,
      "step": 40760
    },
    {
      "epoch": 2.8909058677539443,
      "grad_norm": 0.9666892290115356,
      "learning_rate": 7.261381364345484e-06,
      "loss": 0.2175,
      "step": 40770
    },
    {
      "epoch": 2.8916149618861904,
      "grad_norm": 1.1710665225982666,
      "learning_rate": 7.214106746088026e-06,
      "loss": 0.2188,
      "step": 40780
    },
    {
      "epoch": 2.8923240560184365,
      "grad_norm": 1.3490639925003052,
      "learning_rate": 7.166832127830568e-06,
      "loss": 0.2521,
      "step": 40790
    },
    {
      "epoch": 2.8930331501506825,
      "grad_norm": 0.9626192450523376,
      "learning_rate": 7.1195575095731104e-06,
      "loss": 0.2592,
      "step": 40800
    },
    {
      "epoch": 2.8937422442829286,
      "grad_norm": 1.688420057296753,
      "learning_rate": 7.072282891315653e-06,
      "loss": 0.241,
      "step": 40810
    },
    {
      "epoch": 2.8944513384151747,
      "grad_norm": 0.9707612991333008,
      "learning_rate": 7.025008273058195e-06,
      "loss": 0.253,
      "step": 40820
    },
    {
      "epoch": 2.8951604325474207,
      "grad_norm": 1.3289870023727417,
      "learning_rate": 6.977733654800737e-06,
      "loss": 0.2591,
      "step": 40830
    },
    {
      "epoch": 2.895869526679667,
      "grad_norm": 1.1253395080566406,
      "learning_rate": 6.93045903654328e-06,
      "loss": 0.2199,
      "step": 40840
    },
    {
      "epoch": 2.896578620811913,
      "grad_norm": 1.1707402467727661,
      "learning_rate": 6.883184418285822e-06,
      "loss": 0.2342,
      "step": 40850
    },
    {
      "epoch": 2.897287714944159,
      "grad_norm": 1.7580465078353882,
      "learning_rate": 6.835909800028365e-06,
      "loss": 0.2569,
      "step": 40860
    },
    {
      "epoch": 2.897996809076405,
      "grad_norm": 1.129173755645752,
      "learning_rate": 6.7886351817709085e-06,
      "loss": 0.2539,
      "step": 40870
    },
    {
      "epoch": 2.898705903208651,
      "grad_norm": 0.7337687611579895,
      "learning_rate": 6.741360563513451e-06,
      "loss": 0.2458,
      "step": 40880
    },
    {
      "epoch": 2.899414997340897,
      "grad_norm": 1.9999871253967285,
      "learning_rate": 6.694085945255993e-06,
      "loss": 0.2455,
      "step": 40890
    },
    {
      "epoch": 2.900124091473143,
      "grad_norm": 1.1315948963165283,
      "learning_rate": 6.646811326998535e-06,
      "loss": 0.2129,
      "step": 40900
    },
    {
      "epoch": 2.9008331856053893,
      "grad_norm": 0.9691804051399231,
      "learning_rate": 6.5995367087410774e-06,
      "loss": 0.2191,
      "step": 40910
    },
    {
      "epoch": 2.901542279737635,
      "grad_norm": 1.7532292604446411,
      "learning_rate": 6.55226209048362e-06,
      "loss": 0.2376,
      "step": 40920
    },
    {
      "epoch": 2.9022513738698814,
      "grad_norm": 1.10557222366333,
      "learning_rate": 6.504987472226162e-06,
      "loss": 0.2149,
      "step": 40930
    },
    {
      "epoch": 2.902960468002127,
      "grad_norm": 0.9799390435218811,
      "learning_rate": 6.457712853968705e-06,
      "loss": 0.2234,
      "step": 40940
    },
    {
      "epoch": 2.9036695621343735,
      "grad_norm": 1.147826910018921,
      "learning_rate": 6.410438235711247e-06,
      "loss": 0.2078,
      "step": 40950
    },
    {
      "epoch": 2.904378656266619,
      "grad_norm": 1.158791422843933,
      "learning_rate": 6.363163617453789e-06,
      "loss": 0.2382,
      "step": 40960
    },
    {
      "epoch": 2.9050877503988657,
      "grad_norm": 1.0728468894958496,
      "learning_rate": 6.315888999196332e-06,
      "loss": 0.2674,
      "step": 40970
    },
    {
      "epoch": 2.9057968445311113,
      "grad_norm": 0.9892821907997131,
      "learning_rate": 6.268614380938874e-06,
      "loss": 0.2582,
      "step": 40980
    },
    {
      "epoch": 2.906505938663358,
      "grad_norm": 1.5987054109573364,
      "learning_rate": 6.221339762681417e-06,
      "loss": 0.2779,
      "step": 40990
    },
    {
      "epoch": 2.9072150327956034,
      "grad_norm": 1.1968172788619995,
      "learning_rate": 6.174065144423959e-06,
      "loss": 0.2379,
      "step": 41000
    },
    {
      "epoch": 2.9079241269278495,
      "grad_norm": 0.7505552172660828,
      "learning_rate": 6.126790526166501e-06,
      "loss": 0.2144,
      "step": 41010
    },
    {
      "epoch": 2.9086332210600956,
      "grad_norm": 1.4064033031463623,
      "learning_rate": 6.0795159079090436e-06,
      "loss": 0.2455,
      "step": 41020
    },
    {
      "epoch": 2.9093423151923417,
      "grad_norm": 1.2305090427398682,
      "learning_rate": 6.032241289651587e-06,
      "loss": 0.2464,
      "step": 41030
    },
    {
      "epoch": 2.9100514093245877,
      "grad_norm": 0.9365333914756775,
      "learning_rate": 5.984966671394129e-06,
      "loss": 0.2061,
      "step": 41040
    },
    {
      "epoch": 2.910760503456834,
      "grad_norm": 1.2954256534576416,
      "learning_rate": 5.937692053136671e-06,
      "loss": 0.2399,
      "step": 41050
    },
    {
      "epoch": 2.91146959758908,
      "grad_norm": 1.1410166025161743,
      "learning_rate": 5.890417434879213e-06,
      "loss": 0.2514,
      "step": 41060
    },
    {
      "epoch": 2.912178691721326,
      "grad_norm": 1.0016329288482666,
      "learning_rate": 5.843142816621756e-06,
      "loss": 0.2468,
      "step": 41070
    },
    {
      "epoch": 2.912887785853572,
      "grad_norm": 1.083206295967102,
      "learning_rate": 5.795868198364299e-06,
      "loss": 0.2304,
      "step": 41080
    },
    {
      "epoch": 2.913596879985818,
      "grad_norm": 1.1016491651535034,
      "learning_rate": 5.748593580106841e-06,
      "loss": 0.2271,
      "step": 41090
    },
    {
      "epoch": 2.914305974118064,
      "grad_norm": 1.1215828657150269,
      "learning_rate": 5.701318961849383e-06,
      "loss": 0.2359,
      "step": 41100
    },
    {
      "epoch": 2.91501506825031,
      "grad_norm": 1.5363001823425293,
      "learning_rate": 5.654044343591926e-06,
      "loss": 0.239,
      "step": 41110
    },
    {
      "epoch": 2.9157241623825563,
      "grad_norm": 0.7698983550071716,
      "learning_rate": 5.606769725334468e-06,
      "loss": 0.2796,
      "step": 41120
    },
    {
      "epoch": 2.9164332565148023,
      "grad_norm": 1.107300877571106,
      "learning_rate": 5.5594951070770106e-06,
      "loss": 0.2375,
      "step": 41130
    },
    {
      "epoch": 2.9171423506470484,
      "grad_norm": 1.1351238489151,
      "learning_rate": 5.512220488819553e-06,
      "loss": 0.2373,
      "step": 41140
    },
    {
      "epoch": 2.9178514447792945,
      "grad_norm": 1.0287622213363647,
      "learning_rate": 5.464945870562096e-06,
      "loss": 0.2219,
      "step": 41150
    },
    {
      "epoch": 2.9185605389115405,
      "grad_norm": 1.3477463722229004,
      "learning_rate": 5.417671252304638e-06,
      "loss": 0.2445,
      "step": 41160
    },
    {
      "epoch": 2.9192696330437866,
      "grad_norm": 2.2993099689483643,
      "learning_rate": 5.37039663404718e-06,
      "loss": 0.2698,
      "step": 41170
    },
    {
      "epoch": 2.9199787271760327,
      "grad_norm": 1.136784553527832,
      "learning_rate": 5.3231220157897225e-06,
      "loss": 0.2579,
      "step": 41180
    },
    {
      "epoch": 2.9206878213082788,
      "grad_norm": 1.295568585395813,
      "learning_rate": 5.275847397532266e-06,
      "loss": 0.2639,
      "step": 41190
    },
    {
      "epoch": 2.921396915440525,
      "grad_norm": 2.0857224464416504,
      "learning_rate": 5.228572779274808e-06,
      "loss": 0.281,
      "step": 41200
    },
    {
      "epoch": 2.922106009572771,
      "grad_norm": 0.8835716247558594,
      "learning_rate": 5.18129816101735e-06,
      "loss": 0.265,
      "step": 41210
    },
    {
      "epoch": 2.922815103705017,
      "grad_norm": 1.1100146770477295,
      "learning_rate": 5.134023542759892e-06,
      "loss": 0.238,
      "step": 41220
    },
    {
      "epoch": 2.923524197837263,
      "grad_norm": 1.122260570526123,
      "learning_rate": 5.086748924502435e-06,
      "loss": 0.2344,
      "step": 41230
    },
    {
      "epoch": 2.924233291969509,
      "grad_norm": 0.9885525703430176,
      "learning_rate": 5.0394743062449776e-06,
      "loss": 0.2338,
      "step": 41240
    },
    {
      "epoch": 2.924942386101755,
      "grad_norm": 1.0412172079086304,
      "learning_rate": 4.99219968798752e-06,
      "loss": 0.2274,
      "step": 41250
    },
    {
      "epoch": 2.9256514802340012,
      "grad_norm": 2.547558069229126,
      "learning_rate": 4.944925069730062e-06,
      "loss": 0.2501,
      "step": 41260
    },
    {
      "epoch": 2.926360574366247,
      "grad_norm": 1.0981793403625488,
      "learning_rate": 4.897650451472604e-06,
      "loss": 0.221,
      "step": 41270
    },
    {
      "epoch": 2.9270696684984934,
      "grad_norm": 1.2704362869262695,
      "learning_rate": 4.850375833215147e-06,
      "loss": 0.2463,
      "step": 41280
    },
    {
      "epoch": 2.927778762630739,
      "grad_norm": 1.6213964223861694,
      "learning_rate": 4.8031012149576895e-06,
      "loss": 0.2565,
      "step": 41290
    },
    {
      "epoch": 2.9284878567629855,
      "grad_norm": 1.3906030654907227,
      "learning_rate": 4.755826596700232e-06,
      "loss": 0.234,
      "step": 41300
    },
    {
      "epoch": 2.929196950895231,
      "grad_norm": 0.8846665620803833,
      "learning_rate": 4.708551978442774e-06,
      "loss": 0.2832,
      "step": 41310
    },
    {
      "epoch": 2.9299060450274776,
      "grad_norm": 1.3285633325576782,
      "learning_rate": 4.661277360185317e-06,
      "loss": 0.2263,
      "step": 41320
    },
    {
      "epoch": 2.9306151391597233,
      "grad_norm": 1.4896624088287354,
      "learning_rate": 4.614002741927859e-06,
      "loss": 0.2658,
      "step": 41330
    },
    {
      "epoch": 2.93132423329197,
      "grad_norm": 2.496912956237793,
      "learning_rate": 4.5667281236704015e-06,
      "loss": 0.2405,
      "step": 41340
    },
    {
      "epoch": 2.9320333274242154,
      "grad_norm": 1.0465389490127563,
      "learning_rate": 4.519453505412944e-06,
      "loss": 0.2337,
      "step": 41350
    },
    {
      "epoch": 2.9327424215564615,
      "grad_norm": 1.3175578117370605,
      "learning_rate": 4.472178887155487e-06,
      "loss": 0.2327,
      "step": 41360
    },
    {
      "epoch": 2.9334515156887075,
      "grad_norm": 2.3196847438812256,
      "learning_rate": 4.424904268898029e-06,
      "loss": 0.3026,
      "step": 41370
    },
    {
      "epoch": 2.9341606098209536,
      "grad_norm": 1.4040638208389282,
      "learning_rate": 4.377629650640571e-06,
      "loss": 0.2843,
      "step": 41380
    },
    {
      "epoch": 2.9348697039531997,
      "grad_norm": 1.48458731174469,
      "learning_rate": 4.3303550323831134e-06,
      "loss": 0.2142,
      "step": 41390
    },
    {
      "epoch": 2.9355787980854458,
      "grad_norm": 1.4540034532546997,
      "learning_rate": 4.2830804141256565e-06,
      "loss": 0.2406,
      "step": 41400
    },
    {
      "epoch": 2.936287892217692,
      "grad_norm": 0.9471415877342224,
      "learning_rate": 4.235805795868199e-06,
      "loss": 0.2308,
      "step": 41410
    },
    {
      "epoch": 2.936996986349938,
      "grad_norm": 1.094964623451233,
      "learning_rate": 4.188531177610741e-06,
      "loss": 0.2407,
      "step": 41420
    },
    {
      "epoch": 2.937706080482184,
      "grad_norm": 0.8189399242401123,
      "learning_rate": 4.141256559353283e-06,
      "loss": 0.243,
      "step": 41430
    },
    {
      "epoch": 2.93841517461443,
      "grad_norm": 0.9237013459205627,
      "learning_rate": 4.093981941095825e-06,
      "loss": 0.233,
      "step": 41440
    },
    {
      "epoch": 2.939124268746676,
      "grad_norm": 1.0426995754241943,
      "learning_rate": 4.0467073228383685e-06,
      "loss": 0.2215,
      "step": 41450
    },
    {
      "epoch": 2.939833362878922,
      "grad_norm": 1.5929135084152222,
      "learning_rate": 3.999432704580911e-06,
      "loss": 0.2314,
      "step": 41460
    },
    {
      "epoch": 2.9405424570111682,
      "grad_norm": 0.9831068515777588,
      "learning_rate": 3.952158086323453e-06,
      "loss": 0.237,
      "step": 41470
    },
    {
      "epoch": 2.9412515511434143,
      "grad_norm": 2.362816572189331,
      "learning_rate": 3.904883468065996e-06,
      "loss": 0.2159,
      "step": 41480
    },
    {
      "epoch": 2.9419606452756604,
      "grad_norm": 1.1344729661941528,
      "learning_rate": 3.857608849808538e-06,
      "loss": 0.2442,
      "step": 41490
    },
    {
      "epoch": 2.9426697394079064,
      "grad_norm": 1.120339035987854,
      "learning_rate": 3.8103342315510804e-06,
      "loss": 0.2493,
      "step": 41500
    },
    {
      "epoch": 2.9433788335401525,
      "grad_norm": 1.191452980041504,
      "learning_rate": 3.7630596132936226e-06,
      "loss": 0.2279,
      "step": 41510
    },
    {
      "epoch": 2.9440879276723986,
      "grad_norm": 1.9355770349502563,
      "learning_rate": 3.7157849950361653e-06,
      "loss": 0.228,
      "step": 41520
    },
    {
      "epoch": 2.9447970218046446,
      "grad_norm": 1.0669703483581543,
      "learning_rate": 3.6685103767787075e-06,
      "loss": 0.23,
      "step": 41530
    },
    {
      "epoch": 2.9455061159368907,
      "grad_norm": 0.9009509086608887,
      "learning_rate": 3.6212357585212497e-06,
      "loss": 0.252,
      "step": 41540
    },
    {
      "epoch": 2.946215210069137,
      "grad_norm": 1.106374979019165,
      "learning_rate": 3.5739611402637924e-06,
      "loss": 0.2391,
      "step": 41550
    },
    {
      "epoch": 2.946924304201383,
      "grad_norm": 1.1479392051696777,
      "learning_rate": 3.5266865220063355e-06,
      "loss": 0.2537,
      "step": 41560
    },
    {
      "epoch": 2.947633398333629,
      "grad_norm": 1.3565160036087036,
      "learning_rate": 3.4794119037488777e-06,
      "loss": 0.2364,
      "step": 41570
    },
    {
      "epoch": 2.948342492465875,
      "grad_norm": 0.859965980052948,
      "learning_rate": 3.43213728549142e-06,
      "loss": 0.2308,
      "step": 41580
    },
    {
      "epoch": 2.949051586598121,
      "grad_norm": 1.3681716918945312,
      "learning_rate": 3.384862667233962e-06,
      "loss": 0.2179,
      "step": 41590
    },
    {
      "epoch": 2.9497606807303667,
      "grad_norm": 1.2581068277359009,
      "learning_rate": 3.3375880489765048e-06,
      "loss": 0.2856,
      "step": 41600
    },
    {
      "epoch": 2.950469774862613,
      "grad_norm": 1.013523817062378,
      "learning_rate": 3.290313430719047e-06,
      "loss": 0.2054,
      "step": 41610
    },
    {
      "epoch": 2.951178868994859,
      "grad_norm": 1.2031532526016235,
      "learning_rate": 3.2430388124615892e-06,
      "loss": 0.2432,
      "step": 41620
    },
    {
      "epoch": 2.9518879631271053,
      "grad_norm": 0.9405839443206787,
      "learning_rate": 3.1957641942041314e-06,
      "loss": 0.215,
      "step": 41630
    },
    {
      "epoch": 2.952597057259351,
      "grad_norm": 2.460130453109741,
      "learning_rate": 3.1484895759466745e-06,
      "loss": 0.2254,
      "step": 41640
    },
    {
      "epoch": 2.9533061513915975,
      "grad_norm": 0.8709436655044556,
      "learning_rate": 3.1012149576892167e-06,
      "loss": 0.2362,
      "step": 41650
    },
    {
      "epoch": 2.954015245523843,
      "grad_norm": 1.9480856657028198,
      "learning_rate": 3.0539403394317594e-06,
      "loss": 0.2579,
      "step": 41660
    },
    {
      "epoch": 2.9547243396560896,
      "grad_norm": 1.039057970046997,
      "learning_rate": 3.0066657211743016e-06,
      "loss": 0.2574,
      "step": 41670
    },
    {
      "epoch": 2.9554334337883352,
      "grad_norm": 1.1507235765457153,
      "learning_rate": 2.9593911029168442e-06,
      "loss": 0.2483,
      "step": 41680
    },
    {
      "epoch": 2.9561425279205813,
      "grad_norm": 1.6006748676300049,
      "learning_rate": 2.9121164846593865e-06,
      "loss": 0.2449,
      "step": 41690
    },
    {
      "epoch": 2.9568516220528274,
      "grad_norm": 1.0399916172027588,
      "learning_rate": 2.864841866401929e-06,
      "loss": 0.2514,
      "step": 41700
    },
    {
      "epoch": 2.9575607161850734,
      "grad_norm": 1.3353456258773804,
      "learning_rate": 2.8175672481444713e-06,
      "loss": 0.2309,
      "step": 41710
    },
    {
      "epoch": 2.9582698103173195,
      "grad_norm": 1.1503137350082397,
      "learning_rate": 2.7702926298870136e-06,
      "loss": 0.2408,
      "step": 41720
    },
    {
      "epoch": 2.9589789044495656,
      "grad_norm": 0.8910015225410461,
      "learning_rate": 2.723018011629556e-06,
      "loss": 0.2371,
      "step": 41730
    },
    {
      "epoch": 2.9596879985818116,
      "grad_norm": 1.0206081867218018,
      "learning_rate": 2.675743393372099e-06,
      "loss": 0.219,
      "step": 41740
    },
    {
      "epoch": 2.9603970927140577,
      "grad_norm": 0.9863037467002869,
      "learning_rate": 2.628468775114641e-06,
      "loss": 0.1872,
      "step": 41750
    },
    {
      "epoch": 2.961106186846304,
      "grad_norm": 1.3696959018707275,
      "learning_rate": 2.5811941568571833e-06,
      "loss": 0.2253,
      "step": 41760
    },
    {
      "epoch": 2.96181528097855,
      "grad_norm": 1.753408670425415,
      "learning_rate": 2.533919538599726e-06,
      "loss": 0.2515,
      "step": 41770
    },
    {
      "epoch": 2.962524375110796,
      "grad_norm": 1.2472542524337769,
      "learning_rate": 2.4866449203422686e-06,
      "loss": 0.2498,
      "step": 41780
    },
    {
      "epoch": 2.963233469243042,
      "grad_norm": 1.4439822435379028,
      "learning_rate": 2.439370302084811e-06,
      "loss": 0.2286,
      "step": 41790
    },
    {
      "epoch": 2.963942563375288,
      "grad_norm": 1.393797755241394,
      "learning_rate": 2.392095683827353e-06,
      "loss": 0.2422,
      "step": 41800
    },
    {
      "epoch": 2.964651657507534,
      "grad_norm": 1.1070661544799805,
      "learning_rate": 2.3448210655698957e-06,
      "loss": 0.2289,
      "step": 41810
    },
    {
      "epoch": 2.96536075163978,
      "grad_norm": 1.0917503833770752,
      "learning_rate": 2.2975464473124383e-06,
      "loss": 0.2169,
      "step": 41820
    },
    {
      "epoch": 2.9660698457720263,
      "grad_norm": 0.9083629250526428,
      "learning_rate": 2.2502718290549805e-06,
      "loss": 0.2414,
      "step": 41830
    },
    {
      "epoch": 2.9667789399042723,
      "grad_norm": 1.2698229551315308,
      "learning_rate": 2.2029972107975228e-06,
      "loss": 0.2309,
      "step": 41840
    },
    {
      "epoch": 2.9674880340365184,
      "grad_norm": 2.18636155128479,
      "learning_rate": 2.1557225925400654e-06,
      "loss": 0.2428,
      "step": 41850
    },
    {
      "epoch": 2.9681971281687645,
      "grad_norm": 0.9718664884567261,
      "learning_rate": 2.108447974282608e-06,
      "loss": 0.2399,
      "step": 41860
    },
    {
      "epoch": 2.9689062223010105,
      "grad_norm": 1.4458558559417725,
      "learning_rate": 2.0611733560251503e-06,
      "loss": 0.2895,
      "step": 41870
    },
    {
      "epoch": 2.9696153164332566,
      "grad_norm": 1.0427818298339844,
      "learning_rate": 2.0138987377676925e-06,
      "loss": 0.2593,
      "step": 41880
    },
    {
      "epoch": 2.9703244105655027,
      "grad_norm": 1.3753057718276978,
      "learning_rate": 1.9666241195102347e-06,
      "loss": 0.2297,
      "step": 41890
    },
    {
      "epoch": 2.9710335046977487,
      "grad_norm": 1.0408384799957275,
      "learning_rate": 1.919349501252778e-06,
      "loss": 0.2311,
      "step": 41900
    },
    {
      "epoch": 2.971742598829995,
      "grad_norm": 1.0160621404647827,
      "learning_rate": 1.87207488299532e-06,
      "loss": 0.215,
      "step": 41910
    },
    {
      "epoch": 2.972451692962241,
      "grad_norm": 1.232592225074768,
      "learning_rate": 1.8248002647378622e-06,
      "loss": 0.2488,
      "step": 41920
    },
    {
      "epoch": 2.973160787094487,
      "grad_norm": 1.1591250896453857,
      "learning_rate": 1.7775256464804047e-06,
      "loss": 0.2368,
      "step": 41930
    },
    {
      "epoch": 2.973869881226733,
      "grad_norm": 1.251623272895813,
      "learning_rate": 1.7302510282229473e-06,
      "loss": 0.2297,
      "step": 41940
    },
    {
      "epoch": 2.9745789753589786,
      "grad_norm": 0.7849709391593933,
      "learning_rate": 1.6829764099654898e-06,
      "loss": 0.1935,
      "step": 41950
    },
    {
      "epoch": 2.975288069491225,
      "grad_norm": 1.1524968147277832,
      "learning_rate": 1.635701791708032e-06,
      "loss": 0.242,
      "step": 41960
    },
    {
      "epoch": 2.9759971636234708,
      "grad_norm": 0.7476344704627991,
      "learning_rate": 1.5884271734505744e-06,
      "loss": 0.2237,
      "step": 41970
    },
    {
      "epoch": 2.9767062577557173,
      "grad_norm": 1.2608412504196167,
      "learning_rate": 1.5411525551931169e-06,
      "loss": 0.2189,
      "step": 41980
    },
    {
      "epoch": 2.977415351887963,
      "grad_norm": 1.51261305809021,
      "learning_rate": 1.4938779369356593e-06,
      "loss": 0.2474,
      "step": 41990
    },
    {
      "epoch": 2.9781244460202094,
      "grad_norm": 0.9464707970619202,
      "learning_rate": 1.4466033186782017e-06,
      "loss": 0.2744,
      "step": 42000
    },
    {
      "epoch": 2.978833540152455,
      "grad_norm": 1.0465587377548218,
      "learning_rate": 1.3993287004207442e-06,
      "loss": 0.2259,
      "step": 42010
    },
    {
      "epoch": 2.9795426342847016,
      "grad_norm": 1.2800064086914062,
      "learning_rate": 1.3520540821632866e-06,
      "loss": 0.225,
      "step": 42020
    },
    {
      "epoch": 2.980251728416947,
      "grad_norm": 1.0608551502227783,
      "learning_rate": 1.304779463905829e-06,
      "loss": 0.2117,
      "step": 42030
    },
    {
      "epoch": 2.9809608225491933,
      "grad_norm": 0.9769527316093445,
      "learning_rate": 1.2575048456483715e-06,
      "loss": 0.2283,
      "step": 42040
    },
    {
      "epoch": 2.9816699166814393,
      "grad_norm": 1.0443955659866333,
      "learning_rate": 1.2102302273909139e-06,
      "loss": 0.2588,
      "step": 42050
    },
    {
      "epoch": 2.9823790108136854,
      "grad_norm": 1.0814262628555298,
      "learning_rate": 1.1629556091334563e-06,
      "loss": 0.243,
      "step": 42060
    },
    {
      "epoch": 2.9830881049459315,
      "grad_norm": 1.136198878288269,
      "learning_rate": 1.1156809908759988e-06,
      "loss": 0.2512,
      "step": 42070
    },
    {
      "epoch": 2.9837971990781775,
      "grad_norm": 1.4742323160171509,
      "learning_rate": 1.0684063726185412e-06,
      "loss": 0.2249,
      "step": 42080
    },
    {
      "epoch": 2.9845062932104236,
      "grad_norm": 2.0152785778045654,
      "learning_rate": 1.0211317543610836e-06,
      "loss": 0.2359,
      "step": 42090
    },
    {
      "epoch": 2.9852153873426697,
      "grad_norm": 1.48538339138031,
      "learning_rate": 9.73857136103626e-07,
      "loss": 0.2311,
      "step": 42100
    },
    {
      "epoch": 2.9859244814749157,
      "grad_norm": 1.1716760396957397,
      "learning_rate": 9.265825178461685e-07,
      "loss": 0.2575,
      "step": 42110
    },
    {
      "epoch": 2.986633575607162,
      "grad_norm": 1.240652084350586,
      "learning_rate": 8.793078995887108e-07,
      "loss": 0.2618,
      "step": 42120
    },
    {
      "epoch": 2.987342669739408,
      "grad_norm": 1.0444447994232178,
      "learning_rate": 8.320332813312534e-07,
      "loss": 0.2402,
      "step": 42130
    },
    {
      "epoch": 2.988051763871654,
      "grad_norm": 1.1389026641845703,
      "learning_rate": 7.847586630737957e-07,
      "loss": 0.2318,
      "step": 42140
    },
    {
      "epoch": 2.9887608580039,
      "grad_norm": 0.9192986488342285,
      "learning_rate": 7.374840448163381e-07,
      "loss": 0.2597,
      "step": 42150
    },
    {
      "epoch": 2.989469952136146,
      "grad_norm": 1.8339637517929077,
      "learning_rate": 6.902094265588806e-07,
      "loss": 0.2693,
      "step": 42160
    },
    {
      "epoch": 2.990179046268392,
      "grad_norm": 1.480279564857483,
      "learning_rate": 6.42934808301423e-07,
      "loss": 0.2519,
      "step": 42170
    },
    {
      "epoch": 2.990888140400638,
      "grad_norm": 1.2135308980941772,
      "learning_rate": 5.956601900439654e-07,
      "loss": 0.229,
      "step": 42180
    },
    {
      "epoch": 2.9915972345328843,
      "grad_norm": 1.0364750623703003,
      "learning_rate": 5.483855717865079e-07,
      "loss": 0.2376,
      "step": 42190
    },
    {
      "epoch": 2.9923063286651304,
      "grad_norm": 1.159847617149353,
      "learning_rate": 5.011109535290503e-07,
      "loss": 0.2383,
      "step": 42200
    },
    {
      "epoch": 2.9930154227973764,
      "grad_norm": 1.2387558221817017,
      "learning_rate": 4.5383633527159274e-07,
      "loss": 0.2245,
      "step": 42210
    },
    {
      "epoch": 2.9937245169296225,
      "grad_norm": 1.1458585262298584,
      "learning_rate": 4.065617170141351e-07,
      "loss": 0.241,
      "step": 42220
    },
    {
      "epoch": 2.9944336110618686,
      "grad_norm": 3.2592782974243164,
      "learning_rate": 3.5928709875667755e-07,
      "loss": 0.2641,
      "step": 42230
    },
    {
      "epoch": 2.9951427051941146,
      "grad_norm": 1.1521378755569458,
      "learning_rate": 3.1201248049922e-07,
      "loss": 0.2698,
      "step": 42240
    },
    {
      "epoch": 2.9958517993263607,
      "grad_norm": 1.1277669668197632,
      "learning_rate": 2.647378622417624e-07,
      "loss": 0.2404,
      "step": 42250
    },
    {
      "epoch": 2.9965608934586068,
      "grad_norm": 1.1415773630142212,
      "learning_rate": 2.1746324398430485e-07,
      "loss": 0.2243,
      "step": 42260
    },
    {
      "epoch": 2.997269987590853,
      "grad_norm": 1.2566367387771606,
      "learning_rate": 1.7018862572684726e-07,
      "loss": 0.2016,
      "step": 42270
    },
    {
      "epoch": 2.997979081723099,
      "grad_norm": 1.5569871664047241,
      "learning_rate": 1.2291400746938967e-07,
      "loss": 0.2275,
      "step": 42280
    },
    {
      "epoch": 2.998688175855345,
      "grad_norm": 1.184075951576233,
      "learning_rate": 7.563938921193212e-08,
      "loss": 0.2399,
      "step": 42290
    },
    {
      "epoch": 2.9993972699875906,
      "grad_norm": 1.1318795680999756,
      "learning_rate": 2.8364770954474546e-08,
      "loss": 0.247,
      "step": 42300
    },
    {
      "epoch": 2.9998227264669386,
      "eval_runtime": 1275.6201,
      "eval_samples_per_second": 5.528,
      "eval_steps_per_second": 5.528,
      "step": 42306
    }
  ],
  "logging_steps": 10,
  "max_steps": 42306,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 320852143374336.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
